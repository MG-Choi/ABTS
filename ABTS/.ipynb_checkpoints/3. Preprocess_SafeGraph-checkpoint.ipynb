{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Safegraph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "from tqdm.auto import tqdm\n",
    "import geopandas as gpd\n",
    "from pyproj import Geod\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "### 1) DO to OD: Covert Destination area to Origin area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filteringNeighbor(df, print_progress = True):\n",
    "    neighbors = df.copy()\n",
    "    col = ['area', 'work_behavior_device_home_areas', 'weekday_device_home_areas', 'weekend_device_home_areas']\n",
    "\n",
    "    neighbors = neighbors[col]\n",
    "    \n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DOtoOD(df, print_progress = True):\n",
    "    ODdata = df.copy()\n",
    "\n",
    "    def calculate_work_behavior_from(df):\n",
    "        # 각 area에 대한 방문을 저장할 딕셔너리를 초기화합니다.\n",
    "        all_area_dict = {str(area): {} for area in df['area'].tolist()}\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total = len(df), desc = '1. Work behavior...'):\n",
    "            current_area_dict = eval(row['work_behavior_device_home_areas'])\n",
    "            destination = str(row['area'])\n",
    "\n",
    "            for source, count in current_area_dict.items():\n",
    "                # 추가된 부분: 출발지가 all_area_dict에 없으면 다음 key로 이동합니다.\n",
    "                if source not in all_area_dict:\n",
    "                    continue\n",
    "\n",
    "                if destination not in all_area_dict[source]:\n",
    "                    all_area_dict[source][destination] = 0\n",
    "\n",
    "                all_area_dict[source][destination] += count\n",
    "\n",
    "        # all_area_dict에 기록된 결과를 df의 새로운 컬럼에 할당합니다.\n",
    "        df['work_behavior_from_area'] = df['area'].map(lambda x: all_area_dict[str(x)])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fix_malformed_dict_str(s):\n",
    "        # 문자열이 }'로 끝나지 않는 경우\n",
    "        if not s.endswith(\"}\"):\n",
    "            # 마지막 ,의 인덱스를 찾아\n",
    "            last_comma_index = s.rfind(\",\")\n",
    "            # 그 이전의 문자열에 }'을 붙여준다.\n",
    "            s = s[:last_comma_index] + \"}\"\n",
    "        return s\n",
    "\n",
    "    def calculate_weekday_behavior_from(df):\n",
    "        all_area_dict = {str(area): {} for area in df['area'].tolist()}\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total = len(df), desc = '2) Other behaviors - weekday...'):\n",
    "            # 문자열을 수정\n",
    "            fixed_str = fix_malformed_dict_str(row['weekday_device_home_areas'])\n",
    "            try:\n",
    "                current_area_dict = eval(fixed_str)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in row {_}: {e}\")\n",
    "                continue\n",
    "\n",
    "            destination = str(row['area'])\n",
    "\n",
    "            for source, count in current_area_dict.items():\n",
    "                if source not in all_area_dict:\n",
    "                    continue\n",
    "\n",
    "                if destination not in all_area_dict[source]:\n",
    "                    all_area_dict[source][destination] = 0\n",
    "\n",
    "                all_area_dict[source][destination] += count\n",
    "\n",
    "        df['weekday_device_from_area_home'] = df['area'].map(lambda x: all_area_dict[str(x)])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def calculate_weekend_behavior_from(df):\n",
    "        all_area_dict = {str(area): {} for area in df['area'].tolist()}\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total = len(df), desc = '3) Other behaviors - weekend...'):\n",
    "            # 문자열을 수정\n",
    "            fixed_str = fix_malformed_dict_str(row['weekend_device_home_areas'])\n",
    "            try:\n",
    "                current_area_dict = eval(fixed_str)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in row {_}: {e}\")\n",
    "                continue\n",
    "\n",
    "            destination = str(row['area'])\n",
    "\n",
    "            for source, count in current_area_dict.items():\n",
    "                if source not in all_area_dict:\n",
    "                    continue\n",
    "\n",
    "                if destination not in all_area_dict[source]:\n",
    "                    all_area_dict[source][destination] = 0\n",
    "\n",
    "                all_area_dict[source][destination] += count\n",
    "\n",
    "        df['weekend_device_from_area_home'] = df['area'].map(lambda x: all_area_dict[str(x)])\n",
    "\n",
    "        return df\n",
    "    \n",
    "    # 1) Work behavior\n",
    "    ODdata = calculate_work_behavior_from(ODdata)\n",
    "    \n",
    "    # 2) Other behaviors - Weekday\n",
    "    ODdata = calculate_weekday_behavior_from(ODdata)\n",
    "    \n",
    "    # 3) Other behaviors - Weekend\n",
    "    ODdata = calculate_weekend_behavior_from(ODdata)\n",
    "\n",
    "    # Extracting columns\n",
    "    col = ['area', 'work_behavior_from_area','weekday_device_from_area_home','weekend_device_from_area_home']\n",
    "    ODdata = ODdata[col]\n",
    "    \n",
    "    return ODdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Computing probability of trips from origin cbg to dest cbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilityByk_Ws_Wd(neighbor_safegraphDF, landuseGDF, W_s, W_d):\n",
    "\n",
    "    def compute_probability(df, area='area', cols=['work_behavior_from_area', 'weekday_device_from_area_home', 'weekend_device_from_area_home']):\n",
    "        # Create a copy of the original DataFrame for results\n",
    "\n",
    "        prob_trips_in_space = df.copy()\n",
    "        prob_trips_in_space['work_behavior_from_area'] = prob_trips_in_space['work_behavior_from_area'].astype(str)\n",
    "        prob_trips_in_space['weekday_device_from_area_home'] = prob_trips_in_space['weekday_device_from_area_home'].astype(str)\n",
    "        prob_trips_in_space['weekend_device_from_area_home'] = prob_trips_in_space['weekend_device_from_area_home'].astype(str)\n",
    "\n",
    "        # Iterate over each row in the prob_trips_in_space DataFrame\n",
    "        for index, row in tqdm(prob_trips_in_space.iterrows(), total=prob_trips_in_space.shape[0], desc = '1) Probability A to A_i...1 (add k folmula)'):\n",
    "\n",
    "            # Update each dictionary in the row based on its own total_k\n",
    "            for col in cols:\n",
    "                dict_data = eval(row[col])\n",
    "                total_k = sum(dict_data.values())  # Calculate the total k for the current column only\n",
    "\n",
    "                for key in dict_data:\n",
    "                    if total_k != 0:  # Ensure not to divide by zero\n",
    "                        dict_data[key] = dict_data[key] / total_k\n",
    "                    else:\n",
    "                        dict_data[key] = 0\n",
    "\n",
    "                prob_trips_in_space.at[index, col] = str(dict_data)  # Convert updated dictionary back to string representation\n",
    "\n",
    "        # Additional columns for 'work_behavior_from_area'\n",
    "        prob_trips_in_space['weekday_Work'] = prob_trips_in_space['work_behavior_from_area']\n",
    "        prob_trips_in_space['weekend_Work'] = prob_trips_in_space['work_behavior_from_area']\n",
    "\n",
    "        # Split weekday_device_from_area_home into multiple columns\n",
    "        weekday_cols = ['weekday_School', 'weekday_University', 'weekday_Dailycare', 'weekday_Religion', 'weekday_Large_shop', 'weekday_Etc_shop', 'weekday_Meals', 'weekday_V_fr_rel', 'weekday_Rec_lei', 'weekday_Serv_trip', 'weekday_Others']\n",
    "        for col in weekday_cols:\n",
    "            prob_trips_in_space[col] = prob_trips_in_space['weekday_device_from_area_home']\n",
    "\n",
    "        # Split weekend_device_from_area_home into multiple columns\n",
    "        weekend_cols = [col.replace('weekday', 'weekend') for col in weekday_cols]\n",
    "        for col in weekend_cols:\n",
    "            prob_trips_in_space[col] = prob_trips_in_space['weekend_device_from_area_home']\n",
    "\n",
    "        return prob_trips_in_space\n",
    "\n",
    "    def apply_Ws_formula(prob_trips_in_space, landUse, W_s):\n",
    "        df_ws = prob_trips_in_space.copy()\n",
    "        exclude_columns = ['work_behavior_from_area', 'weekday_device_from_area_home', 'weekend_device_from_area_home']\n",
    "        df_ws.drop(exclude_columns, axis=1, inplace=True)\n",
    "\n",
    "        # Iterate over the rows and columns of df_ws\n",
    "        for index, row in tqdm(df_ws.iterrows(), total=df_ws.shape[0], desc = '2) Probability A to A_i...2 (add Ws weight)'):\n",
    "            for col in df_ws.columns:\n",
    "                # Avoid processing non-dictionary columns and excluded columns\n",
    "                if col == 'area':\n",
    "                    continue\n",
    "\n",
    "                # Extract the purpose from the column name\n",
    "                purpose = \"_\".join(col.split('_')[1:])\n",
    "\n",
    "                dict_data = eval(row[col])\n",
    "\n",
    "                # Calculate C_Ai for each key in the dictionary\n",
    "                C_Ai_dict = {key: landUse[landUse['CBGCODE'] == key]['TRPPURP'].apply(lambda x: purpose in x).sum() for key in dict_data.keys()}\n",
    "    #             print(purpose)\n",
    "    #             print(C_Ai_dict)\n",
    "\n",
    "                # Calculate sum_j C_Aj\n",
    "                sum_C_Aj = sum(C_Ai_dict.values())\n",
    "\n",
    "                # Apply the formula\n",
    "                new_prob_values = {}  # A new dictionary to store normalized probabilities\n",
    "                for key, value in dict_data.items():\n",
    "                    C_Ai = C_Ai_dict[key]\n",
    "                    multiplier = (C_Ai / sum_C_Aj) ** W_s if sum_C_Aj != 0 else 0  # Ensure not to divide by zero\n",
    "                    new_prob_values[key] = value * multiplier\n",
    "\n",
    "                # Normalize the probabilities to sum up to 1\n",
    "                total_probability = sum(new_prob_values.values())\n",
    "                for key in new_prob_values:\n",
    "                    new_prob_values[key] = new_prob_values[key] / total_probability if total_probability != 0 else 0\n",
    "\n",
    "                df_ws.at[index, col] = str(new_prob_values)\n",
    "\n",
    "        return df_ws\n",
    "    \n",
    "    def apply_Ws_formula_optimized(prob_trips_in_space, landUse, W_s):\n",
    "        df_ws = prob_trips_in_space.copy()\n",
    "        exclude_columns = ['work_behavior_from_area', 'weekday_device_from_area_home', 'weekend_device_from_area_home']\n",
    "        df_ws.drop(exclude_columns, axis=1, inplace=True)\n",
    "\n",
    "        # Pre-calculate C_Ai for all keys in landUse\n",
    "        all_keys = set()\n",
    "        for col in tqdm(df_ws.columns, desc = '2) Probability A to A_i...2 (add Ws weight)'):\n",
    "            if col != 'area':\n",
    "                df_ws[col].apply(lambda x: all_keys.update(eval(x).keys()))\n",
    "        all_keys = list(all_keys)\n",
    "        C_Ai_dict = {key: {} for key in all_keys}\n",
    "        for key in tqdm(all_keys, desc = '  2.1) Indexing '):\n",
    "            sub_df = landUse[landUse['CBGCODE'] == key]\n",
    "            for col in df_ws.columns:\n",
    "                if col != 'area':\n",
    "                    purpose = \"_\".join(col.split('_')[1:])\n",
    "                    C_Ai_dict[key][purpose] = sub_df['TRPPURP'].apply(lambda x: purpose in x).sum()\n",
    "\n",
    "        # Iterate over the rows and columns of df_ws\n",
    "        for index, row in tqdm(df_ws.iterrows(), total=df_ws.shape[0], desc='  2.2) add Ws weight '):\n",
    "            for col in df_ws.columns:\n",
    "                if col == 'area':\n",
    "                    continue\n",
    "\n",
    "                # Extract the purpose from the column name\n",
    "                purpose = \"_\".join(col.split('_')[1:])\n",
    "                dict_data = eval(row[col])\n",
    "\n",
    "                # Fetch C_Ai values from the pre-calculated dictionary\n",
    "                local_C_Ai_values = [C_Ai_dict[key][purpose] for key in dict_data.keys()]\n",
    "                sum_C_Aj = sum(local_C_Ai_values)\n",
    "\n",
    "                # Apply the formula\n",
    "                new_prob_values = {}\n",
    "                for (key, value), C_Ai in zip(dict_data.items(), local_C_Ai_values):\n",
    "                    multiplier = (C_Ai / sum_C_Aj) ** W_s if sum_C_Aj != 0 else 0\n",
    "                    new_prob_values[key] = value * multiplier\n",
    "\n",
    "                # Normalize the probabilities to sum up to 1\n",
    "                total_probability = sum(new_prob_values.values())\n",
    "                for key in new_prob_values:\n",
    "                    new_prob_values[key] = new_prob_values[key] / total_probability if total_probability != 0 else 0\n",
    "\n",
    "                df_ws.at[index, col] = str(new_prob_values)\n",
    "\n",
    "        return df_ws\n",
    "\n",
    "    \n",
    "    \n",
    "    def calculate_distance_meters(point1, point2):\n",
    "        # WGS 84\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "        angle1,angle2,distance = geod.inv(point1.x, point1.y, point2.x, point2.y)\n",
    "\n",
    "        return distance\n",
    "\n",
    "\n",
    "    def apply_Wd_formula(df, landUse, W_d=0):\n",
    "        df_wd = df.copy()\n",
    "        # Iterate over the rows and columns of df_wd\n",
    "        for index, row in tqdm(df_wd.iterrows(), total=df_wd.shape[0], desc='3) Probability A to A_i...3 (add W_d weight)'):\n",
    "\n",
    "            # Convert row['area'] to string for matching\n",
    "            area_str = str(row['area'])\n",
    "            area_geometry = landUse[landUse['CBGCODE'] == area_str].geometry.iloc[0]\n",
    "            area_center = area_geometry.centroid\n",
    "\n",
    "            for col in df_wd.columns:\n",
    "                if col == 'area':\n",
    "                    continue\n",
    "\n",
    "                dict_data = eval(row[col])\n",
    "                keys_to_remove = []\n",
    "\n",
    "                for key, value in dict_data.items():\n",
    "                    # Get the destination geometry, if it does not exist, set the value to 0\n",
    "                    destination_geometry_series = landUse[landUse['CBGCODE'] == key].geometry\n",
    "\n",
    "                    if destination_geometry_series.empty:   # value가 (probability) 0이면 지우기.\n",
    "#                         print(key)\n",
    "                        dict_data[key] = 0\n",
    "                        continue\n",
    "\n",
    "                    destination_geometry = destination_geometry_series.iloc[0]\n",
    "                    destination_center = destination_geometry.centroid\n",
    "                    # if area_center is None:\n",
    "                    #     print(dict_data)\n",
    "                    #     print(key)\n",
    "                    distance = calculate_distance_meters(area_center, destination_center)\n",
    "                    distance = distance/1000 # Convert to km\n",
    "\n",
    "                    dict_data[key] = value * np.exp(-W_d * distance)\n",
    "\n",
    "                # Normalize the updated values\n",
    "                total_probability = sum(dict_data.values())\n",
    "                for key in dict_data:\n",
    "                    dict_data[key] = round(dict_data[key] / total_probability if total_probability != 0 else 0, 5)\n",
    "\n",
    "                for key in keys_to_remove:\n",
    "                    del dict_data[key]\n",
    "\n",
    "                df_wd.at[index, col] = str(dict_data)\n",
    "\n",
    "        return df_wd\n",
    "    \n",
    "    def apply_Wd_formula_optimized(df, landUse, W_d=0):\n",
    "        df_wd = df.copy()\n",
    "\n",
    "        # Precompute centroid for all areas in landUse\n",
    "        landUse['centroid'] = landUse['geometry'].centroid\n",
    "\n",
    "        # Create a dictionary for fast lookup of centroids\n",
    "        centroid_lookup = landUse.set_index('CBGCODE')['centroid'].to_dict()\n",
    "\n",
    "        rows_to_drop = []  # List to keep track of rows to drop\n",
    "        areas_to_drop = []\n",
    "\n",
    "        # Iterate over the rows and columns of df_wd\n",
    "        for index, row in tqdm(df_wd.iterrows(), total=df_wd.shape[0], desc='3) Probability A to A_i...3 (add W_d weight)'):\n",
    "\n",
    "            area_str = str(row['area'])\n",
    "            area_center = centroid_lookup.get(area_str)\n",
    "\n",
    "            if area_center is None:  # If there's no centroid for the area, mark row for removal\n",
    "                rows_to_drop.append(index)\n",
    "                areas_to_drop.append(area_str)\n",
    "                continue\n",
    "\n",
    "            for col in df_wd.columns:\n",
    "                if col == 'area':\n",
    "                    continue\n",
    "\n",
    "                dict_data = eval(row[col])\n",
    "                keys_to_remove = []\n",
    "\n",
    "                for key, value in dict_data.items():\n",
    "                    destination_center = centroid_lookup.get(key)\n",
    "\n",
    "                    if destination_center is None:  # Check for missing destination geometry\n",
    "                        dict_data[key] = 0\n",
    "                    else:\n",
    "                        distance = calculate_distance_meters(area_center, destination_center) / 1000\n",
    "                        dict_data[key] = value * np.exp(-W_d * distance)\n",
    "\n",
    "                    # If value is zero, mark for removal\n",
    "                    if dict_data[key] == 0:\n",
    "                        keys_to_remove.append(key)\n",
    "\n",
    "                # Normalize the updated values\n",
    "                total_probability = sum(dict_data.values())\n",
    "                for key in dict_data:\n",
    "                    dict_data[key] = round(dict_data[key] / total_probability if total_probability != 0 else 0, 5)\n",
    "\n",
    "                # Remove keys that have zero values\n",
    "                for key in keys_to_remove:\n",
    "                    del dict_data[key]\n",
    "\n",
    "                df_wd.at[index, col] = str(dict_data)\n",
    "\n",
    "        # Drop rows where area_center is None and reset index\n",
    "        df_wd.drop(rows_to_drop, inplace=True)\n",
    "        df_wd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#         print('dropped area: ', areas_to_drop)\n",
    "        return df_wd\n",
    "    \n",
    "    # k formula\n",
    "    print('----- W_s: ' + str(W_s) + ', W_d: ' + str(W_d) + '-----')\n",
    "    prob_trips_in_space_k = compute_probability(neighbor_safegraphDF)\n",
    "    prob_trips_in_space_ws = apply_Ws_formula_optimized(prob_trips_in_space_k, landUse, W_s = W_s)\n",
    "    prob_trips_in_space_wd = apply_Wd_formula_optimized(prob_trips_in_space_ws, landUse, W_d = W_d)\n",
    "    \n",
    "    # Dealing with Empty variables\n",
    "    for index, row in tqdm(prob_trips_in_space_wd.iterrows(), total = len(prob_trips_in_space_wd), desc = '4) Filling empty values'):\n",
    "        if row['weekday_Work'] == '{}' or row['weekend_Work'] == '{}':\n",
    "            area_value = row['area']\n",
    "            prob_trips_in_space_wd.at[index, 'weekday_Work'] = f\"{{'{area_value}': 1.0}}\"\n",
    "            prob_trips_in_space_wd.at[index, 'weekend_Work'] = f\"{{'{area_value}': 1.0}}\"\n",
    "\n",
    "    # \"Unnamed: 0\" 컬럼 삭제\n",
    "    if 'Unnamed: 0' in prob_trips_in_space_wd.columns:\n",
    "        prob_trips_in_space_wd.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    return prob_trips_in_space_wd\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "### 1) filtering Neighbor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborPath = 'E:/data/Chapter_3_data/Origin_data/Safegraph_neighbor/'\n",
    "\n",
    "neighbor_2020_09 =  pd.read_excel(neighborPath + 'neighbor_2020_09csv.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'area', 'area_type', 'origin_area_type',\n",
       "       'date_range_start', 'date_range_end', 'day_counts', 'raw_stop_counts',\n",
       "       'raw_device_counts', 'stops_by_day', 'stops_by_each_hour',\n",
       "       'device_home_areas', 'weekday_device_home_areas',\n",
       "       'weekend_device_home_areas', 'breakfast_device_home_areas',\n",
       "       'lunch_device_home_areas', 'afternoon_tea_device_home_areas',\n",
       "       'dinner_device_home_areas', 'nightlife_device_home_areas',\n",
       "       'work_hours_device_home_areas', 'work_behavior_device_home_areas',\n",
       "       'device_daytime_areas', 'distance_from_home',\n",
       "       'distance_from_primary_daytime_location', 'median_dwell',\n",
       "       'top_same_day_brand', 'top_same_month_brand', 'popularity_by_each_hour',\n",
       "       'popularity_by_hour_monday', 'popularity_by_hour_tuesday',\n",
       "       'popularity_by_hour_wednesday', 'popularity_by_hour_thursday',\n",
       "       'popularity_by_hour_friday', 'popularity_by_hour_saturday',\n",
       "       'popularity_by_hour_sunday', 'device_type', 'iso_country_code',\n",
       "       'region'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_2020_09.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_2020_09 = filteringNeighbor(neighbor_2020_09, print_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>work_behavior_device_home_areas</th>\n",
       "      <th>weekday_device_home_areas</th>\n",
       "      <th>weekend_device_home_areas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550791854002</td>\n",
       "      <td>{\"550790198003\":4,\"550790066002\":4,\"5507901410...</td>\n",
       "      <td>{\"550791858001\":9,\"550790099002\":9,\"5507900870...</td>\n",
       "      <td>{\"550790162001\":9,\"550790141001\":6,\"5507902000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550790033002</td>\n",
       "      <td>{\"550790902002\":4,\"550790034003\":4}</td>\n",
       "      <td>{\"550790015002\":9,\"550790033002\":5,\"5507900340...</td>\n",
       "      <td>{\"210730706003\":6,\"550790038001\":5,\"2916947028...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           area                    work_behavior_device_home_areas  \\\n",
       "0  550791854002  {\"550790198003\":4,\"550790066002\":4,\"5507901410...   \n",
       "1  550790033002                {\"550790902002\":4,\"550790034003\":4}   \n",
       "\n",
       "                           weekday_device_home_areas  \\\n",
       "0  {\"550791858001\":9,\"550790099002\":9,\"5507900870...   \n",
       "1  {\"550790015002\":9,\"550790033002\":5,\"5507900340...   \n",
       "\n",
       "                           weekend_device_home_areas  \n",
       "0  {\"550790162001\":9,\"550790141001\":6,\"5507902000...  \n",
       "1  {\"210730706003\":6,\"550790038001\":5,\"2916947028...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_2020_09.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Convert O to D\n",
    " - Now, the area column means the destination area (cbg). and other columns shows the number of people who is from cbg to 'area'\n",
    " - So we need to convert origin to dest, which means we convert area into origin area, and convert other columns into dest cbg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c13cacc3d724cb5815eb80ad92a2d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1. Work behavior...:   0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9f0909ff774dc8ba62e62bf0e59e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2) Other behaviors - weekday...:   0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62040564ea51441aa142081bbf00e6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3) Other behaviors - weekend...:   0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neighbor_2020_09_converted = DOtoOD(neighbor_2020_09, print_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>work_behavior_from_area</th>\n",
       "      <th>weekday_device_from_area_home</th>\n",
       "      <th>weekend_device_from_area_home</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550791854002</td>\n",
       "      <td>{'550790042003': 4, '550790042002': 4}</td>\n",
       "      <td>{'550791854002': 6, '550790201003': 4, '550790...</td>\n",
       "      <td>{'550791854002': 4, '550791002003': 4, '550790...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550790033002</td>\n",
       "      <td>{'550790040003': 4, '550790019004': 4, '550790...</td>\n",
       "      <td>{'550790033002': 5, '550790051002': 4, '550790...</td>\n",
       "      <td>{'550791854002': 4, '550791009002': 5, '550790...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           area                            work_behavior_from_area  \\\n",
       "0  550791854002             {'550790042003': 4, '550790042002': 4}   \n",
       "1  550790033002  {'550790040003': 4, '550790019004': 4, '550790...   \n",
       "\n",
       "                       weekday_device_from_area_home  \\\n",
       "0  {'550791854002': 6, '550790201003': 4, '550790...   \n",
       "1  {'550790033002': 5, '550790051002': 4, '550790...   \n",
       "\n",
       "                       weekend_device_from_area_home  \n",
       "0  {'550791854002': 4, '550791002003': 4, '550790...  \n",
       "1  {'550791854002': 4, '550791009002': 5, '550790...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_2020_09_converted.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Computing probability of trips from origin cbg to dest cbg\n",
    " - Ws and Wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/data/Chapter_3_data/\\Analysis/1_Preprocessed_Parcel_data/'\n",
    "landUse = gpd.read_file(path + 'Milwaukee_parcels.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- W_s: 0, W_d: 0-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1) Probability A to A_i...1 (add k folmula):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2) Probability A to A_i...2 (add Ws weight):   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2.1) Indexing :   0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2.2) add Ws weight :   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3) Probability A to A_i...3 (add W_d weight):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4) Filling empty values:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prob_trips_2020_09_ws0_wd0 = compute_probabilityByk_Ws_Wd(neighbor_2020_09_converted[0:3], landUse, W_s = 0, W_d = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>weekday_Work</th>\n",
       "      <th>weekend_Work</th>\n",
       "      <th>weekday_School</th>\n",
       "      <th>weekday_University</th>\n",
       "      <th>weekday_Dailycare</th>\n",
       "      <th>weekday_Religion</th>\n",
       "      <th>weekday_Large_shop</th>\n",
       "      <th>weekday_Etc_shop</th>\n",
       "      <th>weekday_Meals</th>\n",
       "      <th>...</th>\n",
       "      <th>weekend_University</th>\n",
       "      <th>weekend_Dailycare</th>\n",
       "      <th>weekend_Religion</th>\n",
       "      <th>weekend_Large_shop</th>\n",
       "      <th>weekend_Etc_shop</th>\n",
       "      <th>weekend_Meals</th>\n",
       "      <th>weekend_V_fr_rel</th>\n",
       "      <th>weekend_Rec_lei</th>\n",
       "      <th>weekend_Serv_trip</th>\n",
       "      <th>weekend_Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550791854002</td>\n",
       "      <td>{'550790042003': 0.5, '550790042002': 0.5}</td>\n",
       "      <td>{'550790042003': 0.5, '550790042002': 0.5}</td>\n",
       "      <td>{'550791854002': 0.06122, '550790201003': 0.04...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'550791854002': 0.06122, '550790201003': 0.04...</td>\n",
       "      <td>{'550791854002': 0.06122, '550790201003': 0.04...</td>\n",
       "      <td>{'550791854002': 0.06122, '550790201003': 0.04...</td>\n",
       "      <td>{'550791854002': 0.06122, '550790201003': 0.04...</td>\n",
       "      <td>{'550791854002': 0.06122, '550790201003': 0.04...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'550791854002': 0.07692, '550791002003': 0.07...</td>\n",
       "      <td>{'550791854002': 0.07692, '550791002003': 0.07...</td>\n",
       "      <td>{'550791854002': 0.07692, '550791002003': 0.07...</td>\n",
       "      <td>{'550791854002': 0.07692, '550791002003': 0.07...</td>\n",
       "      <td>{'550791854002': 0.07692, '550791002003': 0.07...</td>\n",
       "      <td>{'550791854002': 0.07692, '550791002003': 0.07...</td>\n",
       "      <td>{'550791854002': 0.07692, '550791002003': 0.07...</td>\n",
       "      <td>{'550791854002': 0.07692, '550791002003': 0.07...</td>\n",
       "      <td>{'550791854002': 0.07692, '550791002003': 0.07...</td>\n",
       "      <td>{'550791854002': 0.07692, '550791002003': 0.07...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           area                                weekday_Work  \\\n",
       "0  550791854002  {'550790042003': 0.5, '550790042002': 0.5}   \n",
       "\n",
       "                                 weekend_Work  \\\n",
       "0  {'550790042003': 0.5, '550790042002': 0.5}   \n",
       "\n",
       "                                      weekday_School weekday_University  \\\n",
       "0  {'550791854002': 0.06122, '550790201003': 0.04...                 {}   \n",
       "\n",
       "                                   weekday_Dailycare  \\\n",
       "0  {'550791854002': 0.06122, '550790201003': 0.04...   \n",
       "\n",
       "                                    weekday_Religion  \\\n",
       "0  {'550791854002': 0.06122, '550790201003': 0.04...   \n",
       "\n",
       "                                  weekday_Large_shop  \\\n",
       "0  {'550791854002': 0.06122, '550790201003': 0.04...   \n",
       "\n",
       "                                    weekday_Etc_shop  \\\n",
       "0  {'550791854002': 0.06122, '550790201003': 0.04...   \n",
       "\n",
       "                                       weekday_Meals  ...  \\\n",
       "0  {'550791854002': 0.06122, '550790201003': 0.04...  ...   \n",
       "\n",
       "                                  weekend_University  \\\n",
       "0  {'550791854002': 0.07692, '550791002003': 0.07...   \n",
       "\n",
       "                                   weekend_Dailycare  \\\n",
       "0  {'550791854002': 0.07692, '550791002003': 0.07...   \n",
       "\n",
       "                                    weekend_Religion  \\\n",
       "0  {'550791854002': 0.07692, '550791002003': 0.07...   \n",
       "\n",
       "                                  weekend_Large_shop  \\\n",
       "0  {'550791854002': 0.07692, '550791002003': 0.07...   \n",
       "\n",
       "                                    weekend_Etc_shop  \\\n",
       "0  {'550791854002': 0.07692, '550791002003': 0.07...   \n",
       "\n",
       "                                       weekend_Meals  \\\n",
       "0  {'550791854002': 0.07692, '550791002003': 0.07...   \n",
       "\n",
       "                                    weekend_V_fr_rel  \\\n",
       "0  {'550791854002': 0.07692, '550791002003': 0.07...   \n",
       "\n",
       "                                     weekend_Rec_lei  \\\n",
       "0  {'550791854002': 0.07692, '550791002003': 0.07...   \n",
       "\n",
       "                                   weekend_Serv_trip  \\\n",
       "0  {'550791854002': 0.07692, '550791002003': 0.07...   \n",
       "\n",
       "                                      weekend_Others  \n",
       "0  {'550791854002': 0.07692, '550791002003': 0.07...  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_trips_2020_09_ws0_wd0.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Combine all probability tables by their month\n",
    "  - First, make the tables with all possible ws and wd that user want to set\n",
    "  - Add columns of Ws and Wd\n",
    "  - Save to directory and merge all of those by the Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "probPath = 'E:/data/Chapter_3_data/Analysis/1_Preprocessed_prob_CBG_fromTo/Combined_by_Month/'\n",
    "prob_2020_09 = pd.read_csv(probPath + 'prob_2020_09_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>weekday_Work</th>\n",
       "      <th>weekend_Work</th>\n",
       "      <th>weekday_School</th>\n",
       "      <th>weekday_University</th>\n",
       "      <th>weekday_Dailycare</th>\n",
       "      <th>weekday_Religion</th>\n",
       "      <th>weekday_Large_shop</th>\n",
       "      <th>weekday_Etc_shop</th>\n",
       "      <th>weekday_Meals</th>\n",
       "      <th>...</th>\n",
       "      <th>weekend_Religion</th>\n",
       "      <th>weekend_Large_shop</th>\n",
       "      <th>weekend_Etc_shop</th>\n",
       "      <th>weekend_Meals</th>\n",
       "      <th>weekend_V_fr_rel</th>\n",
       "      <th>weekend_Rec_lei</th>\n",
       "      <th>weekend_Serv_trip</th>\n",
       "      <th>weekend_Others</th>\n",
       "      <th>Ws</th>\n",
       "      <th>Wd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550791854002</td>\n",
       "      <td>{'550790042003': 0.36955, '550790042002': 0.63...</td>\n",
       "      <td>{'550790042003': 0.36955, '550790042002': 0.63...</td>\n",
       "      <td>{'550791854002': 0.2124, '550790201003': 0.017...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'550791854002': 0.3265, '550790201003': 0.027...</td>\n",
       "      <td>{'550791854002': 0.20281, '550790084002': 0.04...</td>\n",
       "      <td>{'550791101002': 0.35012, '550790044001': 0.47...</td>\n",
       "      <td>{'550791854002': 0.21749, '550790032001': 0.01...</td>\n",
       "      <td>{'550791854002': 0.23899, '550790201003': 0.03...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'550791854002': 0.22481, '550790091002': 0.17...</td>\n",
       "      <td>{'550791101003': 1.0}</td>\n",
       "      <td>{'550791854002': 0.35874, '550790144002': 0.22...</td>\n",
       "      <td>{'550791854002': 0.29982, '550790091002': 0.25...</td>\n",
       "      <td>{'550791854002': 0.1629, '550791002003': 0.033...</td>\n",
       "      <td>{'550791854002': 0.25523, '550790144002': 0.14...</td>\n",
       "      <td>{'550791854002': 0.23391, '550790091002': 0.13...</td>\n",
       "      <td>{'550791854002': 0.31536, '550791002003': 0.02...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550790033002</td>\n",
       "      <td>{'550790040003': 0.12963, '550790019004': 0.10...</td>\n",
       "      <td>{'550790040003': 0.12963, '550790019004': 0.10...</td>\n",
       "      <td>{'550790034001': 0.26269, '550790602001': 0.02...</td>\n",
       "      <td>{'550790601012': 0.15189, '550790141001': 0.16...</td>\n",
       "      <td>{'550791201022': 0.00303, '550790050003': 0.07...</td>\n",
       "      <td>{'550790051002': 0.02014, '550790039003': 0.01...</td>\n",
       "      <td>{'550790001021': 0.02615, '550791201022': 0.00...</td>\n",
       "      <td>{'550790033002': 0.03647, '550790034001': 0.06...</td>\n",
       "      <td>{'550790033002': 0.03589, '550790065003': 0.00...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'550791854002': 0.01933, '550790051002': 0.03...</td>\n",
       "      <td>{'550791501004': 0.00128, '550791101002': 0.01...</td>\n",
       "      <td>{'550791854002': 0.01095, '550791009002': 0.01...</td>\n",
       "      <td>{'550791854002': 0.00804, '550791009002': 0.00...</td>\n",
       "      <td>{'550791854002': 0.00578, '550791009002': 0.00...</td>\n",
       "      <td>{'550791854002': 0.00862, '550791009002': 0.01...</td>\n",
       "      <td>{'550791854002': 0.0121, '550791009002': 0.007...</td>\n",
       "      <td>{'550791854002': 0.02209, '550791009002': 0.00...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>550790908001</td>\n",
       "      <td>{'550790903002': 1.0}</td>\n",
       "      <td>{'550790903002': 1.0}</td>\n",
       "      <td>{'550790127001': 0.05153, '550790907002': 0.07...</td>\n",
       "      <td>{'550791501002': 0.00509, '550791863002': 0.03...</td>\n",
       "      <td>{'550791503031': 0.00375, '550791863002': 0.03...</td>\n",
       "      <td>{'550790054003': 0.06015, '550791503031': 0.00...</td>\n",
       "      <td>{'550791501002': 0.00431, '550791503031': 0.00...</td>\n",
       "      <td>{'550790908001': 0.10723, '550790912002': 0.04...</td>\n",
       "      <td>{'550790912002': 0.07258, '550791202031': 0.00...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'550791301002': 0.00569, '550790054003': 0.08...</td>\n",
       "      <td>{'550791101002': 0.03894, '550790902002': 0.17...</td>\n",
       "      <td>{'550790908001': 0.09475, '550791301002': 0.01...</td>\n",
       "      <td>{'550791101002': 0.0122, '550790912002': 0.068...</td>\n",
       "      <td>{'550790908001': 0.07794, '550791301002': 0.00...</td>\n",
       "      <td>{'550791301002': 0.00921, '550790054003': 0.03...</td>\n",
       "      <td>{'550790908001': 0.14125, '550791301002': 0.00...</td>\n",
       "      <td>{'550790908001': 0.03699, '550791301002': 0.00...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           area                                       weekday_Work  \\\n",
       "0  550791854002  {'550790042003': 0.36955, '550790042002': 0.63...   \n",
       "1  550790033002  {'550790040003': 0.12963, '550790019004': 0.10...   \n",
       "2  550790908001                              {'550790903002': 1.0}   \n",
       "\n",
       "                                        weekend_Work  \\\n",
       "0  {'550790042003': 0.36955, '550790042002': 0.63...   \n",
       "1  {'550790040003': 0.12963, '550790019004': 0.10...   \n",
       "2                              {'550790903002': 1.0}   \n",
       "\n",
       "                                      weekday_School  \\\n",
       "0  {'550791854002': 0.2124, '550790201003': 0.017...   \n",
       "1  {'550790034001': 0.26269, '550790602001': 0.02...   \n",
       "2  {'550790127001': 0.05153, '550790907002': 0.07...   \n",
       "\n",
       "                                  weekday_University  \\\n",
       "0                                                 {}   \n",
       "1  {'550790601012': 0.15189, '550790141001': 0.16...   \n",
       "2  {'550791501002': 0.00509, '550791863002': 0.03...   \n",
       "\n",
       "                                   weekday_Dailycare  \\\n",
       "0  {'550791854002': 0.3265, '550790201003': 0.027...   \n",
       "1  {'550791201022': 0.00303, '550790050003': 0.07...   \n",
       "2  {'550791503031': 0.00375, '550791863002': 0.03...   \n",
       "\n",
       "                                    weekday_Religion  \\\n",
       "0  {'550791854002': 0.20281, '550790084002': 0.04...   \n",
       "1  {'550790051002': 0.02014, '550790039003': 0.01...   \n",
       "2  {'550790054003': 0.06015, '550791503031': 0.00...   \n",
       "\n",
       "                                  weekday_Large_shop  \\\n",
       "0  {'550791101002': 0.35012, '550790044001': 0.47...   \n",
       "1  {'550790001021': 0.02615, '550791201022': 0.00...   \n",
       "2  {'550791501002': 0.00431, '550791503031': 0.00...   \n",
       "\n",
       "                                    weekday_Etc_shop  \\\n",
       "0  {'550791854002': 0.21749, '550790032001': 0.01...   \n",
       "1  {'550790033002': 0.03647, '550790034001': 0.06...   \n",
       "2  {'550790908001': 0.10723, '550790912002': 0.04...   \n",
       "\n",
       "                                       weekday_Meals  ...  \\\n",
       "0  {'550791854002': 0.23899, '550790201003': 0.03...  ...   \n",
       "1  {'550790033002': 0.03589, '550790065003': 0.00...  ...   \n",
       "2  {'550790912002': 0.07258, '550791202031': 0.00...  ...   \n",
       "\n",
       "                                    weekend_Religion  \\\n",
       "0  {'550791854002': 0.22481, '550790091002': 0.17...   \n",
       "1  {'550791854002': 0.01933, '550790051002': 0.03...   \n",
       "2  {'550791301002': 0.00569, '550790054003': 0.08...   \n",
       "\n",
       "                                  weekend_Large_shop  \\\n",
       "0                              {'550791101003': 1.0}   \n",
       "1  {'550791501004': 0.00128, '550791101002': 0.01...   \n",
       "2  {'550791101002': 0.03894, '550790902002': 0.17...   \n",
       "\n",
       "                                    weekend_Etc_shop  \\\n",
       "0  {'550791854002': 0.35874, '550790144002': 0.22...   \n",
       "1  {'550791854002': 0.01095, '550791009002': 0.01...   \n",
       "2  {'550790908001': 0.09475, '550791301002': 0.01...   \n",
       "\n",
       "                                       weekend_Meals  \\\n",
       "0  {'550791854002': 0.29982, '550790091002': 0.25...   \n",
       "1  {'550791854002': 0.00804, '550791009002': 0.00...   \n",
       "2  {'550791101002': 0.0122, '550790912002': 0.068...   \n",
       "\n",
       "                                    weekend_V_fr_rel  \\\n",
       "0  {'550791854002': 0.1629, '550791002003': 0.033...   \n",
       "1  {'550791854002': 0.00578, '550791009002': 0.00...   \n",
       "2  {'550790908001': 0.07794, '550791301002': 0.00...   \n",
       "\n",
       "                                     weekend_Rec_lei  \\\n",
       "0  {'550791854002': 0.25523, '550790144002': 0.14...   \n",
       "1  {'550791854002': 0.00862, '550791009002': 0.01...   \n",
       "2  {'550791301002': 0.00921, '550790054003': 0.03...   \n",
       "\n",
       "                                   weekend_Serv_trip  \\\n",
       "0  {'550791854002': 0.23391, '550790091002': 0.13...   \n",
       "1  {'550791854002': 0.0121, '550791009002': 0.007...   \n",
       "2  {'550790908001': 0.14125, '550791301002': 0.00...   \n",
       "\n",
       "                                      weekend_Others   Ws    Wd  \n",
       "0  {'550791854002': 0.31536, '550791002003': 0.02...  0.5  0.25  \n",
       "1  {'550791854002': 0.02209, '550791009002': 0.00...  0.5  0.25  \n",
       "2  {'550790908001': 0.03699, '550791301002': 0.00...  0.5  0.25  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g.,\n",
    "prob_2020_09.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) fill empty probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈것들 채워주는 코드\n",
    "\n",
    "def fill_values(row):\n",
    "    # weekday_Dailycare 컬럼 처리\n",
    "    if row['weekday_Dailycare'] == '{}':\n",
    "        # weekday_Religion, weekday_School 중에서 '{}'가 아닌 값을 찾아 채워주기\n",
    "        if row['weekday_Religion'] != '{}':\n",
    "            row['weekday_Dailycare'] = row['weekday_Religion']\n",
    "        elif row['weekday_School'] != '{}':\n",
    "            row['weekday_Dailycare'] = row['weekday_School']\n",
    "\n",
    "    if row['weekend_Dailycare'] == '{}':\n",
    "        # weekday_Religion, weekday_School 중에서 '{}'가 아닌 값을 찾아 채워주기\n",
    "        if row['weekday_Dailycare'] != '{}':\n",
    "            row['weekend_Dailycare'] = row['weekday_Dailycare']\n",
    "        elif row['weekend_Religion'] != '{}':\n",
    "            row['weekend_Dailycare'] = row['weekend_Religion']\n",
    "        elif row['weekend_School'] != '{}':\n",
    "            row['weekend_Dailycare'] = row['weekend_School']\n",
    "            \n",
    "    # weekday_Large_shop 컬럼 처리\n",
    "    if row['weekday_Large_shop'] == '{}':\n",
    "        # weekday_Etc_shop이 '{}'가 아니라면 값을 채워주기\n",
    "        if row['weekday_Etc_shop'] != '{}':\n",
    "            row['weekday_Large_shop'] = row['weekday_Etc_shop']\n",
    "\n",
    "    # weekday_Large_shop 컬럼 처리\n",
    "    if row['weekend_Large_shop'] == '{}':\n",
    "        # weekday_Etc_shop이 '{}'가 아니라면 값을 채워주기\n",
    "        if row['weekday_Large_shop'] != '{}':\n",
    "            row['weekend_Large_shop'] = row['weekday_Large_shop']\n",
    "        elif row['weekend_Etc_shop'] != '{}':\n",
    "            row['weekend_Large_shop'] = row['weekend_Etc_shop']\n",
    "        elif row['weekday_Etc_shop'] != '{}':\n",
    "            row['weekend_Large_shop'] = row['weekday_Etc_shop']\n",
    "            \n",
    "    # weekday_Large_shop 컬럼 처리\n",
    "    if row['weekend_Etc_shop'] == '{}':\n",
    "        # weekday_Etc_shop이 '{}'가 아니라면 값을 채워주기\n",
    "        if row['weekday_Etc_shop'] != '{}':\n",
    "            row['weekend_Etc_shop'] = row['weekday_Etc_shop']\n",
    "        elif row['weekend_Large_shop'] != '{}':\n",
    "            row['weekend_Etc_shop'] = row['weekend_Large_shop']\n",
    "\n",
    "    if row['weekday_Religion'] == '{}':\n",
    "        # weekday_Religion, weekday_School 중에서 '{}'가 아닌 값을 찾아 채워주기\n",
    "        if row['weekday_Dailycare'] != '{}':\n",
    "            row['weekday_Religion'] = row['weekday_Dailycare']\n",
    "        elif row['weekday_School'] != '{}':\n",
    "            row['weekday_Religion'] = row['weekday_School']\n",
    "            \n",
    "    if row['weekend_Religion'] == '{}':\n",
    "        # weekday_Religion, weekday_School 중에서 '{}'가 아닌 값을 찾아 채워주기\n",
    "        if row['weekday_Religion'] != '{}':\n",
    "            row['weekend_Religion'] = row['weekday_Religion']\n",
    "        elif row['weekend_Dailycare'] != '{}':\n",
    "            row['weekend_Religion'] = row['weekend_Dailycare']\n",
    "            \n",
    "    if row['weekend_School'] == '{}':\n",
    "        # weekday_Religion, weekday_School 중에서 '{}'가 아닌 값을 찾아 채워주기\n",
    "        if row['weekend_Dailycare'] != '{}':\n",
    "            row['weekend_School'] = row['weekend_Dailycare']\n",
    "        elif row['weekend_Religion'] != '{}':\n",
    "            row['weekend_School'] = row['weekend_Religion']            \n",
    "            \n",
    "    if row['weekend_Meals'] == '{}':\n",
    "        # weekday_Religion, weekday_School 중에서 '{}'가 아닌 값을 찾아 채워주기\n",
    "        if row['weekday_Meals'] != '{}':\n",
    "            row['weekend_Meals'] = row['weekday_Meals']\n",
    "        elif row['weekend_Etc_shop'] != '{}':\n",
    "            row['weekend_Meals'] = row['weekend_Etc_shop']              \n",
    "            \n",
    "    if row['weekend_Rec_lei'] == '{}':\n",
    "        # weekday_Religion, weekday_School 중에서 '{}'가 아닌 값을 찾아 채워주기\n",
    "        if row['weekday_Rec_lei'] != '{}':\n",
    "            row['weekend_Rec_lei'] = row['weekday_Rec_lei']   \n",
    "            \n",
    "    if row['weekend_Serv_trip'] == '{}':\n",
    "        # weekday_Religion, weekday_School 중에서 '{}'가 아닌 값을 찾아 채워주기\n",
    "        if row['weekday_Serv_trip'] != '{}':\n",
    "            row['weekend_Serv_trip'] = row['weekday_Serv_trip']\n",
    "            \n",
    "    if row['weekend_Others'] == '{}':\n",
    "        # weekday_Religion, weekday_School 중에서 '{}'가 아닌 값을 찾아 채워주기\n",
    "        if row['weekday_Others'] != '{}':\n",
    "            row['weekend_Others'] = row['weekday_Others']     \n",
    "            \n",
    "            \n",
    "            \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cebbe6d4b4945ce87f9129e86ab7589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 나중에 -> 다 가져와야 함. trip 별로 cbg -> cbg 확률들\n",
    "directory_path = \"E:/data/Chapter_3_data/Analysis/1_Preprocessed_prob_CBG_fromTo/Combined_by_Month\"\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# Load each CSV file and assign to a global variable\n",
    "for file in tqdm(csv_files):\n",
    "    # Create a variable name from the file name (without extension)\n",
    "    variable_name = file.split('.')[0]\n",
    "    # Load the CSV file\n",
    "    globals()[variable_name] = pd.read_csv(os.path.join(directory_path, file))\n",
    "    \n",
    "    globals()[variable_name] = globals()[variable_name].apply(fill_values, axis=1)\n",
    "    \n",
    "    globals()[variable_name].to_csv(directory_path + '/' + variable_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
