{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "# from tqdm import tqdmㅇ\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from shapely.geometry import Point\n",
    "from pyproj import Geod\n",
    "import gzip\n",
    "\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# 경고 메시지 무시 설정\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABTS Simulation code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trip Occurence Builder\n",
    "### 1.1. The probability of a person with age ‘a’ having ‘t’ trips occurs in a single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_prob_with_day(df, age_col, tripPurpose_col, travday_col):\n",
    "    \"\"\"\n",
    "    Calculate the probability of trip purpose by age group and day type (Weekday or Weekend) using Naive Bayes.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the NHTS data\n",
    "    - age_col: The name of the column representing age groups\n",
    "    - tripPurpose_col: The name of the column representing the trip purpose\n",
    "    - travday_col: The name of the column representing the day type (Weekday or Weekend)\n",
    "\n",
    "    Returns:\n",
    "    - result_df: A DataFrame with the calculated probabilities for each combination of age group, trip purpose, and day type\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    result_df = pd.DataFrame(columns=['Age', 'Trip_pur', 'Day_Type', 'Prob'])\n",
    "\n",
    "    # Extract unique values of each Age group, Trip type, and define day types\n",
    "    unique_ages = df[age_col].unique()\n",
    "    unique_trips = df[tripPurpose_col].unique()\n",
    "    day_types = ['Weekday', 'Weekend']\n",
    "    \n",
    "    # Total Population\n",
    "    total_pop = len(df)\n",
    "\n",
    "    # Loop over each Age group\n",
    "    for age in unique_ages:\n",
    "        age_group_pop = len(df[df[age_col] == age])\n",
    "        \n",
    "        # Calculate P(X_a / X) - Probability of being in age group 'a' in the total population\n",
    "        p_xa_x = age_group_pop / total_pop\n",
    "        \n",
    "        # Loop over each Trip type\n",
    "        for trip in unique_trips:\n",
    "            \n",
    "            # Also, loop over each Day Type (Weekday, Weekend)\n",
    "            for day_type in day_types:\n",
    "                \n",
    "                # Subset DataFrame based on day type\n",
    "                if day_type == 'Weekday':\n",
    "                    sub_df = df[df[travday_col] == 'Weekday']\n",
    "                else:  # For Weekend\n",
    "                    sub_df = df[df[travday_col] == 'Weekend']\n",
    "                \n",
    "                # Calculate P(θ_t / θ) - Probability of trip type 't' given the day type\n",
    "                total_trips = sub_df[tripPurpose_col].value_counts().sum()\n",
    "                p_theta_t_theta = sub_df[sub_df[tripPurpose_col] == trip].shape[0] / total_trips\n",
    "                \n",
    "                # Calculate P(X_a | θ_t) - Probability of being in age group 'a' given the trip type 't'\n",
    "                p_xa_thetat = len(sub_df[(sub_df[age_col] == age) & (sub_df[tripPurpose_col] == trip)]) / sub_df[sub_df[tripPurpose_col] == trip].shape[0]\n",
    "                \n",
    "                # Calculate Naive Bayes probability\n",
    "                prob = (p_xa_thetat * p_theta_t_theta) / p_xa_x\n",
    "                \n",
    "                # Setting the result column name based on the day type\n",
    "                col_name = 'WD_prob' if day_type == 'Weekday' else 'WK_prob'\n",
    "                \n",
    "                # Add the result to the DataFrame using concat\n",
    "                temp_df = pd.DataFrame([{'Age': age, 'Trip_pur': trip, 'Day_Type': day_type, 'Prob': prob}])\n",
    "                result_df = pd.concat([result_df, temp_df], ignore_index=True)\n",
    "                \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The number of trips occurring ‘k’ times for a single individual ‘i’ in a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trip_distribution(trippub_total, id_age, id_col, day_type_col, trip_purpose_col):\n",
    "    \"\"\"\n",
    "    Generate a distribution of trips based on unique IDs, age, day type, and trip purpose.\n",
    "\n",
    "    Parameters:\n",
    "    - trippub_total: DataFrame containing trip data (NHTS)\n",
    "    - id_age: The name of the column representing the age identifier\n",
    "    - id_col: The name of the column representing unique identifiers for individuals or entities\n",
    "    - day_type_col: The name of the column representing the type of the day (e.g., Weekday or Weekend)\n",
    "    - trip_purpose_col: The name of the column representing the purpose of the trip\n",
    "\n",
    "    Returns:\n",
    "    - count_series: A DataFrame that shows the count of trips for each combination of ID, age, day type, and trip purpose.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the input DataFrame to avoid modifying the original data\n",
    "    trip_total = trippub_total.copy()\n",
    "    \n",
    "    # Remove rows where the sum of DWELTIME for each combination of PERSONID_new and TRAVDAY_new is 0\n",
    "    sum_dweltime = trip_total.groupby(['uniqID', 'Day_Type'])['Dwell_T_min'].sum().reset_index()\n",
    "    valid_ids = sum_dweltime[sum_dweltime['Dwell_T_min'] != 0][['uniqID', 'Day_Type']]\n",
    "    trip_total = pd.merge(trip_total, valid_ids, on=['uniqID', 'Day_Type'])\n",
    "\n",
    "    # Aggregate the count of rows\n",
    "    count_series = trip_total.groupby([id_col, id_age, day_type_col, trip_purpose_col]).size().reset_index(name='count')\n",
    "    \n",
    "    # The following line is commented out as it seems redundant given that 'day_type_col' already specifies day type\n",
    "    # count_series['TRAVDAY_new'] = count_series[day_type_col].apply(lambda x: 'Weekday' if x == 'Weekday' else 'Weekend')\n",
    "    \n",
    "    return count_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combined_trip_count(naive_prob, trip_count, age_n_dict, method, Home_cbg, W_k_weekday=1.0, W_k_weekend=1.0,\n",
    "                                 W_t_weekday=None, W_t_weekend=None, print_progress=True):\n",
    "    \"\"\"\n",
    "    Generate a combined trip count distribution based on age groups, day type, and trip purpose.\n",
    "    \n",
    "    Parameters:\n",
    "    - naive_prob: DataFrame containing Naive Bayes probabilities.\n",
    "    - trip_count: DataFrame with trip count data.\n",
    "    - age_n_dict: Dictionary mapping age groups to the number of individuals.\n",
    "    - method: String specifying the method to distribute trips ('multinomial' or 'cdf').\n",
    "    - Home_cbg: The home census block group id.\n",
    "    - W_k_weekday: Weight multiplier for trip counts on weekdays.\n",
    "    - W_k_weekend: Weight multiplier for trip counts on weekends.\n",
    "    - W_t_weekday: Dictionary with trip purpose as keys and weights as values for weekdays.\n",
    "    - W_t_weekend: Dictionary with trip purpose as keys and weights as values for weekends.\n",
    "    - print_progress: Boolean flag to print progress.\n",
    "    \n",
    "    Returns:\n",
    "    - combined_result_df: DataFrame with the generated trip distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Print initial message if progress printing is enabled\n",
    "    if print_progress:\n",
    "        print('<Trip occurrence builder>')\n",
    "    \n",
    "    # Initialize weight dictionaries if not provided\n",
    "    if W_t_weekday is None:\n",
    "        W_t_weekday = {}  # Default weekday weights as empty dict\n",
    "    if W_t_weekend is None:\n",
    "        W_t_weekend = {}  # Default weekend weights as empty dict\n",
    "        \n",
    "    \n",
    "    # Create an empty DataFrame to store results\n",
    "    combined_result_df = pd.DataFrame()\n",
    "\n",
    "    # Define extended day types to include specific days of the week\n",
    "    extended_day_types = {\n",
    "        'Weekday': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n",
    "        'Weekend': ['Saturday', 'Sunday']\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Define a nested function to generate trip counts for each age group and day type\n",
    "    def generate_trip_count(naive_result_total, trip_count, age_group, n, method, extended_day_types, start_id=0):\n",
    "        result_list = []  # Initialize an empty list to store intermediate results\n",
    "\n",
    "        # Iterate over each base day type and its actual days\n",
    "        for base_day_type, actual_days in extended_day_types.items():\n",
    "            # Filter trip counts for the current age group and base day type\n",
    "            trip_count_df = trip_count[(trip_count['age_class'] == age_group) & (trip_count['Day_Type'] == base_day_type)]\n",
    "            # Aggregate trip counts by person\n",
    "            trip_count_by_person = trip_count_df.groupby(['uniqID']).agg({'count': 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "            # Filter Naive Bayes probabilities for the current age group and base day type\n",
    "            prob_df = naive_result_total[(naive_result_total['Age'] == age_group) & (naive_result_total['Day_Type'] == base_day_type)]\n",
    "\n",
    "            # Iterate over each actual day\n",
    "            for actual_day in actual_days:\n",
    "                # Repeat process 'n' times for simulation\n",
    "                for i in range(n):\n",
    "                    current_id = start_id + i  # Unique identifier for each simulation iteration\n",
    "                    \n",
    "                    # Determine trip count multiplier based on day type\n",
    "                    if actual_day in ['Sunday', 'Saturday']:  # Weekend\n",
    "                        theta_i = np.random.choice(trip_count_by_person['count']) * W_k_weekend\n",
    "                    else:  # Weekday\n",
    "                        theta_i = np.random.choice(trip_count_by_person['count']) * W_k_weekday\n",
    "                    \n",
    "                    # Adjust probabilities with weights if they exist for the trip purpose\n",
    "                    adjusted_prob_df = prob_df.copy()\n",
    "                    \n",
    "                    if actual_day == 'Sunday' or actual_day == 'Saturday': # weekend\n",
    "                        for trip_purpose, weight in W_t_weekday.items():\n",
    "                            adjusted_prob_df.loc[adjusted_prob_df['Trip_pur'] == trip_purpose, 'Prob'] *= weight\n",
    "                    else:\n",
    "                        for trip_purpose, weight in W_t_weekend.items():\n",
    "                            adjusted_prob_df.loc[adjusted_prob_df['Trip_pur'] == trip_purpose, 'Prob'] *= weight\n",
    "\n",
    "                    # Normalize probabilities again after weighting\n",
    "                    adjusted_prob_df['Prob'] /= adjusted_prob_df['Prob'].sum()\n",
    "\n",
    "                    if method == 'multinomial':\n",
    "                        trips = np.random.multinomial(theta_i, adjusted_prob_df['Prob'])\n",
    "                    elif method == 'cdf':\n",
    "                        cdf = np.cumsum(adjusted_prob_df['Prob'])\n",
    "                        trips = np.histogram(np.random.rand(theta_i), bins=[0] + list(cdf), range=(0,1))[0]\n",
    "                    \n",
    "                    \n",
    "                    home_count = trips[prob_df['Trip_pur'] == 'Home'][0]\n",
    "                    \n",
    "                    # Append results for each trip purpose\n",
    "                    for t, count in zip(prob_df['Trip_pur'], trips):\n",
    "                        result_list.append({'uniqID': current_id, 'ageGroup': age_group, 'Day_Type': actual_day, 'Week_Type': base_day_type, 'TRPPURP': t, 'count': count})\n",
    "\n",
    "        # Convert the list of results into a DataFrame\n",
    "        result_df = pd.DataFrame(result_list)\n",
    "        return result_df\n",
    "\n",
    "    # Initialize the starting unique ID for simulation\n",
    "    current_max_id = 0\n",
    "\n",
    "    # Decide whether to use tqdm for progress indication based on the print_progress flag\n",
    "    iterator = tqdm(age_n_dict.items(), desc='1. Processing age groups to generate trip counts') if print_progress else age_n_dict.items()\n",
    "\n",
    "    # Iterate over each age group and its associated number of individuals\n",
    "    for age_group, n in iterator:\n",
    "        temp_df = generate_trip_count(naive_prob, trip_count, age_group, n, method, extended_day_types, current_max_id)\n",
    "        combined_result_df = pd.concat([combined_result_df, temp_df], ignore_index=True)\n",
    "        current_max_id += n  # Update the current_max_id for the next batch\n",
    "\n",
    "    if print_progress:\n",
    "        tqdm.pandas(desc=\"2. Adjust home counts\")\n",
    "    \n",
    "    # Function to adjust home count in each group\n",
    "    def adjust_home_count(group):\n",
    "        total_count = group['count'].sum()\n",
    "        home_count_row = group[group['TRPPURP'] == 'Home']\n",
    "\n",
    "        # Ensure that there are at least 2 home counts if total count is 2 or more\n",
    "        if total_count >= 2 and home_count_row['count'].iloc[0] < 2:\n",
    "            group.loc[group['TRPPURP'] == 'Home', 'count'] = 2\n",
    "        return group\n",
    "\n",
    "    # Apply the function to adjust home counts\n",
    "    if print_progress:\n",
    "        combined_result_df = combined_result_df.groupby(['uniqID', 'Day_Type']).progress_apply(adjust_home_count).reset_index(drop=True)\n",
    "    else:\n",
    "        combined_result_df = combined_result_df.groupby(['uniqID', 'Day_Type']).apply(adjust_home_count).reset_index(drop=True)\n",
    "\n",
    "    # Assign the Home_cbg to all rows\n",
    "    combined_result_df[\"Home_cbg\"] = Home_cbg\n",
    "    \n",
    "    # columns: Wt amd Wk\n",
    "    \n",
    "    \n",
    "    # Function to retrieve weight for a given trip purpose from the weight dictionaries\n",
    "    def get_weight(trppurp, W_t):\n",
    "        return W_t.get(trppurp, 1.0)  # Return 1.0 if the trip purpose is not in the dictionary\n",
    "\n",
    "    # Assign weekday and weekend trip count weights to all rows\n",
    "    combined_result_df['Wk_wD'] = W_k_weekday\n",
    "    combined_result_df['Wk_wK'] = W_k_weekend\n",
    "\n",
    "    # Apply the get_weight function to assign trip purpose weights for weekdays and weekends\n",
    "    combined_result_df['Wt_wD'] = combined_result_df['TRPPURP'].apply(lambda x: get_weight(x, W_t_weekday))\n",
    "    combined_result_df['Wt_wK'] = combined_result_df['TRPPURP'].apply(lambda x: get_weight(x, W_t_weekend))\n",
    "\n",
    "    return combined_result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Trip Chains Builder\n",
    "### 2.0. preprocessing: Create trip seqeunce using origin NHTS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trip_sequence(df, print_progress=True):\n",
    "    \"\"\"\n",
    "    Creates a trip sequence column by concatenating Trip_pur values for each group defined by age_class, Day_Type, and uniqID.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing trip data (NHTS).\n",
    "    - print_progress: Boolean indicating whether to print progress information.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with an added column 'Trip_sequence' representing the sequence of trip purposes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to aggregate counts after trip sequence generation\n",
    "    def aggregate_count(df):\n",
    "        # Group by relevant columns and aggregate 'count' values\n",
    "        aggregated_df = df.groupby(['age_class', 'Day_Type', 'uniqID', 'Trip_sequence']).agg({'count': 'sum'}).reset_index()\n",
    "        return aggregated_df\n",
    "    \n",
    "    # Sort the DataFrame by start time to ensure trip sequence is chronological\n",
    "    sorted_df = df.sort_values('sta_T_hms')\n",
    "    \n",
    "    # Group the sorted DataFrame by age class, day type, and unique ID\n",
    "    grouped = sorted_df.groupby(['age_class', 'Day_Type', 'uniqID'])\n",
    "    \n",
    "    # Initialize a list to store result data\n",
    "    result_data = []\n",
    "    \n",
    "#     print(\"create trip sequence...\")\n",
    "\n",
    "    # Check if progress should be printed, wrap the iterator with tqdm if true\n",
    "    iterator = tqdm(grouped, desc='Creating trip sequences derived from data...') if print_progress else grouped\n",
    "\n",
    "    # Iterate through each group\n",
    "    for name, group in iterator:\n",
    "        # Concatenate the 'Trip_pur' values to form the trip sequence\n",
    "        trip_sequence = '-'.join(group['Trip_pur'])\n",
    "        # Append the result as a dictionary to the result_data list\n",
    "        result_data.append({\n",
    "            'age_class': name[0],\n",
    "            'Day_Type': name[1],\n",
    "            'uniqID': name[2],\n",
    "            'Trip_sequence': trip_sequence,\n",
    "            # Set initial count to 1 for each unique sequence\n",
    "            'count': 1\n",
    "        })\n",
    "    \n",
    "    # Convert the result_data list to a DataFrame\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "    \n",
    "    # Aggregate counts in case there are duplicate sequences\n",
    "    result_df = aggregate_count(result_df)\n",
    "    \n",
    "    # Return the final DataFrame with aggregated trip sequences\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Finding optimal origin sequence O_i most similar to S_i\n",
    "### 2.2) Randomly assign the trip sequence for S_i\n",
    "### 2.3) Reassign the sequence of trip in S_i based on O_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTripSequence(trip_sequence_simul, trip_sequence_origin, print_progress=True):\n",
    "    \"\"\"\n",
    "    Constructs and refines trip sequences for simulated data to ensure realistic patterns by aligning them with sequences from original NHTS data. \n",
    "    The process includes adjusting 'Home' trip counts to reflect real-world patterns, duplicating rows based on trip counts, \n",
    "    finding and assigning the most similar trip sequence from NHTS data, and sequentially refining trip sequences to eliminate inconsistencies \n",
    "    such as duplicated or consecutive 'Home' trips and ensuring all trip sequences start and end with 'Home'. \n",
    "    The function applies several passes of adjustments to achieve coherent and logically ordered trip sequences that realistically simulate daily travel behavior.\n",
    "\n",
    "    Parameters:\n",
    "    - trip_sequence_simul: DataFrame containing simulated trip data that needs sequence construction and adjustment.\n",
    "    - trip_sequence_origin: DataFrame with original trip sequences from NHTS data, used as a reference for similarity comparison.\n",
    "    - print_progress: Boolean flag indicating whether to print progress messages and bars during the execution.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with refined trip sequences for each individual and day, ready for further analysis or simulation tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Print initial message if progress printing is enabled\n",
    "    if print_progress:\n",
    "        print('<Trip chain builder>')\n",
    "    \n",
    "    # Adjust home count in the simulated trip sequences to match real trip patterns\n",
    "    tqdm.pandas(desc=\"1. Split trips purpose to each row\")\n",
    "    def adjust_home_count(group):\n",
    "        # Calculate the sum of non-home trip counts and adjust home count accordingly\n",
    "        non_home_count_sum = group.loc[group['TRPPURP'] != 'Home', 'count'].sum()\n",
    "        new_home_count = non_home_count_sum + 1\n",
    "\n",
    "        # Adjust the home count in the DataFrame\n",
    "        home_idx = group.loc[group['TRPPURP'] == 'Home'].index\n",
    "        if len(home_idx) > 0:  # If there is a home trip\n",
    "            home_idx = home_idx[0]\n",
    "            if group.at[home_idx, 'count'] > new_home_count:\n",
    "                group.at[home_idx, 'count'] = new_home_count\n",
    "            group = group[group['count'] > 0]\n",
    "\n",
    "        # Duplicate rows based on the 'count' value to create individual trip records\n",
    "        new_rows = []\n",
    "        for _, row in group.iterrows():\n",
    "            new_rows.extend([row] * int(row['count']))  # Duplicate row 'count' times\n",
    "        new_group = pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "        new_group['count'] = 1  # Reset count to 1 for all rows\n",
    "\n",
    "        return new_group\n",
    "\n",
    "    # Apply adjust_home_count function to each group\n",
    "    if print_progress == True:\n",
    "        adjusted_simul_df = trip_sequence_simul.groupby(['Day_Type', 'uniqID']).progress_apply(adjust_home_count).reset_index(drop=True)\n",
    "    else:\n",
    "        adjusted_simul_df = trip_sequence_simul.groupby(['Day_Type', 'uniqID']).apply(adjust_home_count).reset_index(drop=True)\n",
    "#     --------------------#\n",
    "    \n",
    "    tqdm.pandas(desc=\"2. Find similar sequence from NHTS data\") \n",
    "    \n",
    "    def find_most_similar_sequence(query_seq, available_seqs): #1. find trip sequence in origin NHTS, similar to the simulated trips\n",
    "        \n",
    "        max_similarity = 0\n",
    "        most_similar = None\n",
    "\n",
    "        for seq in available_seqs:\n",
    "            \n",
    "            similarity = sum(x == y for x, y in zip(query_seq, seq)) # Calculate string similarity\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                most_similar = seq\n",
    "\n",
    "        return most_similar\n",
    "\n",
    "    \n",
    "    unique_trip_seqs = trip_sequence_origin['Trip_sequence'].unique()\n",
    "    \n",
    "    def get_sequence(group):\n",
    "        # Assign the most similar trip sequence from NHTS data to each group\n",
    "        if len(group) == 1:\n",
    "            group['seq_similar_orig'] = 1\n",
    "            return group\n",
    "\n",
    "        query_seq = \"-\".join(group['TRPPURP'].tolist())\n",
    "#         print(query_seq)\n",
    "        \n",
    "        # 가장 비슷한 sequence 찾기\n",
    "        most_similar_seq = find_most_similar_sequence(query_seq, unique_trip_seqs)\n",
    "        group['seq_similar_orig'] = most_similar_seq\n",
    "        return group\n",
    "\n",
    "    # Apply get_sequence function to each group\n",
    "    if print_progress == True:\n",
    "        df_with_seq = adjusted_simul_df.groupby(['uniqID', 'ageGroup', 'Day_Type']).progress_apply(get_sequence).reset_index(drop=True)\n",
    "    else:\n",
    "        df_with_seq = adjusted_simul_df.groupby(['uniqID', 'ageGroup', 'Day_Type']).apply(get_sequence).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    def MakeSequence(df): \n",
    "        # This function constructs trip sequences for individuals based on their simulated trip purposes and the most similar trip sequence from the original data.\n",
    "\n",
    "        # Initialize an empty column for the sequence\n",
    "        df['sequence'] = None\n",
    "\n",
    "        # Define an iterator for grouping by unique IDs and Day_Type, to process each individual's trips per day\n",
    "        iterator = tqdm(df.groupby(['uniqID', 'Day_Type']), desc='3. Assign trip sequence to individuals') if print_progress else df.groupby(['uniqID', 'Day_Type'])\n",
    "    \n",
    "        for name, group in iterator:\n",
    "        # If the group size is 1, it implies there's only a single trip purpose, simplifying the sequence assignment\n",
    "            if  group['seq_similar_orig'].iloc[0] == 1:\n",
    "#                 group['sequence'].iloc[0] = 1\n",
    "                \n",
    "                df.loc[group.index, 'sequence'] = 1\n",
    "            else:\n",
    "                seq_similar_orig = group['seq_similar_orig'].iloc[0].split('-')\n",
    "                n = len(group)\n",
    "                sequence = []\n",
    "\n",
    "                # Count 'Home' trips and assign them to the first and last sequence positions\n",
    "                home_count = group['TRPPURP'].value_counts().get('Home', 0) \n",
    "                home_assigned = 0\n",
    "                for trppurp in group['TRPPURP']:\n",
    "                    if trppurp == 'Home':\n",
    "                        home_assigned += 1\n",
    "                        sequence.append(1 if home_assigned == 1 else n)   # Ensure 'Home' appears first and last in the sequence.\n",
    "\n",
    "                # Process remaining trip purposes not assigned as 'Home'\n",
    "                remaining = [trppurp for trppurp in group['TRPPURP'] if trppurp != 'Home']\n",
    "#                 print(seq_similar_orig)\n",
    "                remaining_indices = sorted(set(range(2, n)) - set(sequence))\n",
    "\n",
    "#                 display(group)             \n",
    "                \n",
    "                for trppurp in remaining: # Initially assign a random sequence to remaining trip purposes\n",
    "                    random_seq = random.choice(remaining_indices)\n",
    "                    remaining_indices.remove(random_seq)\n",
    "                    sequence.append(random_seq)\n",
    "\n",
    "                    \n",
    "                for idx in range(1, len(seq_similar_orig)): # Reassign sequence numbers based on the original sequence to maintain trip order\n",
    "                    prev_purpose, current_purpose = seq_similar_orig[idx-1], seq_similar_orig[idx]\n",
    "\n",
    "                    # prev_purpose의 sequence를 찾고, 그 다음 sequence를 current_purpose에 할당\n",
    "                    for seq_num, purp in sorted(list(enumerate(sequence)), key=lambda x: x[1]):\n",
    "                        if purp == prev_purpose:\n",
    "                            next_seq = seq_num + 1\n",
    "                            if next_seq in sequence:\n",
    "                                continue  # Skip if already assigned\n",
    "\n",
    "                            # Find and set the next sequence number for current trip purpose based on its predecessor\n",
    "                            for cur_seq_num, cur_purp in sorted(list(enumerate(sequence)), key=lambda x: x[1]):\n",
    "                                if cur_purp == current_purpose:\n",
    "                                    sequence[cur_seq_num] = next_seq\n",
    "                                    break\n",
    "\n",
    "                            break  # Stop after reassigning the current purpose   \n",
    "                \n",
    "                # Update the DataFrame with the newly assigned sequences\n",
    "                df.loc[group.index, 'sequence'] = sequence\n",
    "\n",
    "        return df\n",
    "    \n",
    "    df_with_seq = MakeSequence(df_with_seq)\n",
    "    \n",
    "\n",
    "    #-------------------------\n",
    "    \n",
    "    def print_4():\n",
    "        for i in tqdm(range(1), desc = '4. Organize and arrange tables'):\n",
    "        # This loop does nothing but show progress. It's a visual indicator and does not perform any data manipulation.\n",
    "            None\n",
    "    \n",
    "    if print_progress == True:\n",
    "        print_4()\n",
    "    \n",
    "    #---------------\n",
    "    \n",
    "    def removeDuplHome(df): \n",
    "        # This function removes duplicated 'Home' trip purposes that occur consecutively in the trip sequence, except for the first and last instance.\n",
    "        # It ensures that each trip sequence correctly reflects a realistic pattern of leaving from and returning to home at most once during the trip sequence.\n",
    "\n",
    "        \n",
    "        # Iterate through each group based on 'uniqID' and 'Day_Type'\n",
    "        iterator = tqdm(df.groupby(['uniqID', 'Day_Type']), desc = ' - 4.1. remove duplicated Home trip') if print_progress else df.groupby(['uniqID', 'Day_Type'])\n",
    "        \n",
    "        for name, group in iterator:\n",
    "\n",
    "            if len(group) == 1:\n",
    "                continue\n",
    "            # Calculate the number of 'Home' occurrences in seq_similar_orig\n",
    "            seq_similar_orig_count = group['seq_similar_orig'].iloc[0].split('-').count('Home') - 2  # Exclude first and last\n",
    "            if seq_similar_orig_count < 0:\n",
    "                seq_similar_orig_count = 0\n",
    "\n",
    "            # Get the count of 'Home' in the group\n",
    "            home_count = group['TRPPURP'].value_counts().get('Home', 0)\n",
    "\n",
    "            # Calculate how many 'Home' entries to remove, excluding the first and last\n",
    "            home_to_remove = home_count - 2 - seq_similar_orig_count\n",
    "\n",
    "            # Remove excess 'Home' occurrences\n",
    "            if home_to_remove > 0:\n",
    "                index_to_remove = group[group['TRPPURP'] == 'Home'].index[1:-1][:home_to_remove]\n",
    "                df.drop(index_to_remove, inplace=True)   \n",
    "\n",
    "        return df\n",
    "    \n",
    "    df_removed_dupl_Home = removeDuplHome(df_with_seq)\n",
    "    \n",
    "    \n",
    "    def setHomeSequence(df): \n",
    "        # Adjusts the sequence of 'Home' trips for each group of trips by a unique individual on a specific day. \n",
    "        # If there are exactly two 'Home' entries, their sequences are set to 1 and the last sequence number, ensuring that trips start and end at 'Home'.\n",
    "        \n",
    "        # Iterate through each group based on 'uniqID' and 'Day_Type'\n",
    "        iterator = tqdm(df.groupby(['uniqID', 'Day_Type']), desc = ' - 4.2. reassign Home trip sequence') if print_progress else df.groupby(['uniqID', 'Day_Type'])\n",
    "        \n",
    "        for name, group in iterator:\n",
    "\n",
    "            # If there are exactly 2 'Home' entries, set their sequence to 1 and the total number of rows in the group\n",
    "            if group['TRPPURP'].value_counts().get('Home', 0) == 2:\n",
    "                home_indices = group[group['TRPPURP'] == 'Home'].index.tolist()\n",
    "                # Set the sequence of the first 'Home' entry to 1\n",
    "                df.loc[home_indices[0], 'sequence'] = 1\n",
    "                \n",
    "                # Set the sequence of the second 'Home' entry to the length of the group, making it the last trip\n",
    "                df.loc[home_indices[1], 'sequence'] = len(group)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    df_setHome = setHomeSequence(df_removed_dupl_Home)\n",
    "    \n",
    "    \n",
    "    def reassignMiddleHomeSequences(df):\n",
    "        # For trip sequences with more than two 'Home' entries, this function reassigns the sequences of middle 'Home' entries.\n",
    "        # The aim is to distribute these 'Home' trips more realistically within the sequence, avoiding consecutive 'Home' trips and ensuring that they are placed appropriately among other trip purposes.\n",
    "\n",
    "        # Iterate through each group based on 'uniqID' and 'Day_Type'\n",
    "        iterator = tqdm(df.groupby(['uniqID', 'Day_Type']), desc = ' - 4.3. Reassign Home trips if more than 3 trips') if print_progress else df.groupby(['uniqID', 'Day_Type'])\n",
    "        \n",
    "        for name, group in iterator:\n",
    "\n",
    "            # If there are more than 2 'Home' entries\n",
    "            if group['TRPPURP'].value_counts().get('Home', 0) > 2:\n",
    "\n",
    "                # Get the index for all 'Home' entries\n",
    "                home_indices = group[group['TRPPURP'] == 'Home'].index.tolist()\n",
    "\n",
    "                # Exclude the first and the last 'Home' entries\n",
    "                middle_home_indices = home_indices[1:-1]\n",
    "\n",
    "                # Calculate the range for random sequence values\n",
    "                last_sequence = group['sequence'].max()\n",
    "                possible_sequences = list(range(3, last_sequence - 1)) #\n",
    "\n",
    "                if len(possible_sequences) < len(middle_home_indices):\n",
    "                    # If there are not enough possible sequence numbers, remove excess rows\n",
    "                    df.drop(middle_home_indices[len(possible_sequences):], inplace=True)\n",
    "                    middle_home_indices = middle_home_indices[:len(possible_sequences)]\n",
    "\n",
    "                random.shuffle(possible_sequences)\n",
    "\n",
    "                # Assign random sequence values to the middle 'Home' entries\n",
    "                for i, index in enumerate(middle_home_indices):\n",
    "                    df.loc[index, 'sequence'] = possible_sequences[i]\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Example usage\n",
    "    df_with_middle_home_reassigned = reassignMiddleHomeSequences(df_setHome)\n",
    "\n",
    "    def reassignConsecutiveSequences(df, num): \n",
    "        # Reassigns sequences to ensure they are consecutive without any duplicates or gaps, especially after prior adjustments might have created irregularities in the sequence numbering.\n",
    "\n",
    "        # Iterate through each group based on 'uniqID' and 'Day_Type'        \n",
    "        iterator = tqdm(df.groupby(['uniqID', 'Day_Type']), desc = ' - 4.' + str(num+3) + '. Reassign consecutive numbers of sequence_' + str(num)) if print_progress else df.groupby(['uniqID', 'Day_Type'])\n",
    "        \n",
    "        for name, group in iterator:\n",
    "\n",
    "            # Sort by sequence\n",
    "            sorted_group = group.sort_values('sequence')\n",
    "            new_sequence = 1\n",
    "            prev_sequence = None\n",
    "\n",
    "            for index, row in sorted_group.iterrows():\n",
    "                # If current sequence is same as previous, increment for non-'Home' or drop 'Home'\n",
    "                if row['sequence'] == prev_sequence:\n",
    "                    if row['TRPPURP'] == 'Home':\n",
    "                        # If the TRPPURP is 'Home', remove the row\n",
    "                        df.drop(index, inplace=True)\n",
    "                    else:\n",
    "                        # If the TRPPURP is not 'Home', increment the sequence by 1\n",
    "                        new_sequence += 1\n",
    "\n",
    "                df.loc[index, 'sequence'] = new_sequence\n",
    "                prev_sequence = row['sequence']\n",
    "                new_sequence += 1\n",
    "\n",
    "        return df\n",
    "\n",
    "    \n",
    "    df_with_consecutive_sequences = reassignConsecutiveSequences(df_with_middle_home_reassigned, 1)\n",
    "    df_with_consecutive_sequences = reassignConsecutiveSequences(df_with_consecutive_sequences, 2)\n",
    "\n",
    "    # sort and reorganize -----------------\n",
    "\n",
    "    \n",
    "    # rename column: 'seq_similar_orig' -> seq_NHTS\n",
    "    df_with_consecutive_sequences.rename(columns={'seq_similar_orig': 'seq_NHTS'}, inplace=True)\n",
    "\n",
    "    columns = ['uniqID', 'ageGroup', 'Home_cbg', 'Day_Type', 'Week_Type', 'TRPPURP', 'sequence', 'seq_NHTS']\n",
    "    \n",
    "    # ALl columns of df_with_consecutive_sequences DataFrame\n",
    "    all_columns = df_with_consecutive_sequences.columns.tolist()\n",
    "\n",
    "    # Extract all the columns except selected column\n",
    "    remaining_columns = [col for col in all_columns if col not in columns]\n",
    "\n",
    "    # Create new columns sequence list by adding other columns after selected column\n",
    "    new_column_order = columns + remaining_columns\n",
    "\n",
    "    # Now reassign this list of columns to dataframe\n",
    "    organized_df = df_with_consecutive_sequences.reindex(columns=new_column_order)\n",
    "\n",
    "    # Eradicate 'count' column\n",
    "    if 'count' in organized_df.columns:\n",
    "        organized_df.drop('count', axis=1, inplace=True)    \n",
    "    \n",
    "#     organized_df = df_with_consecutive_sequences[columns]\n",
    "    \n",
    "      # Sort uniqID, Day_Type, sequence by Ascending order \n",
    "    sorted_df = organized_df.sort_values(\n",
    "        by=['uniqID', 'Day_Type', 'sequence'], \n",
    "        ascending=[True, True, True]\n",
    "    )  \n",
    "    \n",
    "    sorted_df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    \n",
    "        \n",
    "    tqdm.pandas(desc=\" - 4.6. drop consecutive homes except start and end\") \n",
    "    \n",
    "    # If there are duplicate Home trips -> delete second one\n",
    "    def drop_consecutive_homes(group):\n",
    "        homes = group['TRPPURP'] == 'Home'\n",
    "        consecutive = homes & homes.shift(fill_value=False)\n",
    "        drop_indices = consecutive[consecutive].index\n",
    "        return group.drop(drop_indices)\n",
    "\n",
    "    if print_progress == True:\n",
    "        droped_consecutive_homes_df = sorted_df.groupby(['uniqID', 'Day_Type']).progress_apply(drop_consecutive_homes).reset_index(drop=True)\n",
    "    else:\n",
    "        droped_consecutive_homes_df = sorted_df.groupby(['uniqID', 'Day_Type']).apply(drop_consecutive_homes).reset_index(drop=True)\n",
    "\n",
    "    # Reassign sequence\n",
    "    \n",
    "    \n",
    "    tqdm.pandas(desc=\" - 4.7. reassign sequence\") \n",
    "    def reset_sequence(group):\n",
    "        group['sequence'] = range(1, len(group) + 1)\n",
    "        return group\n",
    "\n",
    "    if print_progress == True:\n",
    "        reassigned_sequence_df = droped_consecutive_homes_df.groupby(['uniqID', 'Day_Type']).progress_apply(reset_sequence).reset_index(drop=True)\n",
    "    else:\n",
    "        reassigned_sequence_df = droped_consecutive_homes_df.groupby(['uniqID', 'Day_Type']).apply(reset_sequence).reset_index(drop=True)\n",
    "    \n",
    "    #------------------------------- fix errors\n",
    "    \n",
    "    tqdm.pandas(desc=\" - 4.8. fix home error if need ... (1)\") \n",
    "    \n",
    "    # Add new row if the last TRPPURP is not a 'Home' in each group\n",
    "    def add_home_row_if_needed(group):\n",
    "        if group['TRPPURP'].iloc[-1] != 'Home':\n",
    "            new_row = group.iloc[-1].copy()\n",
    "            new_row['TRPPURP'] = 'Home'\n",
    "            new_row['sequence'] = new_row['sequence'] + 1\n",
    "            group = pd.concat([group, pd.DataFrame([new_row])])\n",
    "        return group\n",
    "\n",
    "    if print_progress == True:    \n",
    "        fix_1_df = reassigned_sequence_df.groupby(['uniqID', 'Day_Type']).progress_apply(add_home_row_if_needed).reset_index(drop=True)\n",
    "    else:\n",
    "        fix_1_df = reassigned_sequence_df.groupby(['uniqID', 'Day_Type']).apply(add_home_row_if_needed).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    tqdm.pandas(desc=\" - 4.9. fix home error if need ... (2)\") \n",
    "    \n",
    "    def add_home_to_start_if_needed(group):\n",
    "        if group['TRPPURP'].iloc[0] != 'Home':\n",
    "            new_row = group.iloc[0].copy()\n",
    "            new_row['TRPPURP'] = 'Home'\n",
    "            new_row['sequence'] = 1\n",
    "            group = pd.concat([new_row.to_frame().T, group])\n",
    "            group['sequence'] = group['sequence'].astype(int) + 1\n",
    "            group['sequence'].iloc[0] = 1\n",
    "        return group\n",
    "\n",
    "    if print_progress == True:        \n",
    "        fix_2_df = fix_1_df.groupby(['uniqID', 'Day_Type']).progress_apply(add_home_to_start_if_needed).reset_index(drop=True)\n",
    "    else:\n",
    "        fix_2_df = fix_1_df.groupby(['uniqID', 'Day_Type']).apply(add_home_to_start_if_needed).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    return fix_2_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trip Timing Estimator\n",
    "### 3.0. preprocessing (1): Extracting dwell time by trip purpose using NHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwellTime_listFromNHTS(df):\n",
    "    \"\"\"\n",
    "    Extracts and organizes dwell time distributions by trip count, trip purpose, and other classifications from NHTS data. \n",
    "    This function is intended to run once to prepare the data for further simulation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing NHTS trip data.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with dwell time lists categorized by age class, day type, trip count class, and trip purpose.\n",
    "    \"\"\"\n",
    "    # 1. Calculate trip count: Count the number of trips for each unique ID and day type.\n",
    "    trippub = df.copy()\n",
    "    grouped = trippub.groupby(['uniqID', 'Day_Type'])\n",
    "    trippub['tripCount'] = grouped['Trip_pur'].transform('count')  # Add trip count for each row based on grouping.\n",
    "\n",
    "    # 2. Create trip count class: Classify trip counts into three categories based on the number of trips per day.\n",
    "    # Categories are defined as 1-3 trips, 4-5 trips, and 6 or more trips, considering trips start and end at home.\n",
    "    trippub['tripCount_class'] = pd.cut(trippub['tripCount'], bins=[0, 4, 6, float('inf')], labels=[1, 2, 3], right=False)\n",
    "    # The bins parameter defines the range of trip counts for each class: \n",
    "    # - Class 1 for 1-3 trips (inclusive of starting and ending at home)\n",
    "    # - Class 2 for 4-5 trips\n",
    "    # - Class 3 for 6 or more trips.\n",
    "    # These classifications help in understanding the distribution of trip counts and corresponding dwell times.\n",
    "\n",
    "    dwellTime_dict = trippub.groupby(['age_class', 'Day_Type', 'tripCount_class', 'Trip_pur'])['Dwell_T_min'].apply(list).to_dict()\n",
    "    \n",
    "    return dwellTime_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0. preprocessing (2): Extracting trip start time by trip purpose using NHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startTime_listFromNHTS(df):\n",
    "    \"\"\"\n",
    "    Generates a dictionary mapping the start times of trips based on age class, day type, and the second trip purpose from the NHTS data.\n",
    "    This function groups the data and extracts start times to understand typical trip start patterns. \n",
    "    It's designed to be run once and reused for analysis or simulation purposes.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing NHTS trip data.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary where each key is a tuple of (age class, day type, trip purpose) and each value is a list of \n",
    "      non-zero start times (in minutes from midnight) for that combination.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty dictionary to store start times\n",
    "    startTime_dict = {}\n",
    "    trippub_new = df.copy()\n",
    "\n",
    "    # Group the data by unique ID, age class, and day type. This aggregation is suited for analysis on how \n",
    "    # start times may vary across different demographics and types of days (e.g., weekdays vs. weekends).\n",
    "    grouped_trippub = trippub_new.groupby(['uniqID', 'age_class', 'Day_Type'])\n",
    "\n",
    "    # Iterate through each group to collect start times\n",
    "    for (_, age_class, day_type), group in tqdm(grouped_trippub, desc='Make dict of starting time derived from NHTS data'):\n",
    "        # Only consider groups with more than one trip to ensure we're looking at subsequent trips\n",
    "        if len(group) > 1:\n",
    "            # Extract the trip purpose of the second trip in the sequence. This choice focuses on the start time \n",
    "            # of the day's first major trip after potentially leaving home.\n",
    "            trip_pur = group['Trip_pur'].iloc[1]\n",
    "            key = (age_class, day_type, trip_pur)  # Define a unique key for the dictionary\n",
    "\n",
    "            # Initialize the list in the dictionary if the key doesn't exist\n",
    "            if key not in startTime_dict:\n",
    "                startTime_dict[key] = []\n",
    "\n",
    "            # Add non-zero start times to the list for this key. Zero values are excluded to avoid considering \n",
    "            # trips that might not represent actual departures (e.g., midnight or incorrectly recorded times).\n",
    "            non_zero_values = [value for value in group['sta_T_min'].tolist() if value != 0]\n",
    "            startTime_dict[key].extend(non_zero_values)\n",
    "        \n",
    "    return startTime_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Estimating dwell time\n",
    "### 3.2) Estimating trip start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignDwellStartT(df, dwellTime_dict, startTime_dict, print_progress=True):\n",
    "    \"\"\"\n",
    "    Estimates and assigns dwell times and start times for simulated trip data using distributions derived from NHTS data.\n",
    "    It first classifies each trip within the simulated data by trip count and then assigns dwell times based on\n",
    "    age class, day type, trip count class, and trip purpose. Finally, it assigns start times to the trips using\n",
    "    similar criteria.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the simulated trip data.\n",
    "    - dwellTime_dict: Dictionary containing dwell time distributions from NHTS data.\n",
    "    - startTime_dict: Dictionary containing start time distributions from NHTS data.\n",
    "    - print_progress: Boolean flag to print progress messages.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with 'Dwell_Time' and 'sta_T_min' (start time in minutes from midnight) assigned for each trip.\n",
    "    \"\"\"\n",
    "    \n",
    "    if print_progress == True:\n",
    "        print('<Trip timing estimator>')\n",
    "    \n",
    "    # Function to assign dwell time to each trip based on the dwellTime_dict distributions\n",
    "    def assignDwellTime(df, dwellTime_dict, print_progress):\n",
    "        simul_trip_sequence = df.copy()\n",
    "\n",
    "        tqdm.pandas(desc=\"1. classify tripCount of simulated data\")\n",
    "\n",
    "        # Classify and assign trip count class to each trip based on the size of each group (unique ID and day type)\n",
    "        group_counts = simul_trip_sequence.groupby(['uniqID', 'Day_Type']).size().to_dict()\n",
    "\n",
    "        def assign_class(row):\n",
    "            group_size = group_counts[(row['uniqID'], row['Day_Type'])]\n",
    "\n",
    "            if group_size <= 3:\n",
    "                return 1\n",
    "            elif 4 <= group_size <= 5:\n",
    "                return 2\n",
    "            else:\n",
    "                return 3\n",
    "\n",
    "        # Assign dwell time to each trip by randomly selecting from the corresponding distribution in dwellTime_dict\n",
    "        if print_progress == True:\n",
    "            simul_trip_sequence['trip_count_class'] = simul_trip_sequence.progress_apply(assign_class, axis=1)\n",
    "        else:\n",
    "            simul_trip_sequence['trip_count_class'] = simul_trip_sequence.apply(assign_class, axis=1)\n",
    "\n",
    "\n",
    "        # 2. Sampling Dwell_Time from distribution\n",
    "        tqdm.pandas(desc=\"2. Assign Dwell time\")\n",
    "\n",
    "        def assign_dwell_time(row):\n",
    "            # Convert day type to 'Weekday' or 'Weekend' for consistency with the dictionary keys\n",
    "            day_type_map = {\n",
    "                'Monday': 'Weekday',\n",
    "                'Tuesday': 'Weekday',\n",
    "                'Wednesday': 'Weekday',\n",
    "                'Thursday': 'Weekday',\n",
    "                'Friday': 'Weekday',\n",
    "                'Saturday': 'Weekend',\n",
    "                'Sunday': 'Weekend'\n",
    "            }\n",
    "            day_type = day_type_map[row['Day_Type']]\n",
    "\n",
    "            # Construct the key for the dictionary lookup\n",
    "            key = (row['ageGroup'], day_type, row['trip_count_class'], row['TRPPURP'])\n",
    "            dwell_times = dwellTime_dict.get(key, [0])\n",
    "\n",
    "            if not isinstance(dwell_times, (list, np.ndarray)) or len(dwell_times) == 0:\n",
    "                dwell_time_sample = np.random.randint(10, 301)\n",
    "                print(f\"cannot find from dic, put random dwelltime...({row['ageGroup']}, {row['Day_Type']}, {row['trip_count_class']}, {row['TRPPURP']}, value: {dwell_time_sample})\")\n",
    "                return dwell_time_sample\n",
    "\n",
    "            dwell_time_sample = -1\n",
    "            while dwell_time_sample < 0:\n",
    "                dwell_time_sample = np.random.choice(dwell_times)\n",
    "\n",
    "            return dwell_time_sample\n",
    "        \n",
    "        # Apply function to assign dwell time to each trip\n",
    "        if print_progress == True:\n",
    "            simul_trip_sequence['Dwell_Time'] = simul_trip_sequence.progress_apply(assign_dwell_time, axis=1)\n",
    "        else:\n",
    "            simul_trip_sequence['Dwell_Time'] = simul_trip_sequence.apply(assign_dwell_time, axis=1)\n",
    "\n",
    "        # drop column seq_NHTS\n",
    "        if 'seq_NHTS' in simul_trip_sequence.columns:\n",
    "            simul_trip_sequence = simul_trip_sequence.drop('seq_NHTS', axis=1)\n",
    "\n",
    "\n",
    "        return simul_trip_sequence\n",
    "    \n",
    "    # Apply the function to assign dwell times\n",
    "    dwell_table = assignDwellTime(df, dwellTime_dict, print_progress)\n",
    "    \n",
    "    # Function to assign start times to each trip\n",
    "    def assignStartTime(df, startTime_dict, print_progress):\n",
    "\n",
    "        simul_trip_sequence_dwell_time = df.copy()\n",
    "        simul_trip_sequence_dwell_time['sta_T_min'] = np.nan\n",
    "\n",
    "        tqdm.pandas(desc=\"3. Assign start time\")\n",
    "\n",
    "        # Function to assign start times within each group\n",
    "        def assign_sta_T_min(group):\n",
    "            if len(group) >= 2:\n",
    "                # Set the start time of the first trip to 0 (midnight)\n",
    "                first_row = group.iloc[0]\n",
    "                group.at[first_row.name, 'sta_T_min'] = 0\n",
    "\n",
    "                # Assign start time to the second trip based on startTime_dict\n",
    "                second_row = group.iloc[1]\n",
    "                key = (second_row['ageGroup'], second_row['Week_Type'], group['TRPPURP'].iloc[1])\n",
    "                if key in startTime_dict:\n",
    "                    group.at[second_row.name, 'sta_T_min'] = np.random.choice(startTime_dict[key])\n",
    "            else: \n",
    "                # For groups with only one trip, assign a start time of 0\n",
    "                first_row = group.iloc[0]\n",
    "                group.at[first_row.name, 'sta_T_min'] = 0\n",
    "\n",
    "            return group\n",
    "\n",
    "\n",
    "        if print_progress == True:\n",
    "            # Apply 'assign_sta_T_min' to each group of trips by unique ID and day type\n",
    "            simul_trip_sequence_2_start_time = simul_trip_sequence_dwell_time.groupby(['uniqID', 'Day_Type']).progress_apply(assign_sta_T_min)\n",
    "        else:\n",
    "            simul_trip_sequence_2_start_time = simul_trip_sequence_dwell_time.groupby(['uniqID', 'Day_Type']).apply(assign_sta_T_min)\n",
    "            \n",
    "        simul_trip_sequence_2_start_time.drop(columns=['Week_Type'], inplace=True)  # 임시로 만든 Week_Type 컬럼 삭제\n",
    "\n",
    "        return simul_trip_sequence_2_start_time\n",
    "    \n",
    "    # Assign start times to the trips with previously assigned dwell times\n",
    "    result_table = assignStartTime(dwell_table, startTime_dict, print_progress)\n",
    "    \n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trip Mode Assigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_tripMode(df, trip_mode_origin, print_progress=True):\n",
    "    \"\"\"\n",
    "    Assigns a travel mode to each trip in the simulated dataset based on the mode distribution from original data,\n",
    "    considering the age group and trip purpose. It also adjusts the assigned trip modes to reflect realistic travel patterns,\n",
    "    such as ensuring round trips have consistent modes and modifying modes based on trip sequence.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the simulated trip data.\n",
    "    - trip_mode_origin: DataFrame with the original distribution of trip modes by age class and trip purpose.\n",
    "    - print_progress: Boolean indicating whether to display progress during the execution.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with an assigned 'Trip_mode' for each trip, adjusted for realism based on trip sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    if print_progress == True:\n",
    "        print(\"<Trip mode assigner>\")\n",
    "        \n",
    "    tqdm.pandas(desc=\"1. Assign initial trip modes\")\n",
    "    \n",
    "    # Convert the original trip mode distributions into a dictionary for efficient lookup\n",
    "    probability_dict = trip_mode_origin.set_index(['age_class', 'Trip_pur', 'Trip_mode'])['Trip_modeP'].to_dict()\n",
    "\n",
    "    def assign_trip_mode(row):\n",
    "        \"\"\"\n",
    "        Samples a trip mode based on the distribution for the given age group and trip purpose.\n",
    "        \"\"\"\n",
    "        age = row['ageGroup']\n",
    "        purpose = row['TRPPURP']\n",
    "\n",
    "        # Extract mode probabilities for the given age and purpose, defaulting to 0 if not found\n",
    "        mode_probabilities = {mode: probability_dict.get((age, purpose, mode), 0) for mode in trip_mode_origin['Trip_mode'].unique()}\n",
    "\n",
    "        # Sample a mode based on the probabilities\n",
    "        return np.random.choice(list(mode_probabilities.keys()), p=list(mode_probabilities.values()))\n",
    "\n",
    "    if print_progress == True:\n",
    "        df['Trip_mode'] = df.progress_apply(assign_trip_mode, axis=1)\n",
    "    else:\n",
    "        df['Trip_mode'] = df.apply(assign_trip_mode, axis=1)\n",
    "    \n",
    "    tqdm.pandas(desc=\"2. Modify Trip_mode based on the first row of each group\")\n",
    "\n",
    "    def modify_trip_mode(group):        \n",
    "        \"\"\"\n",
    "        Modifies the trip mode of the first trip in each group if its purpose is not 'Home',\n",
    "        since the mode for trips starting from 'Home' might follow a different pattern.\n",
    "        \"\"\"\n",
    "        \n",
    "        first_row = group.iloc[0]\n",
    "        if first_row['TRPPURP'] == 'Home':\n",
    "            group.at[first_row.name, 'Trip_mode'] = np.nan # Clear the mode if the first trip starts from 'Home'\n",
    "        else:\n",
    "            print(first_row['uniqID'], first_row['Day_Type'], 'Ah uh') # Log if the first trip doesn't start from 'Home'\n",
    "        return group\n",
    "\n",
    "    # Apply mode modification to each group of trips\n",
    "    if print_progress == True:\n",
    "        put_trip_mode_df = df.groupby(['uniqID', 'Day_Type']).progress_apply(modify_trip_mode)\n",
    "    else:\n",
    "        put_trip_mode_df = df.groupby(['uniqID', 'Day_Type']).apply(modify_trip_mode)\n",
    "    \n",
    "    \n",
    "    def adjust_trip_mode_for_car(df):\n",
    "        \"\"\"\n",
    "        Adjusts the trip mode to 'Car' for the last trip if the first trip mode is 'Car',\n",
    "        reflecting the assumption that round trips typically use the same mode.\n",
    "        It also checks for consistency in modes for intermediate trips.\n",
    "        \"\"\"\n",
    "    \n",
    "        tqdm.pandas(desc=\"3. Adjust Trip_mode for round / one-way trip\")\n",
    "    \n",
    "        def process_group(group):\n",
    "            if len(group) == 1:\n",
    "                return group # No adjustment needed for single-trip groups\n",
    "\n",
    "            if group['Trip_mode'].iloc[1] != 'Car':\n",
    "                return group # No adjustment if the first trip mode isn't 'Car'\n",
    "\n",
    "            # 첫 Trip_mode가 Car라면 마지막 Trip_mode를 Car로 설정\n",
    "            group['Trip_mode'].iloc[-1] = 'Car'\n",
    "\n",
    "            # Set the last trip mode to 'Car' if the first is 'Car'\n",
    "            current_mode = 'Car'\n",
    "            for idx in range(2, len(group) - 1):\n",
    "                if group['Trip_mode'].iloc[idx] != current_mode:\n",
    "                    # Check for round trips and adjust modes accordingly\n",
    "                    next_indices = range(idx+1, len(group))\n",
    "                    if group['TRPPURP'].iloc[idx-1] in [group['TRPPURP'].iloc[next_idx] for next_idx in next_indices]:\n",
    "                        current_mode = group['Trip_mode'].iloc[idx]\n",
    "                    else:\n",
    "                        # Adjust mode for one-way trips\n",
    "                        group['Trip_mode'].iloc[idx] = current_mode\n",
    "\n",
    "            return group\n",
    "        \n",
    "        # Apply the adjustment to all groups\n",
    "        if print_progress == True:\n",
    "            df = df.groupby(['uniqID', 'Day_Type']).progress_apply(process_group).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.groupby(['uniqID', 'Day_Type']).apply(process_group).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    adjust_trip_mode_df = adjust_trip_mode_for_car(put_trip_mode_df)\n",
    "    \n",
    "    return adjust_trip_mode_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spatial Trip Route Estimator\n",
    "### 5.0. preprocessing: ratio between straight path and network path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_point_within(polygon):\n",
    "    \"\"\"\n",
    "    Generates a random point within a given polygon.\n",
    "\n",
    "    Parameters:\n",
    "    - polygon: The shapely Polygon object representing an area, such as a census block group (CBG).\n",
    "\n",
    "    Returns:\n",
    "    - A shapely Point object that lies within the polygon.\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = polygon.bounds # Get the bounding box of the polygon.\n",
    "    while True:\n",
    "        # Generate a random point within the bounding box.\n",
    "        p = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "        # Check if the generated point is within the actual polygon.\n",
    "        if polygon.contains(p):\n",
    "            return p # Return the point if it's inside the polygon\n",
    "\n",
    "def calculate_sampled_network_distance(cbg_gdf, network_road, num_samples=100):\n",
    "    \"\"\"\n",
    "    Calculates the average ratio between network (road) distance and straight-line distance for randomly selected pairs of points within CBGs.\n",
    "\n",
    "    Parameters:\n",
    "    - cbg_gdf: GeoDataFrame containing the geometries of census block groups (CBGs).\n",
    "    - network_road: The graph representing the road network, typically from the osmnx library.\n",
    "    - num_samples: The number of random pairs of points to sample for the calculation.\n",
    "\n",
    "    Returns:\n",
    "    - A list of distance ratios for all valid samples and the average ratio across these samples.\n",
    "    \"\"\"\n",
    "    distance_ratios = [] # Store the ratio of network to straight distance for each sample.\n",
    "\n",
    "    for _ in tqdm(range(num_samples), desc=\"Calculating Sampled Distances\"):\n",
    "        try:\n",
    "            # Randomly select two CBGs from the GeoDataFrame.\n",
    "            sampled_cbg = cbg_gdf.sample(n=2).reset_index(drop=True)\n",
    "            cbg_1 = sampled_cbg.loc[0]\n",
    "            cbg_2 = sampled_cbg.loc[1]\n",
    "#             print(cbg_1, cbg_2)\n",
    "\n",
    "            # Generate random points within each selected CBG.\n",
    "            point_1 = random_point_within(cbg_1.geometry)\n",
    "            point_2 = random_point_within(cbg_2.geometry)\n",
    "\n",
    "            # Find the nearest points on the road network to the generated points.\n",
    "            nearest_road_point_1 = ox.distance.nearest_nodes(network_road, point_1.x, point_1.y)\n",
    "            nearest_road_point_2 = ox.distance.nearest_nodes(network_road, point_2.x, point_2.y)\n",
    "\n",
    "            # Calculate the network distance between these nearest road points.\n",
    "            network_distance = nx.shortest_path_length(network_road, nearest_road_point_1, nearest_road_point_2, weight='length') / 1000\n",
    "#             print('network_distance: ', network_distance)\n",
    "\n",
    "            # Calculate the straight-line (geodesic) distance between the generated points.\n",
    "            coords_1 = (point_1.y, point_1.x)\n",
    "            coords_2 = (point_2.y, point_2.x)\n",
    "            \n",
    "            straight_distance = geodesic(coords_1, coords_2).meters / 1000\n",
    "            \n",
    "            # Calculate the ratio of network to straight distance, if straight distance is not zero.\n",
    "            if straight_distance != 0:\n",
    "                distance_ratio = network_distance / straight_distance\n",
    "                distance_ratios.append(distance_ratio)\n",
    "\n",
    "        except Exception as e: # Handle any errors during the process.\n",
    "            print(\"Error:\", e)\n",
    "            \n",
    "            continue\n",
    "\n",
    "    # Calculate and return the average distance ratio if any valid ratios were calculated.\n",
    "    if distance_ratios:\n",
    "        average_distance_ratio = np.mean(distance_ratios)\n",
    "        \n",
    "        return distance_ratios, average_distance_ratio\n",
    "    else:\n",
    "        #print(\"No valid distance ratios calculated.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Estimate probabilistic destinations for trip purpose t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_destination(probabilities):\n",
    "    \"\"\"\n",
    "    Selects a destination based on a set of given probabilities for each potential destination.\n",
    "\n",
    "    Parameters:\n",
    "    - probabilities: A dictionary where keys are destination identifiers (e.g., CBG IDs) and\n",
    "                     values are the corresponding probabilities of choosing each destination.\n",
    "\n",
    "    Returns:\n",
    "    - A selected destination identifier, chosen based on the provided probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    destinations = list(probabilities.keys()) # Extract a list of potential destinations\n",
    "    probabilities = list(probabilities.values())  # Extract the corresponding list of probabilities for each destination\n",
    "    probabilities_sum = sum(probabilities)  # Calculate the sum of all probabilities for normalization\n",
    "    normalized_probabilities = [p / probabilities_sum for p in probabilities]  # Normalize probabilities to ensure they sum to 1\n",
    "    \n",
    "    return np.random.choice(destinations, p=normalized_probabilities) # Randomly choose a destination based on the normalized probabilities\n",
    "\n",
    "\n",
    "\n",
    "def assign_sdr_destination(row, assigned_trip_table, prob_df, ageGroup, day_type):\n",
    "    \"\"\"\n",
    "    Assigns a destination for trips with specific requirements based on the trip's age group and day type.\n",
    "    This function is particularly tailored for 'S_d_r' (School, daily care, and religion) trips, determining\n",
    "    possible destinations and selecting one based on predefined probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - row: The current row of the DataFrame being processed, representing a single trip.\n",
    "    - assigned_trip_table: A DataFrame tracking previously assigned destinations to ensure consistency.\n",
    "    - prob_df: A DataFrame containing probabilities for various destinations based on different conditions.\n",
    "    - ageGroup: The age group of the individual undertaking the trip.\n",
    "    - day_type: The type of day on which the trip occurs ('weekday' or 'weekend').\n",
    "\n",
    "    Returns:\n",
    "    - Dest: The chosen destination based on the trip's conditions.\n",
    "    - TRPPURP_det: A detailed trip purpose, potentially refined during the destination assignment process.\n",
    "    - assigned_trip_table: The updated DataFrame of assigned destinations, including the current trip's assignment.\n",
    "    \"\"\"\n",
    "    \n",
    "    ws_wd = (row['Ws'], row['Wd']) # Extract workplace and dwelling weights for the trip\n",
    "    prob_row = prob_df[(prob_df['area'] == row['Home_cbg']) & (prob_df['Ws'] == ws_wd[0]) & (prob_df['Wd'] == ws_wd[1])]\n",
    "\n",
    "    if prob_row.empty:\n",
    "        return None, None, assigned_trip_table # If no matching probability row, return None for destination and purpose\n",
    "\n",
    "    # 1. Determine possible destinations based on age group and day type\n",
    "    if ageGroup == 'Child':\n",
    "        possible_destinations = ['Religion'] if day_type == 'weekend' else ['School', 'Dailycare']\n",
    "    elif ageGroup == 'Teen':\n",
    "        possible_destinations = ['Religion'] if day_type == 'weekend' else ['School']\n",
    "    elif ageGroup == 'Adult':\n",
    "        possible_destinations = ['University', 'Dailycare'] if day_type == 'weekday' else ['Dailycare', 'Religion']\n",
    "    else:\n",
    "        possible_destinations = ['Dailycare', 'Religion']\n",
    "\n",
    "            \n",
    "    # 2. Calculate probabilities for each possible destination within the chosen category\n",
    "    category_probabilities = {}\n",
    "    for destination in possible_destinations:\n",
    "        prob_col = f'{day_type}_{destination}'\n",
    "        if prob_col in prob_row.columns:\n",
    "            category_probabilities[destination] = eval(prob_row[prob_col].values[0])\n",
    "        else:\n",
    "            category_probabilities[destination] = 0.0\n",
    "\n",
    "            \n",
    "    # 3. Select a destination category from the available options\n",
    "    chosen_category = np.random.choice(list(category_probabilities.keys()))\n",
    "#     print(chosen_category)\n",
    "    \n",
    "    # 4. Further logic to select a specific location within the chosen destination category\n",
    "    if not category_probabilities[chosen_category]:\n",
    "        if chosen_category == 'University':\n",
    "            chosen_category = 'Dailycare'\n",
    "            if not category_probabilities[chosen_category]:\n",
    "                # If there are no destinations for 'University' 'Religion' trip\n",
    "                return None, chosen_category, assigned_trip_table\n",
    "        else:\n",
    "            # Ptobability distribution corresponding to the selected category is empty\n",
    "            return None, chosen_category, assigned_trip_table\n",
    "\n",
    "    \n",
    "    # 5. Update the assigned_trip_table with the chosen destination\n",
    "    if ageGroup == 'Child':\n",
    "        assigned_rows = assigned_trip_table[(assigned_trip_table['uniqID'] == row['uniqID']) & (assigned_trip_table['TRPPURP'].isin(['School', 'Dailycare']))]\n",
    "        if not assigned_rows.empty:\n",
    "            # If one of the School or Dailycare is selected, return only TRPPURP\n",
    "            assigned_category = assigned_rows.iloc[0]['TRPPURP']\n",
    "            return assigned_rows.iloc[0]['area'], f'{assigned_category}', assigned_trip_table\n",
    "    \n",
    "    else:\n",
    "        assigned_row = assigned_trip_table[(assigned_trip_table['uniqID'] == row['uniqID']) & (assigned_trip_table['TRPPURP'] == chosen_category)]\n",
    "        if not assigned_row.empty:\n",
    "            # If there is assigned destination, return that destination\n",
    "            return assigned_row.iloc[0]['area'], f'{assigned_row.iloc[0][\"TRPPURP\"]}', assigned_trip_table\n",
    "    \n",
    "    \n",
    "    areas, probabilities = zip(*category_probabilities[chosen_category].items())\n",
    "    \n",
    "    probabilities = np.array(probabilities) / sum(probabilities)  # Normalize Prob\n",
    "#     print(probabilities)\n",
    "    chosen_destination = np.random.choice(areas, p=probabilities)\n",
    "#     print(\"asd\", chosen_destination)\n",
    "\n",
    "    # 6. Add selected destination and location to assigned_trip_table\n",
    "    \n",
    "    if chosen_destination:\n",
    "        new_row = pd.DataFrame({'uniqID': [row['uniqID']], 'ageGroup': [ageGroup], 'TRPPURP': [chosen_category], 'area': [chosen_destination]})\n",
    "        assigned_trip_table = pd.concat([assigned_trip_table, new_row], ignore_index=True)\n",
    "    \n",
    "    # 7. Update TRPPURP_det\n",
    "    \n",
    "    return chosen_destination, chosen_category, assigned_trip_table\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_origin_destination(simul_df):\n",
    "    \"\"\"\n",
    "    Iteratively sets the 'Origin' for each trip in the simulated dataframe. \n",
    "    The origin of a trip is determined based on the destination of the previous trip for the same individual on the same day.\n",
    "    Special handling is applied to the first trip of the day and to trips with specific purposes.\n",
    "\n",
    "    Parameters:\n",
    "    - simul_df: DataFrame containing the simulated trip data with destinations ('Dest') already assigned.\n",
    "\n",
    "    Returns:\n",
    "    - The modified DataFrame with the 'Origin' column added and populated based on the logic outlined.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy the DataFrame to avoid modifying the original data in place\n",
    "    simul_df = simul_df.copy()\n",
    "    # Initialize the 'Origin' column with NaN values\n",
    "    simul_df['Origin'] = np.nan\n",
    "    \n",
    "    # Group the DataFrame by unique identifier and day type to process each individual's trips per day separately\n",
    "    for (uniqID, day_type), group in simul_df.groupby(['uniqID', 'Day_Type']):\n",
    "        prev_dest = None\n",
    "        for idx, row in group.iterrows():\n",
    "            if pd.isna(prev_dest):  # Check if this is the first trip of the day\n",
    "                \n",
    "                # For the first trip, set the origin as the home location if the trip purpose is 'Home'\n",
    "                if row['TRPPURP'] == 'Home':\n",
    "                    simul_df.at[idx, 'Origin'] = row['Home_cbg']\n",
    "                else:\n",
    "                    # If the first trip's purpose is not 'Home', leave the origin as NaN\n",
    "                    simul_df.at[idx, 'Origin'] = None \n",
    "            else:\n",
    "                # For subsequent trips, set the origin as the destination of the previous trip\n",
    "                simul_df.at[idx, 'Origin'] = prev_dest  \n",
    "                \n",
    "            # Update prev_dest with the current trip's destination for the next iteration    \n",
    "            if pd.notna(row['Dest']):  \n",
    "                prev_dest = row['Dest']\n",
    "    \n",
    "    return simul_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def assignDest(prob_df, simul_df, W_d_W_s = False, print_progress = True):\n",
    "    \"\"\"\n",
    "    Estimates spatial trip routes by assigning origins and destinations for simulated trips. \n",
    "    This process uses probabilities derived from real-world data to make the simulated trips more realistic in terms of spatial distribution. \n",
    "\n",
    "    Parameters:\n",
    "    - prob_df: DataFrame containing probability distributions for various destinations based on certain conditions.\n",
    "    - simul_df: DataFrame containing the simulated trip data.\n",
    "    - W_d_W_s: Either a boolean flag indicating whether custom weights/distributions should be used, \n",
    "             or a dictionary mapping trip purposes to specific weight/distribution combinations.\n",
    "    - print_progress: Boolean indicating whether to display progress information during the execution.\n",
    "\n",
    "    Returns:\n",
    "    - simul_df: Updated DataFrame with 'Origin' and 'Dest' fields assigned based on the estimated routes.\n",
    "    - assigned_trip_table: DataFrame tracking assigned destinations for validation and further analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    if print_progress == True:\n",
    "        print('<spatial trip route estimator>')        \n",
    "        \n",
    "    # Initialize an empty DataFrame to track assigned destinations\n",
    "    assigned_trip_table = pd.DataFrame(columns=['uniqID', 'ageGroup', 'TRPPURP', 'area'])\n",
    "    \n",
    "    # Ensure 'area' column in prob_df is of string type for consistency in key matching\n",
    "    prob_df['area'] = prob_df['area'].astype('str')\n",
    "    \n",
    "    def transform_value(x):\n",
    "        if isinstance(x, float):  # If the value is a float\n",
    "            return f\"{x:.0f}\"  # Convert to string without decimal\n",
    "        elif isinstance(x, int):  # If the value is an integer\n",
    "            return str(x)  # Convert to string\n",
    "        elif isinstance(x, str):  # If the value is already a string\n",
    "            return x  # Keep it as is\n",
    "        else:\n",
    "            return x  # For other types, return as is\n",
    "\n",
    "    # Apply the transformation to the 'Home_cbg' and 'cbg' columns\n",
    "    simul_df['Home_cbg'] = simul_df['Home_cbg'].apply(transform_value)\n",
    "    \n",
    "    if W_d_W_s == False:\n",
    "        wd_ws_combinations = prob_df[['Ws', 'Wd']].drop_duplicates().to_dict('records')\n",
    "#         print(wd_ws_combinations)\n",
    "        trppurp_ws_wd_mapping = {trppurp: random.choice(wd_ws_combinations) for trppurp in simul_df['TRPPURP'].unique() if trppurp != 'Home'}\n",
    "    else:\n",
    "        trppurp_ws_wd_mapping = W_d_W_s\n",
    "        \n",
    "    # Prepare simul_df by initializing necessary columns for processing\n",
    "    simul_df['Origin'] = np.nan\n",
    "    simul_df['Dest'] = np.nan\n",
    "    simul_df['Ws'] = simul_df['TRPPURP'].map(lambda x: trppurp_ws_wd_mapping.get(x, {}).get('Ws'))\n",
    "    simul_df['Wd'] = simul_df['TRPPURP'].map(lambda x: trppurp_ws_wd_mapping.get(x, {}).get('Wd'))\n",
    "    \n",
    "    trppurp_to_prob_col = {\n",
    "        'Work': 'Work',\n",
    "        'Serv_trip': 'Serv_trip',\n",
    "        'Meals': 'Meals',\n",
    "        'Rec_lei': 'Rec_lei',\n",
    "        'V_fr_rel': 'V_fr_rel',\n",
    "        'D_shop': 'D_shop',\n",
    "        'Others': 'Others'\n",
    "    }\n",
    "    \n",
    "    simul_df['TRPPURP_det'] = simul_df['TRPPURP'] # new column: TRPPURP_det - detailed TRPPURP\n",
    "    \n",
    "     # Process each trip to assign destinations based on the specified probabilities and conditions\n",
    "    iterator = tqdm(simul_df.groupby(['uniqID', 'ageGroup']), desc = '1. Assign trip-routes (Origin and Dest)') if print_progress else simul_df.groupby(['uniqID', 'ageGroup'])\n",
    "    \n",
    "    for (uniqID, ageGroup), group in iterator:\n",
    "       \n",
    "        prev_Dest = None\n",
    "        main_workplace_assigned = False\n",
    "        for idx, row in group.iterrows():\n",
    "            # Special handling for trips with the 'Home' purpose\n",
    "            if row['TRPPURP'] == 'Home':  # Home -> Home_cbg\n",
    "                simul_df.at[idx, 'Dest'] = row['Home_cbg']\n",
    "                \n",
    "                continue\n",
    "                \n",
    "            # Additional logic to handle trips with specific purposes such as 'S_d_r'    \n",
    "            if row['TRPPURP'] == 'S_d_r': # S_d_r -> Complex condition\n",
    "                day_type = 'weekend' if row['Day_Type'] in ['Saturday', 'Sunday'] else 'weekday'\n",
    "                Dest, TRPPURP_det, assigned_trip_table = assign_sdr_destination(row, assigned_trip_table, prob_df, ageGroup, day_type)\n",
    "                \n",
    "                simul_df.at[idx, 'Dest'] = Dest\n",
    "                simul_df.at[idx, 'TRPPURP_det'] = TRPPURP_det\n",
    "                continue\n",
    "            \n",
    "\n",
    "            day_type = 'weekend' if row['Day_Type'] in ['Saturday', 'Sunday'] else 'weekday'\n",
    "            trppurp = trppurp_to_prob_col.get(row['TRPPURP'])\n",
    "            \n",
    "            if trppurp is not None:\n",
    "                ws_wd = (row['Ws'], row['Wd'])\n",
    "                prob_row = prob_df[(prob_df['area'] == row['Home_cbg']) & (prob_df['Ws'] == ws_wd[0]) & (prob_df['Wd'] == ws_wd[1])]\n",
    "\n",
    "                if not prob_row.empty:\n",
    "#                     print(row['TRPPURP'])\n",
    "                    if row['TRPPURP'] == 'Work':\n",
    "                        if main_workplace_assigned:\n",
    "                            # Sub workplace: Assign Dest based on prob\n",
    "                            prob_col = f'{day_type}_{trppurp}'\n",
    "                            probabilities = eval(prob_row[prob_col].values[0])\n",
    "#                             print(\"asdasd\", probabilities)\n",
    "                            Dest = choose_destination(probabilities)\n",
    "                        else:\n",
    "                            # Main workplace Assign first destination for Work trip and save\n",
    "                            prob_col = f'{day_type}_{trppurp}'\n",
    "                            probabilities = eval(prob_row[prob_col].values[0])\n",
    "                            Dest = choose_destination(probabilities)\n",
    "                            assigned_trip_table = assigned_trip_table.append({'uniqID': uniqID, 'ageGroup': ageGroup, 'TRPPURP': 'Work', 'area': Dest}, ignore_index=True)\n",
    "                            main_workplace_assigned = True\n",
    "                    elif row['TRPPURP'] == 'D_shop':\n",
    "                        # D_shop: Large_shop at 80% probability, Etc_shop at 20% probability\n",
    "                            \n",
    "                        shop_choice = np.random.choice(['Large_shop', 'Etc_shop'], p=[0.8, 0.2])\n",
    "                        simul_df.at[idx, 'TRPPURP_det'] = shop_choice  # Update TRPPURP_det\n",
    "                        prob_col = f'{day_type}_{shop_choice}'\n",
    "                        probabilities = eval(prob_row[prob_col].values[0])\n",
    "                        Dest = choose_destination(probabilities)\n",
    "                        \n",
    "                    else:\n",
    "                        # Non-Work TRPPURP: Assign Dest based on prob\n",
    "                        prob_col = f'{day_type}_{trppurp}'\n",
    "                        probabilities = eval(prob_row[prob_col].values[0])\n",
    "                        Dest = choose_destination(probabilities)\n",
    "                        \n",
    "\n",
    "                    \n",
    "                    simul_df.at[idx, 'Dest'] = Dest\n",
    "\n",
    "\n",
    "    # Update 'Origin' based on the assigned 'Dest' values\n",
    "    simul_df = set_origin_destination(simul_df)\n",
    "    return simul_df, assigned_trip_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Compute trip distance and duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거리 구하기 \n",
    "\n",
    "def estimate_tripDist_Time(simul_df, cbg_gdf, distance_ratio = 1.2229481, print_progress = True):\n",
    "    \"\"\"\n",
    "    Estimates the distances and travel times for each trip in the simulated dataset. \n",
    "    This function uses the geographic information from census block groups (CBG) and a predefined distance ratio to calculate network distances. \n",
    "    It then applies average speeds based on the mode of transport and the age group of the traveler to estimate travel times.\n",
    "\n",
    "    Parameters:\n",
    "    - simul_df: DataFrame containing simulated trip data with origins and destinations specified by CBG codes.\n",
    "    - cbg_gdf: GeoDataFrame containing CBG polygons and their FIPS codes for locating origins and destinations.\n",
    "    - distance_ratio: The average ratio between straight-line distances and network distances. \n",
    "                       This is used to convert geodesic distances to more realistic network distances.\n",
    "    - print_progress: Boolean flag indicating whether to display a progress bar during execution.\n",
    "\n",
    "    Returns:\n",
    "    - Updated simul_df with estimated trip distances ('TripDist') in kilometers and trip times ('TripTime') in minutes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to generate a random point within a given polygon. This is used to simulate starting\n",
    "    # and ending positions within the origin and destination CBGs.\n",
    "    def random_point_within(polygon):\n",
    "        # Extract the bounds of the polygon to define the area for generating random points\n",
    "        minx, miny, maxx, maxy = polygon.bounds\n",
    "        while True:\n",
    "            p = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "            # Check if the point is within the polygon itself, not just the bounding box\n",
    "            if polygon.contains(p):\n",
    "                return p\n",
    "\n",
    "    # Function to estimate network distances between origin and destination CBGs\n",
    "    def estimate_network_distance(simul_df, cbg_gdf, distance_ratio):\n",
    "        simul_df['TripDist'] = None # Initialize the column for storing trip distances\n",
    "\n",
    "        # Iterate over each row in the dataframe to calculate distances\n",
    "        iterator = tqdm(simul_df.iterrows(), total=simul_df.shape[0], desc='2. Estimating trip distances') if print_progress else simul_df.iterrows()\n",
    "        \n",
    "        for idx, row in iterator:\n",
    "            # Retrieve the polygons for the origin and destination CBGs\n",
    "            origin_cbg = cbg_gdf[cbg_gdf['FIPS_BLKGR'] == str(int(row['Origin']))].geometry.iloc[0]\n",
    "            dest_cbg = cbg_gdf[cbg_gdf['FIPS_BLKGR'] == str(int(row['Dest']))].geometry.iloc[0]\n",
    "\n",
    "            # Generate random points within the origin and destination polygons\n",
    "            origin_point = random_point_within(origin_cbg)\n",
    "            dest_point = random_point_within(dest_cbg)\n",
    "\n",
    "\n",
    "            # Calculate the geodesic (straight-line) distance between the two points\n",
    "            coords_1 = (origin_point.y, origin_point.x)\n",
    "            coords_2 = (dest_point.y, dest_point.x)\n",
    "\n",
    "            straight_distance = geodesic(coords_1, coords_2).meters / 1000\n",
    "\n",
    "            # Estimate the network distance using the provided distance ratio\n",
    "            estimated_network_distance = straight_distance * distance_ratio\n",
    "\n",
    "            # Store the calculated distance in the dataframe\n",
    "            simul_df.at[idx, 'TripDist'] = estimated_network_distance\n",
    "\n",
    "        return simul_df\n",
    "    \n",
    "    # Estimate the network distances for the trips\n",
    "    trip_dist_df = estimate_network_distance(simul_df, cbg, distance_ratio)\n",
    "    \n",
    "    # Calculate trip times based on the mode of transport and the traveler's age group\n",
    "    def calculate_trip_time(row):\n",
    "        speed = np.nan\n",
    "\n",
    "        if row['Trip_mode'] == 'Walk':\n",
    "            if row['ageGroup'] == 'Child' or row['ageGroup'] == 'Teen':\n",
    "                speed = 4.82\n",
    "            elif row['ageGroup'] == 'Adult':\n",
    "                speed = np.random.uniform(4.54, 4.82)\n",
    "            elif row['ageGroup'] == 'MidAdult':\n",
    "                speed = np.random.uniform(4.43, 4.54)\n",
    "            elif row['ageGroup'] == 'Seniors':\n",
    "                speed = np.random.uniform(3.42, 4.34)\n",
    "        elif row['Trip_mode'] == 'Car':\n",
    "            speed = 39.74\n",
    "        elif row['Trip_mode'] == 'PTrans':\n",
    "            speed = 18.79\n",
    "        elif row['Trip_mode'] == 'Bicy':\n",
    "            speed = 7.72\n",
    "\n",
    "        # Calculate the trip time in minutes based on the distance and speed\n",
    "        trip_time = round((row['TripDist'] / speed) * 60, 0)\n",
    "        if trip_time < 1: trip_time = 1\n",
    "\n",
    "        # Convert inf into NaN\n",
    "        if np.isinf(trip_time):\n",
    "            return np.nan\n",
    "\n",
    "        return trip_time\n",
    "    \n",
    "    # Initialize the 'TripTime' column and calculate trip times for each row\n",
    "    trip_dist_df['TripTime'] = None\n",
    "\n",
    "    # Calculate 'TripTime' for each raw\n",
    "    iterator = tqdm(trip_dist_df.iterrows(), total=trip_dist_df.shape[0], desc=\"3. Calculating Trip Time\") if print_progress else trip_dist_df.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        trip_dist_df.at[idx, 'TripTime'] = calculate_trip_time(row)\n",
    "        \n",
    "        \n",
    "    # Final step: Calculate start and end times for each trip and organize the DataFrame columns.\n",
    "\n",
    "    # Integrating tqdm progress bar with pandas operations for visual progress feedback.\n",
    "    tqdm.pandas(desc=\"4. Calculating Start and End Times\")\n",
    "\n",
    "    def calculate_start_end_times(group):\n",
    "        \"\"\"\n",
    "        Calculates start and end times for each trip within a group of trips by the same individual on the same day.\n",
    "        Adjusts the 'Dwell_Time' for the first trip and sets start and end times based on the sequence of trips.\n",
    "\n",
    "        Parameters:\n",
    "        - group: Grouped DataFrame segment representing all trips for an individual on a specific day.\n",
    "\n",
    "        Returns:\n",
    "        - The group with updated start and end times for each trip.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Handle cases where a group has only one trip.\n",
    "        if len(group) == 1:\n",
    "            group['TripSTARTT'] = np.nan\n",
    "            group['TripENDT'] = np.nan\n",
    "            group['start_min'] = np.nan\n",
    "            group['end_min'] = np.nan\n",
    "            return group\n",
    "\n",
    "        # Sort trips within the group based on their sequence.\n",
    "        group = group.sort_values(by='sequence')\n",
    "\n",
    "        # Special handling for the first trip, setting 'Dwell_Time' based on the second trip's start time.\n",
    "        if group.iloc[0]['TRPPURP'] == 'Home':\n",
    "            group.at[group.index[0], 'Dwell_Time'] = group.iloc[1]['sta_T_min']\n",
    "\n",
    "        # Initialize columns for start and end times.\n",
    "        group['TripSTARTT'] = np.nan\n",
    "        group['TripENDT'] = np.nan\n",
    "        group['start_min'] = 0\n",
    "        group['end_min'] = group.iloc[0]['Dwell_Time']\n",
    "\n",
    "        # Iterate over trips to calculate and set start and end times.\n",
    "        for i in range(1, len(group)):\n",
    "            prev_row = group.iloc[i - 1]\n",
    "            prev_start = 0 if pd.isna(prev_row['TripSTARTT']) else prev_row['TripSTARTT']\n",
    "            prev_dwell = 0 if pd.isna(prev_row['Dwell_Time']) else prev_row['Dwell_Time']\n",
    "            prev_trip_time = 0 if pd.isna(prev_row['TripTime']) else prev_row['TripTime']\n",
    "            prev_end = 0 if pd.isna(prev_row['TripENDT']) else prev_row['TripENDT']\n",
    "            prev_end_D_min = 0 if pd.isna(prev_row['end_min']) else prev_row['end_min']\n",
    "\n",
    "            current_dwell = group.at[group.index[i], 'Dwell_Time'] if not pd.isna(group.at[group.index[i], 'Dwell_Time']) else 0\n",
    "\n",
    "            # Calculate the start time for the current trip.\n",
    "            group.at[group.index[i], 'TripSTARTT'] = prev_start + prev_dwell + prev_trip_time\n",
    "            group.at[group.index[i], 'TripENDT'] = group.at[group.index[i], 'TripSTARTT'] + group.at[group.index[i], 'TripTime']\n",
    "            \n",
    "            # Update start and end minutes for the next iteration.\n",
    "            group.at[group.index[i], 'start_min'] = group.at[group.index[i], 'TripENDT']\n",
    "            group.at[group.index[i], 'end_min'] = group.at[group.index[i], 'start_min'] + current_dwell\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each group of trips by the same individual on the same day.\n",
    "    if print_progress:\n",
    "        trip_dist_df = trip_dist_df.groupby(['uniqID', 'Day_Type'], group_keys=False).progress_apply(calculate_start_end_times)\n",
    "    else:\n",
    "        trip_dist_df = trip_dist_df.groupby(['uniqID', 'Day_Type'], group_keys=False).apply(calculate_start_end_times)\n",
    "    \n",
    "    # Clean up: Remove unnecessary columns and rearrange the remaining ones.\n",
    "    if 'sta_T_min' in trip_dist_df.columns:\n",
    "        trip_dist_df.drop('sta_T_min', axis=1, inplace=True)\n",
    "    if 'Unnamed: 0' in trip_dist_df.columns:\n",
    "        trip_dist_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    if 'trip_count_class'in trip_dist_df.columns:\n",
    "        trip_dist_df.drop('trip_count_class', axis=1, inplace=True)\n",
    "\n",
    "    # Define the desired column order for the final DataFrame.\n",
    "    desired_order = [\n",
    "        'TRPPURP', 'TRPPURP_det', 'Ws', 'Wd', 'Origin', 'Dest', \n",
    "        'TripDist', 'Trip_mode', 'TripSTARTT', 'TripTime', 'TripENDT', \n",
    "        'start_min', 'Dwell_Time', 'end_min'\n",
    "    ]\n",
    "\n",
    "    current_columns = trip_dist_df.columns.tolist()\n",
    "\n",
    "    # Ensure the DataFrame follows the desired column order, moving any unspecified columns to the end.\n",
    "    new_order = [col for col in current_columns if col not in desired_order] + desired_order\n",
    "\n",
    "    trip_dist_df = trip_dist_df[new_order]\n",
    "\n",
    "    \n",
    "    return trip_dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Optimize Trips with Logical and space-time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeTrips_constraint(df, dwellTime_dict, print_progress = True):\n",
    "    \"\"\"\n",
    "    Applies constraints to optimize individual trips by adjusting start and end times and updating trip modes based on certain logical conditions, \n",
    "    such as changing modes from walking to driving or public transport for longer trips.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing simulated trip data.\n",
    "    - dwellTime_dict: Dictionary containing dwell time distributions for various scenarios.\n",
    "    - print_progress: Boolean indicating whether to show progress during execution.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with optimized trips.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_rows(group_df):\n",
    "        \"\"\"\n",
    "        Updates the start and end times for trips within a group based on dwell times and trip durations.\n",
    "        \n",
    "        Parameters:\n",
    "        - group_df: DataFrame segment representing trips for an individual on a specific day.\n",
    "        \n",
    "        Returns:\n",
    "        - The updated group DataFrame with recalculated start and end times.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update the first row's end time based on its dwell time\n",
    "        group_df.iloc[0, group_df.columns.get_loc('end_min')] = group_df.iloc[0]['Dwell_Time']\n",
    "\n",
    "        # Iterate through the rest of the rows to update start and end times sequentially\n",
    "        for i in range(1, len(group_df)):\n",
    "            group_df.iloc[i, group_df.columns.get_loc('TripSTARTT')] = group_df.iloc[i-1]['end_min']\n",
    "            group_df.iloc[i, group_df.columns.get_loc('TripENDT')] = group_df.iloc[i]['TripSTARTT'] + group_df.iloc[i]['TripTime']\n",
    "            group_df.iloc[i, group_df.columns.get_loc('start_min')] = group_df.iloc[i]['TripENDT']\n",
    "            group_df.iloc[i, group_df.columns.get_loc('end_min')] = group_df.iloc[i]['start_min'] + group_df.iloc[i]['Dwell_Time']\n",
    "\n",
    "        return group_df\n",
    "\n",
    "\n",
    "    def calculate_trip_time(row):\n",
    "        \"\"\"\n",
    "        Calculates the trip time based on the trip mode and age group, adjusting speeds accordingly.\n",
    "        \n",
    "        Parameters:\n",
    "        - row: A row of the DataFrame representing a single trip.\n",
    "        \n",
    "        Returns:\n",
    "        - The calculated trip time in minutes.\n",
    "        \"\"\"\n",
    "        speed = np.nan\n",
    "\n",
    "        # Define speed values based on trip mode and age group\n",
    "        if row['Trip_mode'] == 'Walk':\n",
    "            if row['ageGroup'] == 'Child' or row['ageGroup'] == 'Teen':\n",
    "                speed = 4.82\n",
    "            elif row['ageGroup'] == 'Adult':\n",
    "                speed = np.random.uniform(4.54, 4.82)\n",
    "            elif row['ageGroup'] == 'MidAdult':\n",
    "                speed = np.random.uniform(4.43, 4.54)\n",
    "            elif row['ageGroup'] == 'Seniors':\n",
    "                speed = np.random.uniform(3.42, 4.34)\n",
    "        elif row['Trip_mode'] == 'Car':\n",
    "            speed = 39.74 # Fixed speed for car travel\n",
    "        elif row['Trip_mode'] == 'PTrans':\n",
    "            speed = 18.79\n",
    "        elif row['Trip_mode'] == 'Bicy':\n",
    "            speed = 7.72\n",
    "\n",
    "        # Calculate trip time (distance/speed), then convert to minutes and round\n",
    "        trip_time = round((row['TripDist'] / speed) * 60, 0)\n",
    "        if trip_time < 1: trip_time = 1\n",
    "\n",
    "        # Convert infinity values to NaN\n",
    "        if np.isinf(trip_time):\n",
    "            return np.nan\n",
    "\n",
    "        return trip_time\n",
    "\n",
    "    def update_group(group):\n",
    "        \"\"\"\n",
    "        Updates the trip mode for each trip in the group based on the previous trip's mode and the current trip's duration.\n",
    "        \n",
    "        Parameters:\n",
    "        - group: DataFrame segment representing trips for an individual on a specific day.\n",
    "        \n",
    "        Returns:\n",
    "        - The updated group DataFrame with modified trip modes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the current trip mode is 'Walk' and the trip time is >= 60 minutes\n",
    "        for idx in group.index[1:]:  # Iterate through the group to check and update the trip mode\n",
    "            if group.loc[idx, 'Trip_mode'] == 'Walk' and group.loc[idx, 'TripTime'] >= 60:\n",
    "                # If the previous trip's mode was 'Car', set the current trip's mode to 'Car'\n",
    "                if group.loc[group.index[group.index.get_loc(idx) - 1], 'Trip_mode'] == 'Car':\n",
    "                    group.loc[idx, 'Trip_mode'] = 'Car'\n",
    "                else:\n",
    "                    # Otherwise, change the mode to 'PTrans' (Public Transport)\n",
    "                    group.loc[idx, 'Trip_mode'] = 'PTrans'\n",
    "\n",
    "                # Recalculate the trip time with the updated mode\n",
    "                group.loc[idx, 'TripTime'] = calculate_trip_time(group.loc[idx])\n",
    "\n",
    "        return group\n",
    "\n",
    "    def logicalConstraint(df):\n",
    "        \"\"\"\n",
    "        Applies a logical constraint to update trip modes based on the previous trip's mode\n",
    "        and the duration of the current trip.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame of the simulated trip data.\n",
    "\n",
    "        Returns:\n",
    "        - The DataFrame with updated trip modes where necessary.\n",
    "        \"\"\"\n",
    "        updated_df = df.copy()\n",
    "        \n",
    "        # Generate lists of Day_Type and uniqID that stastify condition by filtering raws\n",
    "        condition = (updated_df['Trip_mode'] == 'Walk') & (updated_df['TripTime'] >= 60)\n",
    "        unique_pairs = updated_df[condition][['uniqID', 'Day_Type']].drop_duplicates()\n",
    "\n",
    "        iterator = tqdm(unique_pairs.iterrows(), total = len(unique_pairs), desc = '5. Logical constraint (trip mode)' ) if print_progress else unique_pairs.iterrows()\n",
    "        \n",
    "        # Apply the update_group function to each group of trips by the same individual on the same day\n",
    "        for _, row in iterator:\n",
    "            group_data = updated_df.loc[(updated_df['uniqID'] == row['uniqID']) & (updated_df['Day_Type'] == row['Day_Type'])]\n",
    "            updated_group = update_group(group_data)\n",
    "\n",
    "    #         display(updated_group)\n",
    "\n",
    "            updated_group = update_rows(updated_group)\n",
    "            # Update the main DataFrame with the modified group\n",
    "            updated_df.update(updated_group)\n",
    "\n",
    "        return updated_df\n",
    "\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "    # Apply logical constraints to optimize trip modes\n",
    "    updated_df = logicalConstraint(simul_trip_Dist_Time_sample1)\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "\n",
    "    def assignDwellTime(df, distribution_list, print_ = True):\n",
    "        \"\"\"\n",
    "        Assigns dwell times to trips in the dataset based on predefined distributions. The function updates the\n",
    "        'Dwell_Time' column by sampling from distributions specific to each trip's characteristics, including the\n",
    "        trip's day type, age group, and purpose.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame containing the simulated trip data.\n",
    "        - distribution_list: A dictionary or other data structure containing dwell time distributions for various trip scenarios.\n",
    "        - print_: Boolean indicating whether to show progress during the execution.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame with updated 'Dwell_Time' for each trip.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Copy the DataFrame to avoid modifying the original data in place.\n",
    "        simul_trip_sequence = df.copy()\n",
    "\n",
    "        tqdm.pandas(desc=\"1. classify tripCount of simulated data\")\n",
    "\n",
    "        # Calculate the size of each group (by 'uniqID' and 'Day_Type') to determine the trip count class.\n",
    "        group_counts = simul_trip_sequence.groupby(['uniqID', 'Day_Type']).size().to_dict()\n",
    "        \n",
    "        def assign_class(row):\n",
    "            group_size = group_counts[(row['uniqID'], row['Day_Type'])]\n",
    "\n",
    "            if group_size <= 3:\n",
    "                return 1 # Class 1 for groups with up to 3 trips\n",
    "            elif 4 <= group_size <= 5:\n",
    "                return 2 # Class 2 for groups with 4-5 trips\n",
    "            else:\n",
    "                return 3\n",
    "\n",
    "        # Apply the classification function to each row.\n",
    "        if print_ == True:\n",
    "            simul_trip_sequence['trip_count_class'] = simul_trip_sequence.progress_apply(assign_class, axis=1)\n",
    "        else:\n",
    "            simul_trip_sequence['trip_count_class'] = simul_trip_sequence.apply(assign_class, axis=1)\n",
    "\n",
    "\n",
    "        # 2. Sampling Dwell_Time from distribution\n",
    "        tqdm.pandas(desc=\"2. Assign Dwell time\")\n",
    "\n",
    "        def assign_dwell_time(row):\n",
    "            # Convert the 'Day_Type' to a more generic form for lookup.\n",
    "            day_type_map = {\n",
    "                'Monday': 'Weekday',\n",
    "                'Tuesday': 'Weekday',\n",
    "                'Wednesday': 'Weekday',\n",
    "                'Thursday': 'Weekday',\n",
    "                'Friday': 'Weekday',\n",
    "                'Saturday': 'Weekend',\n",
    "                'Sunday': 'Weekend'\n",
    "            }\n",
    "            day_type = day_type_map[row['Day_Type']]\n",
    "\n",
    "            # Construct a key for the distributions based on the row's characteristics\n",
    "            key = (row['ageGroup'], day_type, row['trip_count_class'], row['TRPPURP'])\n",
    "            \n",
    "            # Retrieve the distribution for the given key, defaulting to [0] if not found.\n",
    "            dwell_times = distribution_list.get(key, [0])\n",
    "\n",
    "            # Handle cases where the distribution is missing or empty.\n",
    "            if not isinstance(dwell_times, (list, np.ndarray)) or len(dwell_times) == 0:\n",
    "                # Sample a dwell time from the distribution.\n",
    "                dwell_time_sample = np.random.randint(10, 301)\n",
    "                print(f\"cannot find from dic, put random dwelltime...({row['ageGroup']}, {row['Day_Type']}, {row['trip_count_class']}, {row['TRPPURP']}, value: {dwell_time_sample})\")\n",
    "                return dwell_time_sample\n",
    "\n",
    "            dwell_time_sample = -1\n",
    "            while dwell_time_sample < 0:\n",
    "                dwell_time_sample = np.random.choice(dwell_times)\n",
    "\n",
    "            return dwell_time_sample\n",
    "\n",
    "        # Apply the dwell time assignment function to each row.\n",
    "        if print_ == True:\n",
    "            simul_trip_sequence['Dwell_Time'] = simul_trip_sequence.progress_apply(assign_dwell_time, axis=1)\n",
    "        else:\n",
    "            simul_trip_sequence['Dwell_Time'] = simul_trip_sequence.apply(assign_dwell_time, axis=1)\n",
    "\n",
    "        # drop column seq_NHTS\n",
    "        if 'seq_NHTS' in simul_trip_sequence.columns:\n",
    "            simul_trip_sequence = simul_trip_sequence.drop('seq_NHTS', axis=1)\n",
    "\n",
    "\n",
    "        return simul_trip_sequence\n",
    "    \n",
    "       \n",
    "    # If time schedule for 1 day is too long, adjust the time. if > 2400 of end_min -> adjust end_min below 2400\n",
    "    def timeConstraint(df, dwellTime_dict, assignDwellTime, print_progress):\n",
    "        \"\"\"\n",
    "        Adjusts the dwell times for each trip to ensure that the total time spent on all daily activities does not exceed 24 hours.\n",
    "        This function iteratively adjusts dwell times using a predefined distribution and recalculates trip start and end times \n",
    "        to meet the time constraint.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame containing the simulated trip data with preliminary 'Dwell_Time', 'TripTime', etc.\n",
    "        - dwellTime_dict: Dictionary containing distributions of dwell times for different scenarios.\n",
    "        - assignDwellTime: Function that assigns dwell times to trips based on the given distribution.\n",
    "        - print_progress: Boolean indicating whether to display progress information during execution.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame with adjusted dwell times and updated trip times to ensure daily activities fit within a 24-hour period.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a copy of the DataFrame to avoid modifying the original data in place.\n",
    "        updated_df = df.copy()\n",
    "\n",
    "        # Filter out rows where the end time exceeds 24 hours (1440 minutes) to identify trips that need adjustment.\n",
    "        filtered_df = df[df['end_min'] > 1440]\n",
    "\n",
    "        # Extract unique pairs of 'uniqID' and 'Day_Type' that meet the filter condition for further processing.\n",
    "        unique_pairs = filtered_df[['uniqID', 'Day_Type']].drop_duplicates()\n",
    "\n",
    "        # Process each unique pair to adjust the dwell times and ensure the total schedule fits within 24 hours.\n",
    "        iterator = tqdm(unique_pairs.iterrows(), total = len(unique_pairs), desc = '6. Time constraint') if print_progress else unique_pairs.iterrows()\n",
    "        \n",
    "        for _, row in iterator:\n",
    "            # Extract trips for the current unique pair.\n",
    "            group_df = df[(df['uniqID'] == row['uniqID']) & (df['Day_Type'] == row['Day_Type'])]\n",
    "\n",
    "            # Attempt to adjust the dwell times up to 100 times to fit the schedule within 24 hours.\n",
    "            for attempt in range(1, 101):\n",
    "                # Assign dwell times using the provided function, which may incorporate randomness or specific logic.\n",
    "                updated_group_df = assignDwellTime(group_df, dwellTime_dict, print_ = False)\n",
    "                # Recalculate start and end times based on the newly assigned dwell times.\n",
    "                updated_group_df = update_rows(updated_group_df)\n",
    "\n",
    "                # Check if the adjustments have successfully brought the schedule within 24 hours.\n",
    "                if updated_group_df['end_min'].iloc[-1] <= 1440:\n",
    "                    break\n",
    "\n",
    "            # If the schedule still exceeds 24 hours after 100 attempts, issue a warning.\n",
    "            if updated_group_df['end_min'].iloc[-1] > 1440:\n",
    "                print(f\"Warning: After 100 attempts, 'end_min' is still above 2400 for uniqID {row['uniqID']} and Day_Type {row['Day_Type']}\")\n",
    "\n",
    "            if 'trip_count_class' in updated_group_df.columns:\n",
    "                updated_group_df = updated_group_df.drop('trip_count_class', axis=1) \n",
    "\n",
    "            # Update the main DataFrame with the adjusted trip times for the current unique pair.\n",
    "            updated_df.update(updated_group_df)\n",
    "\n",
    "            updated_df['uniqID'] = updated_df['uniqID'].astype('int64')\n",
    "            updated_df['Home_cbg'] = updated_df['Home_cbg'].astype('int64')\n",
    "            updated_df['sequence'] = updated_df['sequence'].astype('int64')\n",
    "\n",
    "        return updated_df\n",
    "\n",
    "\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "    updated_df = timeConstraint(updated_df, dwellTime_dict, assignDwellTime, print_progress)\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "\n",
    "    # If there are duplicate trips printed, adjust dwell time\n",
    "    def logicalConstraint2(df, print_progress):\n",
    "        \"\"\"\n",
    "        Adjusts simulated trips by identifying and merging consecutive trips that have the same\n",
    "        destination and purpose, effectively simulating a more realistic scenario where such trips\n",
    "        are part of a single, longer stay at the destination rather than multiple, separate trips.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame containing the simulated trip data, including 'Dest' and 'TRPPURP' columns.\n",
    "        - print_progress: Boolean indicating whether to show progress during the execution.\n",
    "\n",
    "        Returns:\n",
    "        - A DataFrame with consecutive trips to the same destination and for the same purpose merged,\n",
    "          resulting in adjusted 'Dwell_Time' and start/end times for the trips.\n",
    "        \"\"\"\n",
    "        \n",
    "        def process_group(group):\n",
    "            \"\"\"\n",
    "            Identifies consecutive trips within a group (for the same individual on the same day) that should be merged\n",
    "            based on having the same destination and purpose. Adjusts the 'Dwell_Time' and start/end times for merged trips.\n",
    "\n",
    "            Parameters:\n",
    "            - group: DataFrame segment representing all trips for an individual on a specific day.\n",
    "\n",
    "            Returns:\n",
    "            - The group with adjusted trips and a list of row indices that should be removed because their trips have been merged.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Create a flag indicating where the 'TRPPURP' or 'Dest' changes from the previous row.\n",
    "            trppurp_changed = group['TRPPURP'] != group['TRPPURP'].shift()\n",
    "            dest_changed = group['Dest'] != group['Dest'].shift()\n",
    "            boundaries = (trppurp_changed | dest_changed)\n",
    "\n",
    "            # Where either flag is True, a new 'group_id' is started to identify trips that can be merged.\n",
    "            group['group_id'] = boundaries.cumsum()\n",
    "\n",
    "            # Identify first and the last raw of consecutive groups\n",
    "            first_rows = group.drop_duplicates(subset=['group_id'], keep='first')\n",
    "            last_rows = group.drop_duplicates(subset=['group_id'], keep='last')\n",
    "\n",
    "            # Calculate end_min and Dwell_Time for the last raw\n",
    "            group.loc[first_rows.index, 'end_min'] = last_rows['end_min'].values\n",
    "            group.loc[first_rows.index, 'Dwell_Time'] = group.loc[first_rows.index, 'end_min'] - group.loc[first_rows.index, 'start_min']\n",
    "\n",
    "            # end_min = 1440\n",
    "            group.iloc[-1, group.columns.get_loc('end_min')] = 1440\n",
    "            group.iloc[-1, group.columns.get_loc('Dwell_Time')] = 1440 - group.iloc[-1, group.columns.get_loc('start_min')]\n",
    "\n",
    "            # filter duplicated raws based on group_id\n",
    "            all_duplicated = group.duplicated(subset=['group_id'], keep=False)\n",
    "            \n",
    "            # extract index of duplicated raws except first raws\n",
    "            index_to_remove = group[all_duplicated & (~group.index.isin(first_rows.index))].index.tolist()\n",
    "    #         print(index_to_remove)\n",
    "\n",
    "            # Delete column group_id\n",
    "            group = group.drop(columns=['group_id'])\n",
    "\n",
    "            return group, index_to_remove\n",
    "\n",
    "        # Create a copy of the DataFrame to avoid modifying the original data in place.\n",
    "        df_copy = df.copy()\n",
    "        # List to store indices of rows that need to be removed after merging consecutive trips.\n",
    "        indices_to_remove = []\n",
    "\n",
    "        # Process each group of trips by the same individual on the same day.\n",
    "        iterator = tqdm(df.groupby(['uniqID', 'Day_Type']), desc = '7. Logical constraint (duplicate trips)') if print_progress else df.groupby(['uniqID', 'Day_Type'])\n",
    "        \n",
    "        for _, group in iterator:\n",
    "            processed_group, to_remove = process_group(group)\n",
    "            # Update the processed group back into the DataFrame copy.\n",
    "            df_copy.loc[processed_group.index] = processed_group\n",
    "            \n",
    "            # Append the indices of rows to be removed to the list.\n",
    "            indices_to_remove.extend(to_remove)\n",
    "\n",
    "        df_copy = df_copy.drop(indices_to_remove)\n",
    "        # After merging trips, the sequence of trips for each day might be disrupted.\n",
    "        # Recalculate the sequence to ensure it's continuous and starts from 1 for each day.\n",
    "        df_copy['sequence'] = df_copy.groupby(['uniqID', 'Day_Type']).cumcount() + 1\n",
    "        # Reset the DataFrame index for cleanliness and to reflect the removal of certain rows.\n",
    "        df_copy.reset_index(drop=True, inplace=True)\n",
    "        return df_copy\n",
    "\n",
    "\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "    updated_df = logicalConstraint2(updated_df, print_progress)\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "    #-----------------------\n",
    "    \n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Code\n",
    "## A1. Preprocess NHTS data\n",
    "### A1.1. Organize columns of NHTS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_columns(df, print_progress = True):\n",
    "    \"\"\"\n",
    "    This function reorganizes the NHTS (National Household Travel Survey) dataset by selecting important columns for analysis, \n",
    "    mapping certain categorical codes to more meaningful values, and introducing new columns to better represent the data. \n",
    "    It focuses on clarifying trip purposes, transportation modes, and travel days based on NHTS coding schemes. \n",
    "    The function aims to make the dataset more accessible and informative for subsequent analysis.\n",
    "\n",
    "    - HOUSEID and PERSONID are maintained as identifiers.\n",
    "    - Trip purposes (TRPPURP) are derived from WHYTO, translating numerical codes to readable categories \n",
    "      like 'Home', 'Work', etc.\n",
    "    - Transportation modes (TRPTRANS) are mapped to categories like 'Walk', 'Bicy', 'Car', 'PTrans' \n",
    "      (public transport), and 'Air', based on the TRPTRANS codes.\n",
    "    - Travel days (TRAVDAY) are categorized into 'Weekend' or 'Weekday' to facilitate analysis based \n",
    "      on the day of the week.\n",
    "    - Respondent ages (R_AGE, R_AGE_IMP) are reclassified into broader age groups for more general analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame containing NHTS trip data to be organized.\n",
    "    - print_progress: A Boolean flag that indicates whether progress should be printed during execution.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame (trippub_re) that has been reorganized and cleaned for easier analysis, with selected columns \n",
    "      and new mappings applied to enhance readability and interpretability.\n",
    "    \"\"\"\n",
    "    \n",
    "    trippub_re = df.copy()\n",
    "    \n",
    "    # Explicitly select and reorder columns relevant for analysis from the NHTS data.\n",
    "    columns = ['HOUSEID', 'PERSONID', 'HHSTFIPS','R_AGE', 'R_AGE_IMP', 'TRIPPURP', 'WHYTRP1S', 'WHYTRP90', 'WHYFROM', 'WHYTO', 'TRPMILES', 'DWELTIME','STRTTIME', 'ENDTIME', 'TRPTRANS','TRAVDAY', 'TDAYDATE']\n",
    "    trippub_re = trippub_re[columns]\n",
    "    \n",
    "    # 0) TRPPURP_column\n",
    "    tqdm.pandas(desc=\"0) mapping TRPPURP\")\n",
    "    \n",
    "    # Map the 'WHYTO' column to new, more understandable trip purpose labels.\n",
    "    def map_purpose(row):\n",
    "        if row['WHYTO'] in [1, 2]:\n",
    "            return 'Home'\n",
    "        elif row['WHYTO'] in [3, 4]:\n",
    "            return 'Work'\n",
    "        elif row['WHYTO'] in [6, 8, 9, 10, 19]:\n",
    "            return 'S_d_r'\n",
    "        elif row['WHYTO'] == 11:\n",
    "            return 'D_shop'\n",
    "        elif row['WHYTO'] == 13:\n",
    "            return 'Meals'\n",
    "        elif row['WHYTO'] == 17:\n",
    "            return 'V_fr_rel'\n",
    "        elif row['WHYTO'] in [15, 16]:\n",
    "            return 'Rec_lei'\n",
    "        elif row['WHYTO'] in [12, 14, 18]:\n",
    "            return 'Serv_trip'\n",
    "        elif row['WHYTO'] in [5, 97]:\n",
    "            return 'Others'\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "    if print_progress == True:\n",
    "        trippub_re['TRPPURP_new'] = trippub_re.progress_apply(map_purpose, axis=1)\n",
    "    else:\n",
    "        trippub_re['TRPPURP_new'] = trippub_re.apply(map_purpose, axis=1)\n",
    "    \n",
    "\n",
    "    # Map transportation modes from the 'TRPTRANS' column to more readable labels.\n",
    "    tqdm.pandas(desc=\"1) mapping TRPTRANS\")\n",
    "    \n",
    "    def map_mode(row):\n",
    "        if row['TRPTRANS'] == 1:\n",
    "            return 'Walk'\n",
    "        elif row['TRPTRANS'] == 2:\n",
    "            return 'Bicy'\n",
    "        elif row['TRPTRANS'] in [3,4,5,6,8,9,10,18]:\n",
    "            return 'Car'\n",
    "        elif row['TRPTRANS'] in [10,11,12,13,14,16]:\n",
    "            return 'PTrans'\n",
    "        elif row['TRPTRANS'] == 19:\n",
    "            return 'Air'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    if print_progress == True:    \n",
    "        trippub_re['TRPTRANS_new'] = trippub_re.progress_apply(map_mode, axis=1)\n",
    "    else:\n",
    "        trippub_re['TRPTRANS_new'] = trippub_re.apply(map_mode, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Categorize days of travel into weekdays and weekends based on 'TRAVDAY'.\n",
    "    def map_week(row):\n",
    "        if row['TRAVDAY'] in [1,7]:\n",
    "            return 'Weekend'\n",
    "        elif row['TRAVDAY'] in [2,3,4,5,6]:\n",
    "            return 'Weekday'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    if print_progress == True:    \n",
    "        trippub_re['TRAVDAY_new'] = trippub_re.progress_apply(map_week, axis=1)\n",
    "    else:\n",
    "        trippub_re['TRAVDAY_new'] = trippub_re.apply(map_week, axis=1)\n",
    "    \n",
    "    \n",
    "    # Reclassify 'R_AGE_IMP' into broader age groups to facilitate demographic analysis.\n",
    "    tqdm.pandas(desc=\"3) mapping R_AGE_new\")\n",
    "    \n",
    "    def reclassify_age(age):\n",
    "        if age < 10:\n",
    "            return 'Child'\n",
    "        elif 10 <= age < 20:\n",
    "            return 'Teen'\n",
    "        elif 20 <= age < 40:\n",
    "            return 'Adult'\n",
    "        elif 40 <= age < 60:\n",
    "            return 'MidAdult'\n",
    "        else:\n",
    "            return 'Seniors'\n",
    "\n",
    "    if print_progress == True:    \n",
    "        trippub_re['R_AGE_new'] = trippub_re['R_AGE_IMP'].progress_apply(reclassify_age)\n",
    "    else:\n",
    "        trippub_re['R_AGE_new'] = trippub_re['R_AGE_IMP'].apply(reclassify_age)\n",
    "    \n",
    "    \n",
    "    if print_progress == True:   \n",
    "        print()\n",
    "        print('4) Now, Other columns...')\n",
    "    \n",
    "    # Merge 'HOUSEID' and 'PERSONID' into a new identifier for unique respondents.\n",
    "    trippub_re['HOUSEID'] = trippub_re['HOUSEID'].astype(str)\n",
    "    trippub_re['PERSONID'] = trippub_re['PERSONID'].astype(str)\n",
    "    trippub_re['PERSONID_new'] = trippub_re['HOUSEID'] + trippub_re['PERSONID']\n",
    "    \n",
    "    num_empty_rows = trippub_re['TRPPURP_new'].isnull().sum()\n",
    "\n",
    "    # Remove rows with missing new trip purpose to ensure dataset completeness.\n",
    "    trippub_re.dropna(subset=['TRPPURP_new'], inplace=True)\n",
    "    trippub_re.dropna(subset=['TRPTRANS_new'], inplace=True)\n",
    "    trippub_re.dropna(subset=['R_AGE_IMP'], inplace=True)\n",
    "    trippub_re.dropna(subset=['TRAVDAY_new'], inplace=True)\n",
    "    \n",
    "    \n",
    "    if print_progress == True:   \n",
    "        print()\n",
    "        print('... done!')\n",
    "        \n",
    "    # Filtering Columns\n",
    "    selected_columns = ['HOUSEID', 'PERSONID', 'PERSONID_new', 'HHSTFIPS', 'R_AGE_IMP', 'R_AGE_new', 'WHYFROM', 'WHYTO', 'TRPMILES', 'DWELTIME',\n",
    "                        'STRTTIME', 'ENDTIME', 'TRAVDAY', 'TRAVDAY_new', 'TDAYDATE', 'TRPPURP_new', 'TRPTRANS_new']\n",
    "    \n",
    "    trippub_re = trippub_re[selected_columns]\n",
    "    \n",
    "    \n",
    "    return trippub_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1.2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_NHTS(df, print_progress = True):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the NHTS dataset (After running organize_columns function) to correct data anomalies and enhance data quality for analysis. \n",
    "    The preprocessing steps include adjusting dwelling times, calculating travel times, and ensuring data consistency across the travel records.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The filtered NHTS DataFrame (returned from organize_columns function) to preprocess.\n",
    "    - print_progress: If true, prints the progress of preprocessing steps.\n",
    "\n",
    "    Returns:\n",
    "    - A preprocessed DataFrame with corrected and calculated fields relevant for travel analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    trip_total = df.copy()\n",
    "    \n",
    "    if print_progress == True:\n",
    "        print('Start preprocessing... NHTS data...')\n",
    "      \n",
    "    # Step 1 ~ 2\n",
    "    # Correct dwelling times: Set dwelling times less than 0 to 0.\n",
    "    trip_total['DWELTIME'] = trip_total['DWELTIME'].apply(lambda x: 0 if x < 0 else x)\n",
    "    \n",
    "    # Extract hours and minutes from ENDTIME and STRTTIME to calculate travel times.\n",
    "    trip_total['ARRIVAL_hour'] = trip_total['ENDTIME'] // 100\n",
    "    trip_total['ARRIVAL_minute'] = trip_total['ENDTIME'] % 100\n",
    "    trip_total['STRTTIME_hour'] = trip_total['STRTTIME'] // 100\n",
    "    trip_total['STRTTIME_minute'] = trip_total['STRTTIME'] % 100\n",
    "    \n",
    "    # Convert arrival and start times to minutes.\n",
    "    trip_total['ARRIVAL_in_minutes'] = trip_total['ARRIVAL_hour'] * 60 + trip_total['ARRIVAL_minute']\n",
    "    trip_total['STRTTIME_in_minutes'] = trip_total['STRTTIME_hour'] * 60 + trip_total['STRTTIME_minute']\n",
    "    \n",
    "    # Calculate the end time in minutes by adding dwelling time to arrival time.\n",
    "    trip_total['ENDTIME_minute'] = trip_total['ARRIVAL_in_minutes'] + trip_total['DWELTIME']\n",
    "    \n",
    "    # Calculate travel time in minutes as the difference between arrival and start times.\n",
    "    trip_total['TRAVTIME'] = trip_total['ARRIVAL_in_minutes'] - trip_total['STRTTIME_in_minutes']\n",
    "\n",
    "    # Adjust for cases where travel time calculation rolls over a day boundary.\n",
    "    trip_total['TRAVTIME'] = trip_total['TRAVTIME'].apply(lambda x: x if x >= 0 else x + 2400)\n",
    "    \n",
    "   \n",
    "    # Remove records where the sum of dwelling time for a unique person ID and travel day combination is 0.\n",
    "    sum_dweltime = trip_total.groupby(['PERSONID_new', 'TRAVDAY_new'])['DWELTIME'].sum().reset_index()\n",
    "    valid_ids = sum_dweltime[sum_dweltime['DWELTIME'] != 0][['PERSONID_new', 'TRAVDAY_new']]\n",
    "    trip_total = pd.merge(trip_total, valid_ids, on=['PERSONID_new', 'TRAVDAY_new'])\n",
    "    \n",
    "    # Rename columns for clarity and consistency with analysis needs.\n",
    "    trip_total = trip_total.rename(columns={\n",
    "        'PERSONID_new': 'uniqID',\n",
    "        'R_AGE_IMP': 'age_Group',\n",
    "        'TRAVDAY_new': 'Day_Type',\n",
    "        'TRPPURP_new': 'Trip_pur',\n",
    "        'DWELTIME': 'Dwell_T_min',\n",
    "        'TRAVTIME': 'Trip_T_min',\n",
    "        'STRTTIME': 'sta_T_hms',\n",
    "        'STRTTIME_in_minutes': 'sta_T_min',\n",
    "        'ENDTIME': 'arr_T_hms',\n",
    "        'ARRIVAL_in_minutes': 'arr_T_min',\n",
    "        'ENDTIME_minute' : 'end_T_min'\n",
    "    })\n",
    "    \n",
    "    # Organize the preprocessed data for further steps.\n",
    "    trip_total = trip_total[['uniqID', 'age_Group', 'Day_Type', 'Trip_pur', 'sta_T_hms', 'arr_T_hms', 'Dwell_T_min', 'Trip_T_min', 'sta_T_min', 'arr_T_min', 'end_T_min']]\n",
    "    trip_total = trip_total.sort_values(['Day_Type', 'uniqID', 'sta_T_min'])\n",
    "\n",
    "    tqdm.pandas(desc=\"0) remove duplicate home\")\n",
    "    \n",
    "    # Step 3\n",
    "    def remove_duplicate_home(group):\n",
    "        if len(group) > 1:\n",
    "            if group.iloc[0]['Trip_pur'] == 'Home' and group.iloc[1]['Trip_pur'] == 'Home':\n",
    "                return group.iloc[1:]\n",
    "        return group\n",
    "\n",
    "    trip_total = trip_total.sort_values(['Day_Type', 'uniqID', 'sta_T_min'])\n",
    "\n",
    "    # Remove duplicated Home\n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(remove_duplicate_home).reset_index(drop=True)\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(remove_duplicate_home).reset_index(drop=True)\n",
    "\n",
    "    # display(trip_total[trip_total['uniqID'] ==300010541])\n",
    "\n",
    "    # Step 4\n",
    "    tqdm.pandas(desc=\"1) Start trip from home\")\n",
    "\n",
    "    def add_home_if_needed(group):\n",
    "        \n",
    "        if group.iloc[0]['sta_T_min'] == 0:\n",
    "            return None\n",
    "    \n",
    "        if group.iloc[0]['Trip_pur'] != 'Home':\n",
    "            new_row = group.iloc[0].copy()\n",
    "            new_row['Trip_pur'] = 'Home'\n",
    "            new_row['Dwell_T_min'] = group.iloc[0]['sta_T_min']\n",
    "            new_row['Trip_T_min'] = 0\n",
    "            new_row['sta_T_min'] = 0\n",
    "            new_row['arr_T_min'] = 0\n",
    "            new_row['end_T_min'] = new_row['Dwell_T_min']\n",
    "            group = pd.concat([pd.DataFrame([new_row]), group]).reset_index(drop=True)\n",
    "        return group\n",
    "    \n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(add_home_if_needed).reset_index(drop=True).dropna()\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(add_home_if_needed).reset_index(drop=True).dropna()\n",
    "    \n",
    "    \n",
    "    tqdm.pandas(desc=\"2) Adjusting initial Home Time\")\n",
    "\n",
    "    # Step 5: adjust_home_time function corrects the start and end times for the first 'Home' trip of the day\n",
    "    def adjust_home_time(group):\n",
    "        first_row = group.iloc[0]\n",
    "        if first_row['Trip_pur'] == 'Home':\n",
    "            # print(first_row['uniqID'])\n",
    "\n",
    "            # display(group)\n",
    "            next_row_idx = first_row.name + 1\n",
    "            # print(next_row_idx)\n",
    "            if next_row_idx in group.index:\n",
    "                next_row = group.loc[next_row_idx]\n",
    "\n",
    "                group.at[first_row.name, 'sta_T_min'] = 0\n",
    "                group.at[first_row.name, 'arr_T_min'] = 0\n",
    "                group.at[first_row.name, 'end_T_min'] = next_row['sta_T_min']\n",
    "                group.at[first_row.name, 'Dwell_T_min'] = group.at[first_row.name, 'end_T_min']\n",
    "                group.at[first_row.name, 'Trip_T_min'] = 0\n",
    "            else:\n",
    "                group.at[first_row.name, 'sta_T_min'] = 0\n",
    "                group.at[first_row.name, 'arr_T_min'] = 0\n",
    "                group.at[first_row.name, 'end_T_min'] = 1440\n",
    "                group.at[first_row.name, 'Dwell_T_min'] = 1440\n",
    "                group.at[first_row.name, 'Trip_T_min'] = 0\n",
    "        return group\n",
    "\n",
    "    \n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(adjust_home_time).reset_index(drop=True)\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(adjust_home_time).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # Remove entries where the start time is not less than the end time, essentially removing records where travel times might extend beyond a 24-hour period, which could indicate data errors or unusual travel behavior.\n",
    "    trip_total = trip_total[trip_total['sta_T_min'] < trip_total['end_T_min']]\n",
    "\n",
    "    \n",
    "    tqdm.pandas(desc=\"3) Addressing Dwell Time\")  \n",
    "    \n",
    "    # Step 6: Filters out groups where dwell time is zero, as it's unrealistic for someone to have no dwell time at a location unless it's a pass-through, which wouldn't typically be recorded as a separate trip. This helps in maintaining the quality of the dataset by ensuring all recorded trips reflect actual stops.\n",
    "    def filter_invalid_groups(group):\n",
    "#         global deleted_rows_counter\n",
    "        dwell_zero_count = sum(group['Dwell_T_min'] == 0)\n",
    "        last_row = group.iloc[-1]\n",
    "\n",
    "        # Delete groupes that disatisfy following conditions\n",
    "        if dwell_zero_count >= 2:\n",
    "#             deleted_rows_counter += len(group)\n",
    "            return None\n",
    "        if dwell_zero_count >= 1 and last_row['Trip_pur'] != 'Home':\n",
    "#             deleted_rows_counter += len(group)\n",
    "            return None\n",
    "        if last_row['Trip_pur'] != 'Home' and last_row['Dwell_T_min'] == 0:\n",
    "#             deleted_rows_counter += len(group)\n",
    "            return None\n",
    "\n",
    "        return group\n",
    "\n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(filter_invalid_groups).reset_index(drop=True).dropna()\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(filter_invalid_groups).reset_index(drop=True).dropna()\n",
    "\n",
    "\n",
    "    # Step 7: Ensure each travel day ends at 'Home' for each unique ID and Day_Type combination.\n",
    "    tqdm.pandas(desc=\"4) Add Home as a last travel\")  # tqdm의 pandas 확장을 활성화\n",
    "    \n",
    "    def add_or_remove_home_at_end(group):\n",
    "\n",
    "        # Check if the last trip purpose is not 'Home'.\n",
    "        if group.iloc[-1]['Trip_pur'] != 'Home':\n",
    "            last_row = group.iloc[-1]\n",
    "            new_row = last_row.copy()\n",
    "            new_row['Trip_pur'] = 'Home'\n",
    "            new_row['sta_T_min'] = last_row['end_T_min']\n",
    "\n",
    "            # Calculate the average travel time for non-zero travel times to estimate the arrival time.\n",
    "            trip_t_min_mean = int(group[group['Trip_T_min'] != 0]['Trip_T_min'].mean())\n",
    "\n",
    "            new_row['arr_T_min'] = trip_t_min_mean + new_row['sta_T_min']\n",
    "\n",
    "            # If the estimated arrival time exceeds the daily limit, discard the group.\n",
    "            if new_row['arr_T_min'] > 1440:\n",
    "#                 deleted_rows_counter += len(group)\n",
    "                return None\n",
    "\n",
    "            new_row['end_T_min'] = 1440\n",
    "            new_row['Trip_T_min'] = new_row['arr_T_min'] - new_row['sta_T_min']\n",
    "            new_row['Dwell_T_min'] = 1440 - new_row['arr_T_min']\n",
    "\n",
    "            # Update the start and arrival times to HHMM format.\n",
    "            new_row['sta_T_hms'] = (new_row['sta_T_min'] // 60) * 100 + (new_row['sta_T_min'] % 60)\n",
    "            new_row['arr_T_hms'] = (new_row['arr_T_min'] // 60) * 100 + (new_row['arr_T_min'] % 60)\n",
    "\n",
    "            # Add the new 'Home' trip to the group.\n",
    "            group = pd.concat([group, pd.DataFrame([new_row])]).reset_index(drop=True)\n",
    "            # display(group)\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each group and handle NaN values from the function (if any).\n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(add_or_remove_home_at_end).reset_index(drop=True).dropna()\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(add_or_remove_home_at_end).reset_index(drop=True).dropna()\n",
    "    \n",
    "    \n",
    "    # Step 8: Remove consecutive 'Home' trips at the end of the day and adjust the columns accordingly.\n",
    "    tqdm.pandas(desc=\"5) Remove duplicate last Home and Adjust\")\n",
    "\n",
    "    def remove_last_home(group):\n",
    "        group.reset_index(inplace = True, drop=True)\n",
    "        last_idx = len(group) - 1\n",
    "        \n",
    "        if last_idx > 0: # Ensure there are at least two trips to compare.\n",
    "            if group.iloc[last_idx]['Trip_pur'] == 'Home' and group.iloc[last_idx - 1]['Trip_pur'] == 'Home':\n",
    "                # print(group['uniqID'])\n",
    "                group = group.iloc[:-1]  #Delete the last row\n",
    "                \n",
    "                # Adjust the remaining last trip's times to align with the day's end.\n",
    "                group.reset_index(inplace = True, drop=True)\n",
    "                new_last_idx = len(group) - 1\n",
    "                group.at[new_last_idx, 'end_T_min'] = 1440\n",
    "                group.at[new_last_idx, 'Dwell_T_min'] = 1440 - group.at[new_last_idx, 'arr_T_min']\n",
    "\n",
    "                # Update time formats to HHMM for consistency.\n",
    "                group.at[new_last_idx, 'sta_T_hms'] = (group.at[new_last_idx, 'sta_T_min'] // 60) * 100 + (group.at[new_last_idx, 'sta_T_min'] % 60)\n",
    "                group.at[new_last_idx, 'arr_T_hms'] = (group.at[new_last_idx, 'arr_T_min'] // 60) * 100 + (group.at[new_last_idx, 'arr_T_min'] % 60)\n",
    "\n",
    "                # display(group)\n",
    "        return group\n",
    "    \n",
    "    # Apply the function to each group, ensuring that NaN values are handled correctly.\n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(remove_last_home).reset_index(drop=True).dropna()\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(remove_last_home).reset_index(drop=True).dropna()\n",
    "     \n",
    "    \n",
    "    # Step 9: Adjust the time details for the final 'Home' trip of each day to ensure it accurately reflects the end of the day.\n",
    "    tqdm.pandas(desc=\"6) Adjusting Last Home Time\") \n",
    "    \n",
    "    def adjust_home_time(group):\n",
    "        group.reset_index(inplace = True, drop=True)\n",
    "        \n",
    "        last_idx = len(group) - 1\n",
    "        if last_idx >= 0:  # Check there is at least one entry to adjust.\n",
    "            if group.iloc[last_idx]['Trip_pur'] == 'Home':\n",
    "                # Adjust the last 'Home' trip to ensure it represents the end of the day.\n",
    "                group.at[last_idx, 'end_T_min'] = 1440\n",
    "                \n",
    "                group.at[last_idx, 'Dwell_T_min'] = 1440 - group.at[last_idx, 'arr_T_min']\n",
    "                group.at[last_idx, 'sta_T_hms'] = (group.at[last_idx, 'sta_T_min'] // 60) * 100 + (group.at[last_idx, 'sta_T_min'] % 60)\n",
    "                group.at[last_idx, 'arr_T_hms'] = (group.at[last_idx, 'arr_T_min'] // 60) * 100 + (group.at[last_idx, 'arr_T_min'] % 60)\n",
    "\n",
    "        # display(group)\n",
    "        return group\n",
    "    \n",
    "    # Apply the adjustment to each group.\n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(adjust_home_time).reset_index(drop=True).dropna()\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(adjust_home_time).reset_index(drop=True).dropna()\n",
    "\n",
    "    \n",
    "    # Step 10: Merge consecutive 'Home' trip records, excluding the first and last trip of each day, into a single 'Home' trip.\n",
    "    tqdm.pandas(desc=\"7) merge consecutive home\")  \n",
    "    \n",
    "    # Assuming repaired_orig_data is your DataFrame\n",
    "    def merge_consecutive_home_groups(group):\n",
    "        consecutive_home_groups = []\n",
    "        current_group = []\n",
    "\n",
    "        # Apply the consolidation function to each group.\n",
    "        for idx, row in group.iterrows():\n",
    "            if row['Trip_pur'] == 'Home':\n",
    "                current_group.append(idx)\n",
    "            else:\n",
    "                if current_group:\n",
    "                    consecutive_home_groups.append(current_group.copy())\n",
    "                    current_group = []\n",
    "\n",
    "        if current_group:\n",
    "            consecutive_home_groups.append(current_group)\n",
    "\n",
    "        # Merge consecutive home groups\n",
    "        for home_group in consecutive_home_groups:\n",
    "            if len(home_group) > 1:\n",
    "                total_dwell_time = group.loc[home_group, 'Dwell_T_min'].sum()\n",
    "                total_trip_time = group.loc[home_group, 'Trip_T_min'].sum()\n",
    "\n",
    "                # Update the end time of the first row with the end time of the last row in the group\n",
    "                group.at[home_group[0], 'end_T_min'] = group.at[home_group[-1], 'end_T_min']\n",
    "            \n",
    "                # Update the first row in the consecutive home group\n",
    "                group.at[home_group[0], 'Dwell_T_min'] = group.at[home_group[0], 'end_T_min'] - group.at[home_group[0], 'arr_T_min']\n",
    "\n",
    "                # Drop the rows in the consecutive home group except for the first one\n",
    "                group = group.drop(home_group[1:])\n",
    "\n",
    "        # Reset the index\n",
    "        group = group.reset_index(drop=True)\n",
    "\n",
    "        # Update sta_T_hms and arr_T_hms\n",
    "        group['sta_T_hms'] = (group['sta_T_min'] // 60) * 100 + (group['sta_T_min'] % 60)\n",
    "        group['arr_T_hms'] = (group['arr_T_min'] // 60) * 100 + (group['arr_T_min'] % 60)\n",
    "\n",
    "        # display(group)\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each group\n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(merge_consecutive_home_groups).reset_index(drop=True).dropna()\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(merge_consecutive_home_groups).reset_index(drop=True).dropna()\n",
    "\n",
    "   \n",
    "    # Step 11: Ensure that the end time of one trip matches the start time of the following trip.\n",
    "    tqdm.pandas(desc=\"8) align 'start time' with next 'end time'\")  # tqdm의 pandas 확장을 활성화\n",
    "    \n",
    "    def align_end_time_with_next_start_time(group):\n",
    "        for idx in range(len(group) - 1):\n",
    "            current_row = group.iloc[idx]\n",
    "            next_row = group.iloc[idx + 1]\n",
    "\n",
    "            if current_row['end_T_min'] != next_row['sta_T_min']:\n",
    "                # Update end_T_min and Dwell_T_min of the current row\n",
    "                group.at[current_row.name, 'end_T_min'] = next_row['sta_T_min']\n",
    "                group.at[current_row.name, 'Dwell_T_min'] = next_row['sta_T_min'] - current_row['arr_T_min']\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each group\n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(align_end_time_with_next_start_time).reset_index(drop=True)\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(align_end_time_with_next_start_time).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Step 12: Remove groups that do not start or end with a 'Home' trip or where times do not make logical sense.\n",
    "    tqdm.pandas(desc=\"9) filter Home as a start and end pur\")  \n",
    "    \n",
    "    def filter_group(group):\n",
    "#         global deleted_rows_counter\n",
    "        \n",
    "        group.reset_index(inplace=True, drop=True)\n",
    "        last_idx = len(group) - 1\n",
    "        first_idx = 0\n",
    "\n",
    "        if last_idx >= 0:  # inspection only if there is at least one row in the group\n",
    "            first_condition = group.at[first_idx, 'sta_T_min'] != 0\n",
    "            last_condition = group.at[last_idx, 'end_T_min'] != 1440\n",
    "            trip_pur_condition = group.at[last_idx, 'Trip_pur'] != 'Home'\n",
    "            \n",
    "            if first_condition or last_condition or trip_pur_condition:\n",
    "#                 deleted_rows_counter += len(group)\n",
    "                return None  # This group does not logically conclude or initiate with 'Home'.\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each group and filter out None values\n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(filter_group).reset_index(drop=True).dropna()\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(filter_group).reset_index(drop=True).dropna()\n",
    "    \n",
    "    \n",
    "    # Step 13: Refine start, arrival, and end times to HHMM format based on minute values.\n",
    "    tqdm.pandas(desc=\"10) refine start, arr, end time with hms format\")  # tqdm의 pandas 확장을 활성화\n",
    "        \n",
    "    def hms_format(group):        \n",
    "        group['sta_T_hms'] = (group['sta_T_min'] // 60) * 100 + (group['sta_T_min'] % 60)\n",
    "        group['arr_T_hms'] = (group['arr_T_min'] // 60) * 100 + (group['arr_T_min'] % 60)\n",
    "        \n",
    "        # Add end_T_min\n",
    "        group['end_T_hms'] = (group['end_T_min'] // 60) * 100 + (group['end_T_min'] % 60)\n",
    "        \n",
    "        return group\n",
    "        \n",
    "    \n",
    "    # Apply the function to each group and filter out None values\n",
    "    trip_total['end_T_hms'] = None\n",
    "    \n",
    "    if print_progress == True:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).progress_apply(hms_format).reset_index(drop=True).dropna()\n",
    "    else:\n",
    "        trip_total = trip_total.groupby(['Day_Type', 'uniqID']).apply(hms_format).reset_index(drop=True).dropna()\n",
    "\n",
    "\n",
    "    # Step 14: Reclassify ages into categorical groups for easier analysis.\n",
    "    tqdm.pandas(desc=\"11) Reclassifying age variables\")  \n",
    "    \n",
    "    def reclassify_age(age):\n",
    "        if age < 10:\n",
    "            return 'Child'\n",
    "        elif 10 <= age < 20:\n",
    "            return 'Teen'\n",
    "        elif 20 <= age < 40:\n",
    "            return 'Adult'\n",
    "        elif 40 <= age < 60:\n",
    "            return 'MidAdult'\n",
    "        else:\n",
    "            return 'Seniors'\n",
    "\n",
    "        \n",
    "    if print_progress == True:\n",
    "        trip_total['age_class'] = trip_total['age_Group'].progress_apply(reclassify_age)\n",
    "    else:\n",
    "        trip_total['age_class'] = trip_total['age_Group'].apply(reclassify_age)\n",
    "    \n",
    "    if \"Unnamed: 0\" in trip_total.columns:\n",
    "        trip_total.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    \n",
    "    trip_total.rename(columns={\"age_Group\": \"age\"}, inplace=True)\n",
    "    \n",
    "        \n",
    "    return trip_total[['uniqID', 'age', 'age_class', 'Day_Type', 'Trip_pur', 'sta_T_hms', 'arr_T_hms', 'end_T_hms', 'Dwell_T_min', 'Trip_T_min', 'sta_T_min', 'arr_T_min', 'end_T_min']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1.3. Preprocessing for tripmode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 컬럼 TRPPURP_new 생성\n",
    "def preprocess_NHTS_tripMode(df, print_progress = True):\n",
    "    \"\"\"\n",
    "    Processes the NHTS dataset to create a new trip purpose category and calculates the probability of trip mode choices by age and newly defined trip purpose. This function aims to simplify the analysis of trip behaviors across different demographics and trip purposes by mapping detailed purposes into broader categories and excluding air travel from the analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - df : DataFrame\n",
    "        The NHTS DataFrame after initial processing, which includes WHYFROM codes that need to be mapped to broader trip purposes.\n",
    "    - print_progress : bool, optional\n",
    "        A flag to print progress updates during the execution of the function, by default True.\n",
    "\n",
    "    Steps:\n",
    "    1. Maps the WHYFROM column values to a new set of trip purpose categories.\n",
    "    2. Removes any unnecessary columns, specifically 'Unnamed: 0' if present.\n",
    "    3. Renames columns for clarity.\n",
    "    4. Drops duplicate rows based on a subset of columns to ensure unique records for aggregation.\n",
    "    5. Filters out trips made by air as they are not relevant for the probability calculation.\n",
    "    6. Aggregates the data to count each trip mode within each age class and trip purpose combination.\n",
    "    7. Counts the total number of records for each age class and trip purpose to serve as the denominator in the probability calculation.\n",
    "    8. Merges the counts and total counts dataframes to calculate the probability of choosing each trip mode for the given demographics and trip purposes.\n",
    "\n",
    "    Returns:\n",
    "    - result_total : DataFrame\n",
    "        A DataFrame containing the age class, trip purpose, trip mode, and the calculated probability of selecting each trip mode for each age class and trip purpose combination.\n",
    "    \"\"\"\n",
    "    \n",
    "    trippub_total = df.copy()\n",
    "    \n",
    "    tqdm.pandas(desc=\"0) Mapping trippurpose\") \n",
    "    \n",
    "    # Maps WHYFROM values to more generalized trip purpose categories.\n",
    "    def map_purpose(row):\n",
    "        if row['WHYFROM'] in [1, 2]:\n",
    "            return 'Home'\n",
    "        elif row['WHYFROM'] in [3, 4]:\n",
    "            return 'Work'\n",
    "        elif row['WHYFROM'] in [6, 8, 9, 10, 19]:\n",
    "            return 'S_d_r'\n",
    "        elif row['WHYFROM'] == 11:\n",
    "            return 'D_shop'\n",
    "        elif row['WHYFROM'] == 13:\n",
    "            return 'Meals'\n",
    "        elif row['WHYFROM'] == 17:\n",
    "            return 'V_fr_rel'\n",
    "        elif row['WHYFROM'] in [15, 16]:\n",
    "            return 'Rec_lei'\n",
    "        elif row['WHYFROM'] in [12, 14, 18]:\n",
    "            return 'Serv_trip'\n",
    "        elif row['WHYFROM'] in [5, 97]:\n",
    "            return 'Others'\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    # Apply the mapping function to create a new column for the mapped trip purposes.\n",
    "    trippub_total['TRPFROM_new'] = trippub_total.progress_apply(map_purpose, axis=1)\n",
    "    \n",
    "    # Removes an unnecessary column if present.\n",
    "    if \"Unnamed: 0\" in trippub_total.columns:\n",
    "        trippub_total.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    \n",
    "    # replace name of R_AGE_new with age_class\n",
    "    trippub_total.rename(columns={'R_AGE_new': 'age_class'}, inplace=True)\n",
    "    \n",
    "    if print_progress == True:\n",
    "        print('Compute probability of trip mode choice by age and trip purpose ... ')\n",
    "    \n",
    "    # Drops duplicate rows based on specific columns to ensure unique records for the aggregation.\n",
    "    trippub_total = trippub_total.drop_duplicates(subset=['PERSONID_new', 'age_class', 'TRPPURP_new', 'TRPTRANS_new'])\n",
    "    trippub_total = trippub_total[['PERSONID_new', 'age_class', 'TRPPURP_new', 'TRPTRANS_new']]\n",
    "\n",
    "    trippub_total.rename(columns={'PERSONID_new': 'uniqID'}, inplace=True)\n",
    "    trippub_total.rename(columns={'TRPPURP_new': 'Trip_pur'}, inplace=True)\n",
    "    trippub_total.rename(columns={'TRPTRANS_new': 'Trip_mode'}, inplace=True)\n",
    "    trippub_total.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    # Count the number of each Trip_mode per age_class and Trip_pur\n",
    "    trippub_total = trippub_total[trippub_total['Trip_mode'] != 'Air'] \n",
    "\n",
    "    count_df_total = trippub_total.groupby(['age_class', 'Trip_pur', 'Trip_mode']).size().reset_index(name='nominator')\n",
    "\n",
    "    # Count the total number of records per age_class and Trip_pur\n",
    "    total_df_total = trippub_total.groupby(['age_class', 'Trip_pur']).size().reset_index(name='denominator')\n",
    "\n",
    "    # Merge the two dataframes on age_class and Trip_pur\n",
    "    result_total = pd.merge(count_df_total, total_df_total, on=['age_class', 'Trip_pur'])\n",
    "\n",
    "    # Calculate the probability\n",
    "    result_total['Trip_modeP'] = result_total['nominator'] / result_total['denominator']\n",
    "    result_total\n",
    "    \n",
    "    print('Done!')\n",
    "    \n",
    "    return result_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Preprocess Safegraph Data to Compute probability of trips from origin cbg to dest cbg\n",
    "### A2.1. DO to OD, Covert Destination area to Origin area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DOtoOD(df, print_progress = True):\n",
    "    \"\"\"\n",
    "    Converts Destination-Origin (DO) data into Origin-Destination (OD) data using the input SafeGraph Neighborhood data.\n",
    "    The function processes columns related to work behavior and device home areas during weekdays and weekends,\n",
    "    transforming them into a format that specifies how many devices move from one area to another.\n",
    "    \n",
    "    The process involves three main steps\n",
    "    1) Calculating work behavior flows: Aggregates device counts moving from their home areas to work areas.\n",
    "    2) Calculating weekday behavior flows: Similar aggregation for devices traveling based on weekday patterns.\n",
    "    3) Calculating weekend behavior flows: Aggregates travels for devices during the weekend.\n",
    "        \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the DO data to be converted.\n",
    "    - print_progress (bool): If True, prints progress updates during the calculation process.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with the calculated OD data, including columns for area,\n",
    "      work behavior from the area, weekday device home from the area, and weekend device home from the area.\n",
    "    \"\"\"\n",
    "    \n",
    "    ODdata = df.copy()\n",
    "\n",
    "    def calculate_work_behavior_from(df):\n",
    "        # Initialize a dictionary to store visits for each area, using area values from the DataFrame as keys.\n",
    "        all_area_dict = {str(area): {} for area in df['area'].tolist()}\n",
    "\n",
    "        # Iterate over each row of the DataFrame to calculate work behavior flows from one area to another.\n",
    "        for _, row in tqdm(df.iterrows(), total = len(df), desc = '1. Work behavior...'):\n",
    "            # Parse the 'work_behavior_device_home_areas' column as a dictionary to get current area dict.\n",
    "            current_area_dict = eval(row['work_behavior_device_home_areas'])\n",
    "            destination = str(row['area'])\n",
    "\n",
    "            for source, count in current_area_dict.items():\n",
    "                # Skip the iteration if the source area is not in the all_area_dict.\n",
    "                if source not in all_area_dict:\n",
    "                    continue\n",
    "\n",
    "                # Initialize the destination in the source's dict if it doesn't exist.\n",
    "                if destination not in all_area_dict[source]:\n",
    "                    all_area_dict[source][destination] = 0\n",
    "\n",
    "                # Add the count of devices moving from source to destination.\n",
    "                all_area_dict[source][destination] += count\n",
    "\n",
    "        # Map the all_area_dict data to a new column 'work_behavior_from_area' in the DataFrame.\n",
    "        df['work_behavior_from_area'] = df['area'].map(lambda x: all_area_dict[str(x)])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fix_malformed_dict_str(s):\n",
    "        # If the string does not end with a closing brace, attempt to correct it.\n",
    "        if not s.endswith(\"}\"):\n",
    "            # Find the index of the last comma, which is where the string was likely cut off.\n",
    "            last_comma_index = s.rfind(\",\")\n",
    "            # Correct the string by ending it at the last comma and adding a closing brace.\n",
    "            s = s[:last_comma_index] + \"}\"\n",
    "        return s\n",
    "\n",
    "    def calculate_weekday_behavior_from(df):\n",
    "        # Initialize a dictionary to store visits for each area.\n",
    "        all_area_dict = {str(area): {} for area in df['area'].tolist()}\n",
    "\n",
    "        # Iterate over each row in the DataFrame.\n",
    "        for _, row in tqdm(df.iterrows(), total = len(df), desc = '2) Other behaviors - weekday...'):\n",
    "            # Correct any malformed dictionary strings.\n",
    "            fixed_str = fix_malformed_dict_str(row['weekday_device_home_areas'])\n",
    "            try:\n",
    "                # Attempt to parse the corrected string into a dictionary.\n",
    "                current_area_dict = eval(fixed_str)\n",
    "            except Exception as e:\n",
    "                # If parsing fails, print an error message and skip this row.\n",
    "                print(f\"Error in row {_}: {e}\")\n",
    "                continue\n",
    "\n",
    "            destination = str(row['area'])\n",
    "\n",
    "            for source, count in current_area_dict.items():\n",
    "                # If the source area is not recognized, skip it.\n",
    "                if source not in all_area_dict:\n",
    "                    continue\n",
    "\n",
    "                # Initialize the destination area in the source's dictionary if necessary.\n",
    "                if destination not in all_area_dict[source]:\n",
    "                    all_area_dict[source][destination] = 0\n",
    "\n",
    "                # Increment the count of devices moving from source to destination.\n",
    "                all_area_dict[source][destination] += count\n",
    "\n",
    "        # Assign the calculated data to a new column in the DataFrame.\n",
    "        df['weekday_device_from_area_home'] = df['area'].map(lambda x: all_area_dict[str(x)])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def calculate_weekend_behavior_from(df):\n",
    "        # Initialize a dictionary to store visits for each area.\n",
    "        all_area_dict = {str(area): {} for area in df['area'].tolist()}\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total = len(df), desc = '3) Other behaviors - weekend...'):\n",
    "            fixed_str = fix_malformed_dict_str(row['weekend_device_home_areas'])\n",
    "            try:\n",
    "                current_area_dict = eval(fixed_str)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in row {_}: {e}\")\n",
    "                continue\n",
    "\n",
    "            destination = str(row['area'])\n",
    "\n",
    "            for source, count in current_area_dict.items():\n",
    "                if source not in all_area_dict:\n",
    "                    continue\n",
    "\n",
    "                if destination not in all_area_dict[source]:\n",
    "                    all_area_dict[source][destination] = 0\n",
    "\n",
    "                all_area_dict[source][destination] += count\n",
    "\n",
    "        df['weekend_device_from_area_home'] = df['area'].map(lambda x: all_area_dict[str(x)])\n",
    "\n",
    "        return df\n",
    "    \n",
    "    # 1) Work behavior\n",
    "    ODdata = calculate_work_behavior_from(ODdata)\n",
    "    \n",
    "    # 2) Other behaviors - Weekday\n",
    "    ODdata = calculate_weekday_behavior_from(ODdata)\n",
    "    \n",
    "    # 3) Other behaviors - Weekend\n",
    "    ODdata = calculate_weekend_behavior_from(ODdata)\n",
    "\n",
    "    # Extracting columns\n",
    "    col = ['area', 'work_behavior_from_area','weekday_device_from_area_home','weekend_device_from_area_home']\n",
    "    ODdata = ODdata[col]\n",
    "    \n",
    "    return ODdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2.2. Computing probability of trips from origin cbg to dest cbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilityByk_Ws_Wd(neighbor_safegraphDF, landuseGDF, W_s, W_d):\n",
    "    \"\"\"\n",
    "    Computes the probability of moving from an origin CBG to a destination CBG,\n",
    "    factoring in the spatial attractiveness weight (W_s) and the distance sensitivity index (W_d).\n",
    "    \n",
    "    Parameters:\n",
    "    - neighbor_safegraphDF (DataFrame): The DataFrame containing origin-destination data derived from the previous step (DOtoOD function).\n",
    "    - landuseGDF (GeoDataFrame): A GeoDataFrame containing land use data for different CBGs.\n",
    "    - W_s (float): The spatial attractiveness weight, affecting preference for destinations based on trip purpose density.\n",
    "    - W_d (float): The distance sensitivity index, affecting preference for closer destinations.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame with probabilities adjusted by W_s and W_d for trips from each origin to possible destinations.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_probability(df, area='area', cols=['work_behavior_from_area', 'weekday_device_from_area_home', 'weekend_device_from_area_home']):\n",
    "        \"\"\"\n",
    "        Initializes and computes base probabilities of moving from one area to another based on work behavior, weekday, and weekend device home areas.\n",
    "        The probabilities are normalized by the total count of movements (k) for each category.\n",
    "        \"\"\"\n",
    "        # Create a copy of the original DataFrame for results\n",
    "        prob_trips_in_space = df.copy()\n",
    "        \n",
    "        prob_trips_in_space['work_behavior_from_area'] = prob_trips_in_space['work_behavior_from_area'].astype(str)\n",
    "        prob_trips_in_space['weekday_device_from_area_home'] = prob_trips_in_space['weekday_device_from_area_home'].astype(str)\n",
    "        prob_trips_in_space['weekend_device_from_area_home'] = prob_trips_in_space['weekend_device_from_area_home'].astype(str)\n",
    "\n",
    "        # Iterate over each row and normalize the movement counts to probabilities.\n",
    "        for index, row in tqdm(prob_trips_in_space.iterrows(), total=prob_trips_in_space.shape[0], desc = '1) Probability A to A_i...1 (add k folmula)'):\n",
    "\n",
    "            # Update each dictionary in the row based on its own total_k\n",
    "            for col in cols:\n",
    "                dict_data = eval(row[col]) # Convert the string back to a dictionary.\n",
    "                # Convert the string back to a dictionary.\n",
    "                total_k = sum(dict_data.values())  # Calculate the total k for the current column only\n",
    "\n",
    "                # Convert counts to probabilities by dividing each count by the total.\n",
    "                for key in dict_data:\n",
    "                    if total_k != 0:  # Ensure not to divide by zero\n",
    "                        dict_data[key] = dict_data[key] / total_k\n",
    "                    else:\n",
    "                        dict_data[key] = 0\n",
    "\n",
    "                # Update the DataFrame with the normalized probabilities.\n",
    "                prob_trips_in_space.at[index, col] = str(dict_data)  # Convert updated dictionary back to string representation\n",
    "\n",
    "        # Duplicate 'work_behavior_from_area' into new columns for further processing.\n",
    "        prob_trips_in_space['weekday_Work'] = prob_trips_in_space['work_behavior_from_area']\n",
    "        prob_trips_in_space['weekend_Work'] = prob_trips_in_space['work_behavior_from_area']\n",
    "\n",
    "        # Split 'weekday_device_from_area_home' into multiple columns for different trip purposes.\n",
    "        weekday_cols = ['weekday_School', 'weekday_University', 'weekday_Dailycare', 'weekday_Religion', 'weekday_Large_shop', 'weekday_Etc_shop', 'weekday_Meals', 'weekday_V_fr_rel', 'weekday_Rec_lei', 'weekday_Serv_trip', 'weekday_Others']\n",
    "        for col in weekday_cols:\n",
    "            prob_trips_in_space[col] = prob_trips_in_space['weekday_device_from_area_home']\n",
    "\n",
    "        # Repeat the process for weekend data.\n",
    "        weekend_cols = [col.replace('weekday', 'weekend') for col in weekday_cols]\n",
    "        for col in weekend_cols:\n",
    "            prob_trips_in_space[col] = prob_trips_in_space['weekend_device_from_area_home']\n",
    "\n",
    "        return prob_trips_in_space\n",
    "\n",
    "    def apply_Ws_formula(prob_trips_in_space, landUse, W_s):\n",
    "        df_ws = prob_trips_in_space.copy()\n",
    "        exclude_columns = ['work_behavior_from_area', 'weekday_device_from_area_home', 'weekend_device_from_area_home']\n",
    "        df_ws.drop(exclude_columns, axis=1, inplace=True)\n",
    "\n",
    "        # Iterate over the rows and columns of df_ws\n",
    "        for index, row in tqdm(df_ws.iterrows(), total=df_ws.shape[0], desc = '2) Probability A to A_i...2 (add Ws weight)'):\n",
    "            for col in df_ws.columns:\n",
    "                # Avoid processing non-dictionary columns and excluded columns\n",
    "                if col == 'area':\n",
    "                    continue\n",
    "\n",
    "                # Extract the purpose from the column name\n",
    "                purpose = \"_\".join(col.split('_')[1:])\n",
    "\n",
    "                dict_data = eval(row[col])\n",
    "\n",
    "                # Calculate C_Ai for each key in the dictionary\n",
    "                C_Ai_dict = {key: landUse[landUse['CBGCODE'] == key]['TRPPURP'].apply(lambda x: purpose in x).sum() for key in dict_data.keys()}\n",
    "    #             print(purpose)\n",
    "    #             print(C_Ai_dict)\n",
    "\n",
    "                # Calculate sum_j C_Aj\n",
    "                sum_C_Aj = sum(C_Ai_dict.values())\n",
    "\n",
    "                # Apply the formula\n",
    "                new_prob_values = {}  # A new dictionary to store normalized probabilities\n",
    "                for key, value in dict_data.items():\n",
    "                    C_Ai = C_Ai_dict[key]\n",
    "                    multiplier = (C_Ai / sum_C_Aj) ** W_s if sum_C_Aj != 0 else 0  # Ensure not to divide by zero\n",
    "                    new_prob_values[key] = value * multiplier\n",
    "\n",
    "                # Normalize the probabilities to sum up to 1\n",
    "                total_probability = sum(new_prob_values.values())\n",
    "                for key in new_prob_values:\n",
    "                    new_prob_values[key] = new_prob_values[key] / total_probability if total_probability != 0 else 0\n",
    "\n",
    "                df_ws.at[index, col] = str(new_prob_values)\n",
    "\n",
    "        return df_ws\n",
    "    \n",
    "    def apply_Ws_formula_optimized(prob_trips_in_space, landUse, W_s):\n",
    "        \"\"\"\n",
    "        Optimizes and adjusts the probabilities of moving from an origin to various destinations by considering the spatial attractiveness weight (W_s).\n",
    "        This approach pre-calculates and indexes the attractiveness measures (density or concentration of facilities relevant to the trip purpose) for each destination, \n",
    "        streamlining the adjustment of probabilities.\n",
    "\n",
    "        The spatial attractiveness weight (W_s) is used to adjust the base probabilities by the relative density of facilities that match the trip purpose at each destination. \n",
    "        This makes destinations with a higher concentration of relevant facilities more likely to be chosen, proportional to the value of W_s.\n",
    "        \"\"\"\n",
    "        df_ws = prob_trips_in_space.copy()\n",
    "        exclude_columns = ['work_behavior_from_area', 'weekday_device_from_area_home', 'weekend_device_from_area_home']\n",
    "        df_ws.drop(exclude_columns, axis=1, inplace=True)\n",
    "\n",
    "        # Pre-calculate C_Ai (attractiveness measure) for all destinations across all purposes.\n",
    "        all_keys = set()\n",
    "        for col in tqdm(df_ws.columns, desc = '2) Probability A to A_i...2 (add Ws weight)'):\n",
    "            if col != 'area':\n",
    "                df_ws[col].apply(lambda x: all_keys.update(eval(x).keys()))\n",
    "        all_keys = list(all_keys)\n",
    "        \n",
    "        # Create a dictionary to index C_Ai values for each destination and purpose.\n",
    "        C_Ai_dict = {key: {} for key in all_keys}\n",
    "        for key in tqdm(all_keys, desc = ' --- 2.1) Indexing '):\n",
    "            sub_df = landUse[landUse['CBGCODE'] == key]\n",
    "            for col in df_ws.columns:\n",
    "                if col != 'area':\n",
    "                    purpose = \"_\".join(col.split('_')[1:])\n",
    "                    C_Ai_dict[key][purpose] = sub_df['TRPPURP'].apply(lambda x: purpose in x).sum()\n",
    "\n",
    "        # Adjust the base probabilities using the pre-calculated attractiveness measures.\n",
    "        for index, row in tqdm(df_ws.iterrows(), total=df_ws.shape[0], desc=' --- 2.2) add Ws weight '):\n",
    "            for col in df_ws.columns:\n",
    "                if col == 'area':\n",
    "                    continue\n",
    "\n",
    "                # Extract the purpose from the column name\n",
    "                purpose = \"_\".join(col.split('_')[1:])\n",
    "                dict_data = eval(row[col])\n",
    "\n",
    "                # Fetch pre-calculated attractiveness measures for the current purpose.\n",
    "                local_C_Ai_values = [C_Ai_dict[key][purpose] for key in dict_data.keys()]\n",
    "                sum_C_Aj = sum(local_C_Ai_values)\n",
    "\n",
    "                # Apply the formula\n",
    "                new_prob_values = {}\n",
    "                for (key, value), C_Ai in zip(dict_data.items(), local_C_Ai_values):\n",
    "                    multiplier = (C_Ai / sum_C_Aj) ** W_s if sum_C_Aj != 0 else 0\n",
    "                    new_prob_values[key] = value * multiplier\n",
    "\n",
    "                # Normalize the probabilities to sum up to 1\n",
    "                total_probability = sum(new_prob_values.values())\n",
    "                for key in new_prob_values:\n",
    "                    new_prob_values[key] = new_prob_values[key] / total_probability if total_probability != 0 else 0\n",
    "\n",
    "                df_ws.at[index, col] = str(new_prob_values)\n",
    "\n",
    "        return df_ws\n",
    "\n",
    "    \n",
    "    \n",
    "    def calculate_distance_meters(point1, point2):\n",
    "        # WGS 84\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "        angle1,angle2,distance = geod.inv(point1.x, point1.y, point2.x, point2.y)\n",
    "\n",
    "        return distance\n",
    "\n",
    "    \n",
    "    def apply_Wd_formula(df, landUse, W_d=0):\n",
    "        \"\"\"\n",
    "        Adjusts the probabilities of moving from an origin to various destinations based on the distance sensitivity index (W_d). \n",
    "        This function calculates the distance between origin and destination CBGs and applies a decay function to adjust probabilities based on these distances.\n",
    "        The closer a destination is to the origin, the higher the probability of movement towards it, adjusted by the W_d parameter.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_wd = df.copy()\n",
    "        \n",
    "        # Iterate over the rows and columns of df_wd to adjust probabilities based on distance.\n",
    "        for index, row in tqdm(df_wd.iterrows(), total=df_wd.shape[0], desc='3) Probability A to A_i...3 (add W_d weight)'):\n",
    "\n",
    "            # Convert row['area'] to string for matching and get the geometry of the area.\n",
    "            area_str = str(row['area'])\n",
    "            area_geometry = landUse[landUse['CBGCODE'] == area_str].geometry.iloc[0]\n",
    "            area_center = area_geometry.centroid\n",
    "\n",
    "            for col in df_wd.columns:\n",
    "                if col == 'area':\n",
    "                    continue # Skip the 'area' column as it's not needed for probability adjustment.\n",
    "\n",
    "                dict_data = eval(row[col]) # Convert the string representation back to a dictionary.\n",
    "                keys_to_remove = [] # List to keep track of destinations with zero probability.\n",
    "\n",
    "                for key, value in dict_data.items():\n",
    "                    # Attempt to get the destination geometry; if missing, mark for removal.\n",
    "                    destination_geometry_series = landUse[landUse['CBGCODE'] == key].geometry\n",
    "\n",
    "                    if destination_geometry_series.empty:\n",
    "#                         print(key)\n",
    "                        dict_data[key] = 0# Mark the probability as zero if the destination geometry is missing.\n",
    "                        continue\n",
    "\n",
    "                    destination_geometry = destination_geometry_series.iloc[0]\n",
    "                    destination_center = destination_geometry.centroid  # Calculate the centroid of the destination area.\n",
    "                    distance = calculate_distance_meters(area_center, destination_center)\n",
    "                    distance = distance/1000 # Convert distance to kilometers for the decay function.\n",
    "\n",
    "                    # Adjust the probability based on distance using the W_d parameter.\n",
    "                    dict_data[key] = value * np.exp(-W_d * distance)\n",
    "\n",
    "                # Normalize the updated probabilities so they sum up to 1.\n",
    "                total_probability = sum(dict_data.values())\n",
    "                for key in dict_data:\n",
    "                    dict_data[key] = round(dict_data[key] / total_probability if total_probability != 0 else 0, 5)\n",
    "\n",
    "                for key in keys_to_remove:\n",
    "                    del dict_data[key] # Remove destinations with zero probability.\n",
    "\n",
    "                df_wd.at[index, col] = str(dict_data)  # Update the DataFrame with the adjusted probabilities.\n",
    "\n",
    "        return df_wd\n",
    "    \n",
    "    def apply_Wd_formula_optimized(df, landUse, W_d=0):\n",
    "        \"\"\"\n",
    "        Optimizes the adjustment of movement probabilities between origins and destinations based on the distance sensitivity index (W_d) by pre-computing geometric centroids for all areas. \n",
    "        This approach facilitates faster distance calculations and the efficient application of the distance decay function to the movement probabilities, \n",
    "        reflecting the preference for closer destinations.\n",
    "\n",
    "        The function modifies the probabilities such that destinations closer to the origin are more likely to be selected, \n",
    "        with the degree of preference for proximity determined by the W_d parameter. \n",
    "        This is achieved by applying an exponential decay function based on the calculated distances between each origin and destination pair.\n",
    "        \"\"\"\n",
    "        df_wd = df.copy()\n",
    "\n",
    "        # Precompute centroids for all areas in the land use GeoDataFrame for faster lookup.\n",
    "        landUse['centroid'] = landUse['geometry'].centroid\n",
    "\n",
    "        # Create a dictionary for fast lookup of centroids\n",
    "        centroid_lookup = landUse.set_index('CBGCODE')['centroid'].to_dict()\n",
    "\n",
    "        rows_to_drop = []  ## Keep track of rows with undefined areas, if any.\n",
    "        areas_to_drop = []\n",
    "\n",
    "        # Iterate over the rows and columns of df_wd\n",
    "        for index, row in tqdm(df_wd.iterrows(), total=df_wd.shape[0], desc='3) Probability A to A_i...3 (add W_d weight)'):\n",
    "\n",
    "            area_str = str(row['area'])\n",
    "            area_center = centroid_lookup.get(area_str)\n",
    "\n",
    "            if area_center is None:  # If there's no centroid for the area, mark row for removal\n",
    "                rows_to_drop.append(index)\n",
    "                areas_to_drop.append(area_str)\n",
    "                continue\n",
    "\n",
    "            for col in df_wd.columns:\n",
    "                if col == 'area':\n",
    "                    continue\n",
    "\n",
    "                dict_data = eval(row[col])\n",
    "                keys_to_remove = []\n",
    "\n",
    "                # Adjust each probability value based on the distance to the destination.\n",
    "                for key, value in dict_data.items():\n",
    "                    destination_center = centroid_lookup.get(key)\n",
    "\n",
    "                    if destination_center is None:  # Check for missing destination geometry\n",
    "                        dict_data[key] = 0\n",
    "                    else:\n",
    "                        distance = calculate_distance_meters(area_center, destination_center) / 1000\n",
    "                        dict_data[key] = value * np.exp(-W_d * distance)\n",
    "\n",
    "                    # If value is zero, mark for removal\n",
    "                    if dict_data[key] == 0:\n",
    "                        keys_to_remove.append(key)\n",
    "\n",
    "                # Normalize the updated probabilities.\n",
    "                total_probability = sum(dict_data.values())\n",
    "                for key in dict_data:\n",
    "                    dict_data[key] = round(dict_data[key] / total_probability if total_probability != 0 else 0, 5)\n",
    "\n",
    "                # Remove destination probabilities set to zero.\n",
    "                for key in keys_to_remove:\n",
    "                    del dict_data[key]\n",
    "\n",
    "                df_wd.at[index, col] = str(dict_data)\n",
    "\n",
    "        # Drop rows where area_center is None and reset index\n",
    "        df_wd.drop(rows_to_drop, inplace=True)\n",
    "        df_wd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#         print('dropped area: ', areas_to_drop)\n",
    "        return df_wd\n",
    "    \n",
    "    # k formula\n",
    "    print('----- W_s: ' + str(W_s) + ', W_d: ' + str(W_d) + '-----')\n",
    "    prob_trips_in_space_k = compute_probability(neighbor_safegraphDF)\n",
    "    prob_trips_in_space_ws = apply_Ws_formula_optimized(prob_trips_in_space_k, landUse, W_s = W_s)\n",
    "    prob_trips_in_space_wd = apply_Wd_formula_optimized(prob_trips_in_space_ws, landUse, W_d = W_d)\n",
    "    \n",
    "    # Dealing with Empty variables\n",
    "    for index, row in tqdm(prob_trips_in_space_wd.iterrows(), total = len(prob_trips_in_space_wd), desc = '4) Filling empty values'):\n",
    "        if row['weekday_Work'] == '{}' or row['weekend_Work'] == '{}':\n",
    "            area_value = row['area']\n",
    "            prob_trips_in_space_wd.at[index, 'weekday_Work'] = f\"{{'{area_value}': 1.0}}\"\n",
    "            prob_trips_in_space_wd.at[index, 'weekend_Work'] = f\"{{'{area_value}': 1.0}}\"\n",
    "\n",
    "    # Remove \"Unnamed: 0\" if existed.\n",
    "    if 'Unnamed: 0' in prob_trips_in_space_wd.columns:\n",
    "        prob_trips_in_space_wd.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    return prob_trips_in_space_wd\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.3. fill empty probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈것들 채워주는 코드\n",
    "\n",
    "def fill_values(row):\n",
    "    \"\"\"\n",
    "    Fills empty probability distributions for trip purposes in a row of a DataFrame with alternative values based on a predefined hierarchy of preferences. \n",
    "    This ensures that each trip purpose category has a valid probability distribution, either specific or borrowed from a related category.\n",
    "\n",
    "    Parameters:\n",
    "    - row (pd.Series): A row from a DataFrame representing trip distributions for various purposes.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: The input row with empty trip distribution categories filled with alternative values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process 'weekday_Dailycare' column.\n",
    "    # If there are no probabilities defined for weekday Dailycare, attempt to use the\n",
    "    # probabilities from 'weekday_Religion' or 'weekday_School' as alternatives.\n",
    "    if row['weekday_Dailycare'] == '{}':\n",
    "        if row['weekday_Religion'] != '{}':\n",
    "            row['weekday_Dailycare'] = row['weekday_Religion']\n",
    "        elif row['weekday_School'] != '{}':\n",
    "            row['weekday_Dailycare'] = row['weekday_School']\n",
    "\n",
    "    if row['weekend_Dailycare'] == '{}':\n",
    "        if row['weekday_Dailycare'] != '{}':\n",
    "            row['weekend_Dailycare'] = row['weekday_Dailycare']\n",
    "        elif row['weekend_Religion'] != '{}':\n",
    "            row['weekend_Dailycare'] = row['weekend_Religion']\n",
    "        elif row['weekend_School'] != '{}':\n",
    "            row['weekend_Dailycare'] = row['weekend_School']\n",
    "            \n",
    "    # weekday_Large_shop\n",
    "    if row['weekday_Large_shop'] == '{}':\n",
    "        if row['weekday_Etc_shop'] != '{}':\n",
    "            row['weekday_Large_shop'] = row['weekday_Etc_shop']\n",
    "\n",
    "    # weekend_Large_shop\n",
    "    if row['weekend_Large_shop'] == '{}':\n",
    "        if row['weekday_Large_shop'] != '{}':\n",
    "            row['weekend_Large_shop'] = row['weekday_Large_shop']\n",
    "        elif row['weekend_Etc_shop'] != '{}':\n",
    "            row['weekend_Large_shop'] = row['weekend_Etc_shop']\n",
    "        elif row['weekday_Etc_shop'] != '{}':\n",
    "            row['weekend_Large_shop'] = row['weekday_Etc_shop']\n",
    "            \n",
    "    # weekend_Etc_shop\n",
    "    if row['weekend_Etc_shop'] == '{}':\n",
    "        if row['weekday_Etc_shop'] != '{}':\n",
    "            row['weekend_Etc_shop'] = row['weekday_Etc_shop']\n",
    "        elif row['weekend_Large_shop'] != '{}':\n",
    "            row['weekend_Etc_shop'] = row['weekend_Large_shop']\n",
    "    \n",
    "    # weekday_Religion\n",
    "    if row['weekday_Religion'] == '{}':\n",
    "        if row['weekday_Dailycare'] != '{}':\n",
    "            row['weekday_Religion'] = row['weekday_Dailycare']\n",
    "        elif row['weekday_School'] != '{}':\n",
    "            row['weekday_Religion'] = row['weekday_School']\n",
    "            \n",
    "    # weekend_Religion\n",
    "    if row['weekend_Religion'] == '{}':\n",
    "        if row['weekday_Religion'] != '{}':\n",
    "            row['weekend_Religion'] = row['weekday_Religion']\n",
    "        elif row['weekend_Dailycare'] != '{}':\n",
    "            row['weekend_Religion'] = row['weekend_Dailycare']\n",
    "            \n",
    "    if row['weekend_School'] == '{}':\n",
    "        if row['weekend_Dailycare'] != '{}':\n",
    "            row['weekend_School'] = row['weekend_Dailycare']\n",
    "        elif row['weekend_Religion'] != '{}':\n",
    "            row['weekend_School'] = row['weekend_Religion']            \n",
    "            \n",
    "    if row['weekend_Meals'] == '{}':\n",
    "        if row['weekday_Meals'] != '{}':\n",
    "            row['weekend_Meals'] = row['weekday_Meals']\n",
    "        elif row['weekend_Etc_shop'] != '{}':\n",
    "            row['weekend_Meals'] = row['weekend_Etc_shop']              \n",
    "            \n",
    "    if row['weekend_Rec_lei'] == '{}':\n",
    "        if row['weekday_Rec_lei'] != '{}':\n",
    "            row['weekend_Rec_lei'] = row['weekday_Rec_lei']   \n",
    "            \n",
    "    if row['weekend_Serv_trip'] == '{}':\n",
    "        if row['weekday_Serv_trip'] != '{}':\n",
    "            row['weekend_Serv_trip'] = row['weekday_Serv_trip']\n",
    "            \n",
    "    if row['weekend_Others'] == '{}':\n",
    "        if row['weekday_Others'] != '{}':\n",
    "            row['weekend_Others'] = row['weekday_Others']     \n",
    "            \n",
    "            \n",
    "            \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execusion by User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data/Origin폴더에 있는 데이터들은 용량이 크니 그냥 Google Drive에 넣어서 공유하기. \n",
    "## 여기에 있는 데이터들은 Preprocessing NHTS 와 SafeGraph에만 사용하는 것이니 다운받아서 해보려면 해라. 이런 식으로만 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preprocessing NHTS and SafeGraph Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import for preprocessing Origin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path to relative path - only for publishing\n",
    "# current_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "# os.chdir(current_directory)\n",
    "\n",
    "path = \"./data/Origin/\"\n",
    "trippub = pd.read_csv(path + 'trippub.csv') # NHTS\n",
    "neighbor_2020_09 =  pd.read_csv(path + 'neighbor_2020_09.csv') # SafeGraph\n",
    "landUse = gpd.read_file(path + 'Milwaukee_parcels.shp') # Landuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Preprocess NHTS\n",
    "#### 0.1.1 Organize columns of NHTS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce802f0e88048bb98968c52d642f83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0) mapping TRPPURP:   0%|          | 0/923572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71147b14cfcb449294c9e787b4ee85cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1) mapping TRPTRANS:   0%|          | 0/923572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5724b620a194ac8bdc87c0e79473b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1) mapping TRPTRANS:   0%|          | 0/923572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cd4ac6a48c423198cddd312b5a4cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3) mapping R_AGE_new:   0%|          | 0/923572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4) Now, Other columns...\n",
      "\n",
      "... done!\n"
     ]
    }
   ],
   "source": [
    "trippub_organized = organize_columns(trippub, print_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1.2. Preprocess NHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing... NHTS data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e52bc3949d4c10931065967a29cce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0) remove duplicate home:   0%|          | 0/211519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-5cb7c02ff3c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreprocessed_NHTS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_NHTS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrippub_organized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_progress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-81-254445540e91>\u001b[0m in \u001b[0;36mpreprocess_NHTS\u001b[1;34m(df, print_progress)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m# Remove duplicated Home\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprint_progress\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mtrip_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrip_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Day_Type'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'uniqID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_duplicate_home\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtrip_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrip_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Day_Type'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'uniqID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_duplicate_home\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m                 \u001b[1;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1565\u001b[0m                     \u001b[0mold_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                 ) if is_np_func else nullcontext():\n\u001b[1;32m-> 1567\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1568\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m                 \u001b[1;31m# gh-20949\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1656\u001b[0m             \u001b[0moverride_group_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1658\u001b[1;33m         return self._wrap_applied_output(\n\u001b[0m\u001b[0;32m   1659\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m_wrap_applied_output\u001b[1;34m(self, data, values, not_indexed_same, override_group_keys)\u001b[0m\n\u001b[0;32m   1035\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_not_none\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m             return self._concat_objects(\n\u001b[0m\u001b[0;32m   1038\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m                 \u001b[0mnot_indexed_same\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnot_indexed_same\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_concat_objects\u001b[1;34m(self, values, not_indexed_same, override_group_keys)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 \u001b[0mgroup_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m                 result = concat(\n\u001b[0m\u001b[0;32m   1150\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 )\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;36m1\u001b[0m   \u001b[1;36m3\u001b[0m   \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \"\"\"\n\u001b[1;32m--> 368\u001b[1;33m     op = _Concatenator(\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_new_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m_get_new_axes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_new_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_result_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m         return [\n\u001b[0m\u001b[0;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concat_axis\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_comb_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_result_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         return [\n\u001b[1;32m--> 634\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concat_axis\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_comb_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         ]\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m_get_concat_axis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[0mconcat_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concat_indexes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             concat_axis = _make_concat_multiindex(\n\u001b[0m\u001b[0;32m    694\u001b[0m                 \u001b[0mindexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m_make_concat_multiindex\u001b[1;34m(indexes, keys, levels, names)\u001b[0m\n\u001b[0;32m    750\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key {key} not in level {level}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m                 \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m                 \u001b[0mto_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mnonzero\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mnonzero\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1983\u001b[0m     \"\"\"\n\u001b[1;32m-> 1984\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nonzero'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\geo_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "repaired_NHTS = preprocess_NHTS(trippub_organized, print_progress = True) # too long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1.3. Preprocessing for tripmode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_mode_prop_all = preprocess_NHTS_tripMode(trippub_organized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Preprocess SafeGraph data\n",
    "#### 0.2.1 DO to OD, Covert Destination area to Origin area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae82f09050744c32a277fd82d7d9ec20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1. Work behavior...:   0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282ac6e53ae542e198bd4f7dc4fc4f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2) Other behaviors - weekday...:   0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c673e4ccd914315b23cff77532a946c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3) Other behaviors - weekend...:   0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neighbor_2020_09_OD = DOtoOD(neighbor_2020_09, print_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2.2. Computing probability of trips from origin cbg to dest cbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- W_s: 0, W_d: 0-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c121c846699047a3acd1fd70f2de3334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1) Probability A to A_i...1 (add k folmula):   0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a6bbe8fb9741d08914cf0953007ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2) Probability A to A_i...2 (add Ws weight):   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcbbf0b2f0a40cdaa916f6bdf7fb907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " --- 2.1) Indexing :   0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fc9fe44c2848b7992740a830d82f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " --- 2.2) add Ws weight :   0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca2ce3fc2684834a691de45c9dc2d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3) Probability A to A_i...3 (add W_d weight):   0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4702c3c8e4bc4589922bcb1e06770c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4) Filling empty values:   0%|          | 0/821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prob_trips_2020_09_ws0_wd0 = compute_probabilityByk_Ws_Wd(neighbor_2020_09_OD, landUse, W_s = 0, W_d = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2.3. Combine all probability tables by their month\n",
    " - We concatenate all the dataframes that are resulted from different combinations of W_s and W_d\n",
    " - 'prob_2020_09_combined' data was made by combinations: Wd ([0.25, 0.5, 0.75]), Ws ([0.5, 1, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_trips_2020_09_ws0_wd0 -> prob_2020_09_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2.4. fill empty probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_2020_09_combined = prob_2020_09_combined.apply(fill_values, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기서부터가 실질적인 시작임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import for preprocessing for ABTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"./data/\"\n",
    "\n",
    "prob_2020_09_combined = pd.read_csv(dataPath + 'prob_2020_09_combined.csv') # Combined Probability of travels from O to D in Sep, 2020\n",
    "repaired_NHTS = pd.read_csv(dataPath + 'repaired_NHTS.csv') # preprocessed NHTS\n",
    "trip_mode_prop_all = pd.read_csv(dataPath + 'trip_mode_prop_all.csv') # trip mode\n",
    "cbg = gpd.read_file(dataPath + 'cbg_milwaukee.shp') # CBG shp file\n",
    "network_road = ox.graph_from_place('Milwaukee, Wisconsin, USA', network_type='drive') # road network data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trip Occurence Builder\n",
    "### 1.1. The probability of a person with age ‘a’ having ‘t’ trips occurs in a single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기는 사실 Preprocessing에 가까움. 이런 경우, preprocessing이라고 쓰기. 그래서 설명에 이 과정은 한번만 시행해도 되니 앞으로 빼서 한번에 해도 됨 이라고 쓰기 각각에."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The number of trips occurring ‘k’ times for a single individual ‘i’ in a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Trip Chains Builder\n",
    "### 2.0. preprocessing: Create trip seqeunce using origin NHTS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Finding optimal origin sequence O_i most similar to S_i\n",
    "### 2.2) Randomly assign the trip sequence for S_i\n",
    "### 2.3) Reassign the sequence of trip in S_i based on O_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trip Timing Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trip Mode Assigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spatial Trip Route Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
